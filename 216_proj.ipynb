{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "216_proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWZJbaQzheLA"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j77e54qKhngq"
      },
      "source": [
        "# 64 classes of optotypes. Should correspond to the folder names under images/testing and images/training\n",
        "labels = [\"+blank\", \"+circle\", \"+diamond\", \"+square\", \"2\", \"3\", \"5\", \"6\", \"8\", \"9\", \"apple\", \"bird\", \"C\", \"C-0\", \"C-45\",\n",
        "          \"C-90\", \"C-135\", \"C-180\", \"C-225\", \"C-270\", \"C-315\", \"cake\", \"car\", \"circle\", \"cow\", \"cup\", \"D\",\n",
        "          \"duck\", \"E\", \"E-0\", \"E-90\", \"E-180\", \"E-270\", \"F\", \"flat-line\", \"flat-square\",\n",
        "          \"frown-line\", \"frown-square\", \"H\", \"hand\", \"horse\", \"house\", \"K\", \"L\", \"N\", \"O\", \"P\",\n",
        "          \"panda\", \"phone\", \"R\", \"S\", \"smile-line\", \"smile-square\", \"square\", \"star\", \"T\", \"train\",\n",
        "          \"tree\", \"V\", \"x-blank\", \"x-circle\", \"x-diamond\", \"x-square\", \"Z\"]\n",
        "img_size = 400\n",
        "channels = 3\n",
        "training_dir = 'drive/MyDrive/opt_images/training'\n",
        "testing_dir = 'drive/MyDrive/opt_images/testing'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSytkUUMivSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe42b11a-f68d-40d0-da18-aa957a7c9ddf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqe6B3Amhtx4"
      },
      "source": [
        "def create_datasets_test_train():\n",
        "    \"\"\"\n",
        "    Generates two `tf.data.Dataset` from image files in the project.\n",
        "    NOTE: images must be put under project root in directory images/testing and images/training\n",
        "    :return: Two `tf.data.Dataset` objects, one for testing and one for training.\n",
        "    \"\"\"\n",
        "    training_set = image_dataset_from_directory(training_dir,\n",
        "                                                shuffle=True,\n",
        "                                                batch_size=32,\n",
        "                                                image_size=(img_size, img_size))\n",
        "    testing_set = image_dataset_from_directory(testing_dir,\n",
        "                                               shuffle=True,\n",
        "                                               batch_size=32,\n",
        "                                               image_size=(img_size, img_size))\n",
        "    return training_set, testing_set\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAwNoDu5h2eE"
      },
      "source": [
        "def create_datasets(validation_split=0.2):\n",
        "    \"\"\"\n",
        "    Generates three `tf.data.Dataset` from image files in the project.\n",
        "    @:param validation_split: float between 0 and 1, fraction of data to reserve for validation.\n",
        "    NOTE: images must be put under project root in directory images/testing and images/training.\n",
        "    :return: Three `tf.data.Dataset` objects, testing, training, and validation.\n",
        "    \"\"\"\n",
        "    training_set = image_dataset_from_directory(training_dir,\n",
        "                                                shuffle=True,\n",
        "                                                batch_size=32,\n",
        "                                                validation_split=validation_split,\n",
        "                                                subset=\"training\",\n",
        "                                                seed=0,\n",
        "                                                image_size=(img_size, img_size))\n",
        "    validation_set = image_dataset_from_directory(training_dir,\n",
        "                                                  shuffle=False,\n",
        "                                                  batch_size=32,\n",
        "                                                  validation_split=validation_split,\n",
        "                                                  subset=\"validation\",\n",
        "                                                  seed=0,\n",
        "                                                  image_size=(img_size, img_size))\n",
        "    testing_set = image_dataset_from_directory(testing_dir,\n",
        "                                               shuffle=True,\n",
        "                                               batch_size=32,\n",
        "                                               image_size=(img_size, img_size))\n",
        "    return training_set, validation_set, testing_set"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFT_O8Oh4Z1"
      },
      "source": [
        "# Just trying to replicate the same thing that I had last time\n",
        "def create_model():\n",
        "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_size, img_size, channels))\n",
        "    vgg.trainable = False\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "    x = preprocess_input(inputs)\n",
        "    x = vgg(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whq2eDzaxAuP"
      },
      "source": [
        "def create_base_model():\n",
        "    \"\"\"\n",
        "    Creates base of VGG16 transfer learning model with imagenet weights and input shape of the same size in config.\n",
        "    :return: VGG16 model\n",
        "    \"\"\"\n",
        "    vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "    vgg.trainable = False\n",
        "    return vgg"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc1cu2Tqh6tE"
      },
      "source": [
        "def runner():\n",
        "    \"\"\"\n",
        "    Essentially the main function. Feel free to change as you see fit.\n",
        "    \"\"\"\n",
        "\n",
        "    # Uncomment if on arcus servers\n",
        "    # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
        "\n",
        "    training_set, validation_set, testing_set = create_datasets()\n",
        "\n",
        "    # base_model = create_base_model()\n",
        "\n",
        "    # inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "    # x = base_model(inputs, training=False)\n",
        "    # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    # x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    # outputs = tf.keras.layers.Dense(1, activation=tf.keras.activations.softmax)(x)\n",
        "\n",
        "    # model = tf.keras.Model(inputs, outputs)\n",
        "    # model.summary()\n",
        "\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # model.fit(training_set, epochs=25, verbose=2, validation_data=validation_set,\n",
        "    #           callbacks=[WandbCallback(data_type=\"image\", labels=labels), TensorBoard(log_dir=wandb.run.dir)])\n",
        "    # model.evaluate(testing_set, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfxOEMMQiMww",
        "outputId": "bf17f415-6c62-47dc-ebf5-a0686c7a611d"
      },
      "source": [
        "training_set, validation_set, testing_set = create_datasets()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1500 files belonging to 64 classes.\n",
            "Using 1200 files for training.\n",
            "Found 1500 files belonging to 64 classes.\n",
            "Using 300 files for validation.\n",
            "Found 3223 files belonging to 64 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgjN1s1Cr6oZ",
        "outputId": "134674bf-49ec-4a2e-9811-8eb474602a40"
      },
      "source": [
        "# View a single example entry from a batch\n",
        "features, label = iter(training_set).next()\n",
        "print(\"example features:\", features.shape)\n",
        "print(\"example label:\", label[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example features: (32, 400, 400, 3)\n",
            "example label: tf.Tensor(11, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOU_Rl0oQIE9"
      },
      "source": [
        "### Quick test to see how to use given code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsjrQCVAvPTy",
        "outputId": "b126dae2-6e77-4f26-be6f-833a02ecd032"
      },
      "source": [
        "base_model = create_base_model()\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_26 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_26 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_26 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_27 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_27 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_27 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_10  (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_28 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_28 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_28 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_29 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_29 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_29 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_11  (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_30 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_30 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_30 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_31 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_31 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_31 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_32 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_32 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_32 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_12  (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_33 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_33 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_33 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_34 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_34 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_34 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_35 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_35 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_35 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_13  (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_36 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_36 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_36 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_37 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_37 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_37 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_38 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_38 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_38 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_14  (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaFpolbuweQH",
        "outputId": "230f2772-fded-4919-a5e7-0803d98313ff"
      },
      "source": [
        "model.fit(training_set, epochs=15, verbose=2, validation_data=validation_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "38/38 - 25s - loss: 0.9447 - accuracy: 0.6658 - val_loss: 0.3688 - val_accuracy: 0.8767\n",
            "Epoch 2/15\n",
            "38/38 - 25s - loss: 0.9465 - accuracy: 0.6608 - val_loss: 0.3475 - val_accuracy: 0.8633\n",
            "Epoch 3/15\n",
            "38/38 - 25s - loss: 0.9089 - accuracy: 0.6700 - val_loss: 0.3409 - val_accuracy: 0.8900\n",
            "Epoch 4/15\n",
            "38/38 - 25s - loss: 0.9267 - accuracy: 0.6575 - val_loss: 0.3438 - val_accuracy: 0.8600\n",
            "Epoch 5/15\n",
            "38/38 - 25s - loss: 0.9676 - accuracy: 0.6658 - val_loss: 0.3418 - val_accuracy: 0.8867\n",
            "Epoch 6/15\n",
            "38/38 - 25s - loss: 0.9276 - accuracy: 0.6783 - val_loss: 0.3576 - val_accuracy: 0.8600\n",
            "Epoch 7/15\n",
            "38/38 - 25s - loss: 0.9330 - accuracy: 0.6783 - val_loss: 0.3498 - val_accuracy: 0.8667\n",
            "Epoch 8/15\n",
            "38/38 - 25s - loss: 0.8873 - accuracy: 0.6883 - val_loss: 0.3444 - val_accuracy: 0.8567\n",
            "Epoch 9/15\n",
            "38/38 - 25s - loss: 0.9082 - accuracy: 0.6842 - val_loss: 0.3354 - val_accuracy: 0.8700\n",
            "Epoch 10/15\n",
            "38/38 - 25s - loss: 0.9022 - accuracy: 0.6717 - val_loss: 0.3549 - val_accuracy: 0.8667\n",
            "Epoch 11/15\n",
            "38/38 - 25s - loss: 0.8780 - accuracy: 0.6983 - val_loss: 0.3443 - val_accuracy: 0.8767\n",
            "Epoch 12/15\n",
            "38/38 - 25s - loss: 0.8847 - accuracy: 0.6792 - val_loss: 0.3442 - val_accuracy: 0.8567\n",
            "Epoch 13/15\n",
            "38/38 - 25s - loss: 0.8783 - accuracy: 0.6808 - val_loss: 0.3357 - val_accuracy: 0.8733\n",
            "Epoch 14/15\n",
            "38/38 - 25s - loss: 0.8451 - accuracy: 0.7058 - val_loss: 0.3272 - val_accuracy: 0.8867\n",
            "Epoch 15/15\n",
            "38/38 - 24s - loss: 0.8668 - accuracy: 0.7000 - val_loss: 0.3541 - val_accuracy: 0.8767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92d858ce50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO6uJRY1xT_L",
        "outputId": "f83d26f2-36cc-4730-ddb8-29eae1052707"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 55s 533ms/step - loss: 2.6762 - accuracy: 0.3010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.676180362701416, 0.30096182227134705]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcvAJB-VOtAG"
      },
      "source": [
        "### Trying VGG16 with different epochs, all layers unfrozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxSgXDtORBYa",
        "outputId": "58f21bc4-02d4-4ac8-e042-3630659b28c2"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUdIO0t0TIHm",
        "outputId": "02f075f0-a97a-4980-f5ef-8fda733b85cc"
      },
      "source": [
        "model.fit(training_set, epochs=60, validation_data=validation_set)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "38/38 [==============================] - 226s 4s/step - loss: 4.3538 - accuracy: 0.0275 - val_loss: 2.8420 - val_accuracy: 0.3033\n",
            "Epoch 2/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 3.7540 - accuracy: 0.0967 - val_loss: 2.2993 - val_accuracy: 0.4767\n",
            "Epoch 3/60\n",
            "38/38 [==============================] - 23s 600ms/step - loss: 3.3757 - accuracy: 0.1583 - val_loss: 2.0543 - val_accuracy: 0.5667\n",
            "Epoch 4/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 3.1331 - accuracy: 0.2050 - val_loss: 1.8614 - val_accuracy: 0.6200\n",
            "Epoch 5/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 2.8923 - accuracy: 0.2483 - val_loss: 1.6806 - val_accuracy: 0.6467\n",
            "Epoch 6/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 2.7394 - accuracy: 0.2767 - val_loss: 1.5287 - val_accuracy: 0.6933\n",
            "Epoch 7/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 2.5333 - accuracy: 0.3150 - val_loss: 1.3814 - val_accuracy: 0.7533\n",
            "Epoch 8/60\n",
            "38/38 [==============================] - 24s 616ms/step - loss: 2.3951 - accuracy: 0.3583 - val_loss: 1.2579 - val_accuracy: 0.7833\n",
            "Epoch 9/60\n",
            "38/38 [==============================] - 24s 611ms/step - loss: 2.3147 - accuracy: 0.3608 - val_loss: 1.1639 - val_accuracy: 0.7967\n",
            "Epoch 10/60\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 2.1769 - accuracy: 0.3792 - val_loss: 1.0851 - val_accuracy: 0.8000\n",
            "Epoch 11/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 2.0903 - accuracy: 0.4225 - val_loss: 0.9744 - val_accuracy: 0.8167\n",
            "Epoch 12/60\n",
            "38/38 [==============================] - 24s 612ms/step - loss: 2.0029 - accuracy: 0.4183 - val_loss: 0.9116 - val_accuracy: 0.8267\n",
            "Epoch 13/60\n",
            "38/38 [==============================] - 24s 610ms/step - loss: 1.9123 - accuracy: 0.4475 - val_loss: 0.8723 - val_accuracy: 0.8167\n",
            "Epoch 14/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.8719 - accuracy: 0.4642 - val_loss: 0.8176 - val_accuracy: 0.8400\n",
            "Epoch 15/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.8043 - accuracy: 0.4742 - val_loss: 0.7416 - val_accuracy: 0.8233\n",
            "Epoch 16/60\n",
            "38/38 [==============================] - 24s 610ms/step - loss: 1.7921 - accuracy: 0.4792 - val_loss: 0.7600 - val_accuracy: 0.8233\n",
            "Epoch 17/60\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 1.6794 - accuracy: 0.5025 - val_loss: 0.7240 - val_accuracy: 0.8367\n",
            "Epoch 18/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.6200 - accuracy: 0.5092 - val_loss: 0.6727 - val_accuracy: 0.8533\n",
            "Epoch 19/60\n",
            "38/38 [==============================] - 24s 617ms/step - loss: 1.6058 - accuracy: 0.5133 - val_loss: 0.6621 - val_accuracy: 0.8133\n",
            "Epoch 20/60\n",
            "38/38 [==============================] - 24s 610ms/step - loss: 1.5375 - accuracy: 0.5375 - val_loss: 0.6315 - val_accuracy: 0.8467\n",
            "Epoch 21/60\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 1.4724 - accuracy: 0.5467 - val_loss: 0.5751 - val_accuracy: 0.8433\n",
            "Epoch 22/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.4190 - accuracy: 0.5442 - val_loss: 0.5949 - val_accuracy: 0.8333\n",
            "Epoch 23/60\n",
            "38/38 [==============================] - 24s 612ms/step - loss: 1.4543 - accuracy: 0.5483 - val_loss: 0.5823 - val_accuracy: 0.8300\n",
            "Epoch 24/60\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.3811 - accuracy: 0.5700 - val_loss: 0.5457 - val_accuracy: 0.8367\n",
            "Epoch 25/60\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.3555 - accuracy: 0.5758 - val_loss: 0.5285 - val_accuracy: 0.8333\n",
            "Epoch 26/60\n",
            "38/38 [==============================] - 24s 618ms/step - loss: 1.3177 - accuracy: 0.5817 - val_loss: 0.5058 - val_accuracy: 0.8633\n",
            "Epoch 27/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.3200 - accuracy: 0.5808 - val_loss: 0.4953 - val_accuracy: 0.8567\n",
            "Epoch 28/60\n",
            "38/38 [==============================] - 23s 601ms/step - loss: 1.2675 - accuracy: 0.5933 - val_loss: 0.5240 - val_accuracy: 0.8333\n",
            "Epoch 29/60\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.2520 - accuracy: 0.5975 - val_loss: 0.5008 - val_accuracy: 0.8267\n",
            "Epoch 30/60\n",
            "38/38 [==============================] - 24s 618ms/step - loss: 1.2497 - accuracy: 0.5967 - val_loss: 0.4781 - val_accuracy: 0.8400\n",
            "Epoch 31/60\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 1.2535 - accuracy: 0.5967 - val_loss: 0.4639 - val_accuracy: 0.8500\n",
            "Epoch 32/60\n",
            "38/38 [==============================] - 23s 601ms/step - loss: 1.1818 - accuracy: 0.6200 - val_loss: 0.4795 - val_accuracy: 0.8667\n",
            "Epoch 33/60\n",
            "38/38 [==============================] - 24s 620ms/step - loss: 1.1268 - accuracy: 0.6492 - val_loss: 0.4772 - val_accuracy: 0.8467\n",
            "Epoch 34/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.1359 - accuracy: 0.6267 - val_loss: 0.4600 - val_accuracy: 0.8400\n",
            "Epoch 35/60\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 1.1095 - accuracy: 0.6267 - val_loss: 0.4082 - val_accuracy: 0.8433\n",
            "Epoch 36/60\n",
            "38/38 [==============================] - 24s 619ms/step - loss: 1.0842 - accuracy: 0.6308 - val_loss: 0.4031 - val_accuracy: 0.8600\n",
            "Epoch 37/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.0926 - accuracy: 0.6408 - val_loss: 0.4160 - val_accuracy: 0.8833\n",
            "Epoch 38/60\n",
            "38/38 [==============================] - 23s 601ms/step - loss: 1.0562 - accuracy: 0.6592 - val_loss: 0.4032 - val_accuracy: 0.8533\n",
            "Epoch 39/60\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.0923 - accuracy: 0.6325 - val_loss: 0.4028 - val_accuracy: 0.8767\n",
            "Epoch 40/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.0741 - accuracy: 0.6375 - val_loss: 0.3764 - val_accuracy: 0.8633\n",
            "Epoch 41/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.0389 - accuracy: 0.6517 - val_loss: 0.3699 - val_accuracy: 0.9033\n",
            "Epoch 42/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.0713 - accuracy: 0.6242 - val_loss: 0.3858 - val_accuracy: 0.8600\n",
            "Epoch 43/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.0074 - accuracy: 0.6542 - val_loss: 0.3823 - val_accuracy: 0.8600\n",
            "Epoch 44/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 0.9914 - accuracy: 0.6658 - val_loss: 0.3823 - val_accuracy: 0.8600\n",
            "Epoch 45/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.0051 - accuracy: 0.6617 - val_loss: 0.3875 - val_accuracy: 0.8833\n",
            "Epoch 46/60\n",
            "38/38 [==============================] - 24s 610ms/step - loss: 0.9709 - accuracy: 0.6825 - val_loss: 0.4129 - val_accuracy: 0.8600\n",
            "Epoch 47/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 0.9264 - accuracy: 0.7017 - val_loss: 0.3725 - val_accuracy: 0.8567\n",
            "Epoch 48/60\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 0.9556 - accuracy: 0.6700 - val_loss: 0.3905 - val_accuracy: 0.8633\n",
            "Epoch 49/60\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 0.9140 - accuracy: 0.7067 - val_loss: 0.3738 - val_accuracy: 0.8767\n",
            "Epoch 50/60\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 0.9653 - accuracy: 0.6708 - val_loss: 0.3448 - val_accuracy: 0.8800\n",
            "Epoch 51/60\n",
            "38/38 [==============================] - 24s 610ms/step - loss: 0.9324 - accuracy: 0.6733 - val_loss: 0.3669 - val_accuracy: 0.8600\n",
            "Epoch 52/60\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 0.9509 - accuracy: 0.6675 - val_loss: 0.3810 - val_accuracy: 0.8600\n",
            "Epoch 53/60\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 0.9624 - accuracy: 0.6767 - val_loss: 0.3851 - val_accuracy: 0.8800\n",
            "Epoch 54/60\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 0.9127 - accuracy: 0.6833 - val_loss: 0.3758 - val_accuracy: 0.8700\n",
            "Epoch 55/60\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 0.9498 - accuracy: 0.6800 - val_loss: 0.3556 - val_accuracy: 0.8833\n",
            "Epoch 56/60\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 0.9174 - accuracy: 0.6908 - val_loss: 0.3232 - val_accuracy: 0.8967\n",
            "Epoch 57/60\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 0.9259 - accuracy: 0.6742 - val_loss: 0.3513 - val_accuracy: 0.8600\n",
            "Epoch 58/60\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 0.9401 - accuracy: 0.6800 - val_loss: 0.3671 - val_accuracy: 0.8667\n",
            "Epoch 59/60\n",
            "38/38 [==============================] - 24s 616ms/step - loss: 0.8982 - accuracy: 0.6817 - val_loss: 0.3577 - val_accuracy: 0.8767\n",
            "Epoch 60/60\n",
            "38/38 [==============================] - 24s 616ms/step - loss: 0.9182 - accuracy: 0.6725 - val_loss: 0.3623 - val_accuracy: 0.8600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f11e55e9350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoREEvRYiKNO",
        "outputId": "6177d0be-a565-475c-fbe3-3d3476277c9e"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #60"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 411s 4s/step - loss: 2.6906 - accuracy: 0.3013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6905651092529297, 0.30127209424972534]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYZmRPEfTKhj",
        "outputId": "516a9423-ef47-47ab-8de7-0067f4bbfff7"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 51s 498ms/step - loss: 2.8165 - accuracy: 0.3174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.816490411758423, 0.31740614771842957]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aLCWznFKstr",
        "outputId": "383ec857-97c2-45ac-fc6d-a1fdce86ad1e"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 54s 524ms/step - loss: 2.6981 - accuracy: 0.3162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.69810152053833, 0.3161650598049164]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b223fM1nMsd1",
        "outputId": "02ed8779-aba5-4234-b64b-be24a364c569"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #90"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 53s 522ms/step - loss: 2.7421 - accuracy: 0.3159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7420878410339355, 0.3158547878265381]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhY-z5MOWDh"
      },
      "source": [
        "### Trying VGG19 with different epochs, all layers unfrozen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Fe5wdMOftT"
      },
      "source": [
        "Intially trained model with 70 epochs and kept increasing it by 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiuNKuQiTOQk",
        "outputId": "d7c63f69-3b05-4fd5-cf97-d8c94a12af29"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_13 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_14 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_5 ( (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_15 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_16 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_16 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_16 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_6 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_17 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_17 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_17 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_18 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_18 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_18 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_19 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_19 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_19 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_20 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_20 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_20 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_7 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_21 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_21 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_21 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_22 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_22 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_22 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_23 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_23 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_23 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_24 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_24 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_24 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_8 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_25 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_25 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_25 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_26 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_26 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_26 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_27 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_27 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_27 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_28 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_28 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_28 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_9 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63ArhzxYGrr",
        "outputId": "aed07cbd-f0a9-4871-e160-7ee1c3f0096e"
      },
      "source": [
        "model.fit(training_set, epochs=10, validation_data=validation_set)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 27s 696ms/step - loss: 0.6665 - accuracy: 0.7533 - val_loss: 0.2894 - val_accuracy: 0.8833\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 28s 726ms/step - loss: 0.6628 - accuracy: 0.7517 - val_loss: 0.2871 - val_accuracy: 0.8867\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 29s 758ms/step - loss: 0.6699 - accuracy: 0.7583 - val_loss: 0.3026 - val_accuracy: 0.8767\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 28s 737ms/step - loss: 0.7179 - accuracy: 0.7342 - val_loss: 0.3306 - val_accuracy: 0.8633\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 28s 735ms/step - loss: 0.6666 - accuracy: 0.7525 - val_loss: 0.3101 - val_accuracy: 0.8800\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 29s 743ms/step - loss: 0.6689 - accuracy: 0.7533 - val_loss: 0.3048 - val_accuracy: 0.8767\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 29s 741ms/step - loss: 0.7001 - accuracy: 0.7492 - val_loss: 0.2800 - val_accuracy: 0.8800\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 29s 742ms/step - loss: 0.6941 - accuracy: 0.7375 - val_loss: 0.3017 - val_accuracy: 0.8833\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 29s 747ms/step - loss: 0.7058 - accuracy: 0.7467 - val_loss: 0.2913 - val_accuracy: 0.8767\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 29s 755ms/step - loss: 0.6595 - accuracy: 0.7608 - val_loss: 0.2832 - val_accuracy: 0.8767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66409896d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpUH2X5sRhyM"
      },
      "source": [
        "### Highest accuracy here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRjV2Wp5YKgN",
        "outputId": "ebec5d88-b2b4-402e-de8f-81269241ddd8"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 62s 611ms/step - loss: 2.7316 - accuracy: 0.3428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7316198348999023, 0.3428482711315155]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5LrVTmhYOGA",
        "outputId": "803f2930-86e9-4f62-ff82-988da765bd1a"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 60s 587ms/step - loss: 2.7110 - accuracy: 0.3323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7110321521759033, 0.3322991132736206]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlIpwih42Fgc",
        "outputId": "606b4f5d-173d-4d1f-9062-f2f41b160165"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #90"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 60s 589ms/step - loss: 2.7838 - accuracy: 0.3397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.783827066421509, 0.33974558115005493]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkaOgZ5J50Lt",
        "outputId": "ad0fdd18-3585-43a4-bc95-f7531e307d81"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 61s 595ms/step - loss: 2.8927 - accuracy: 0.3301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.8927271366119385, 0.33012720942497253]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUUifuO0N7ye"
      },
      "source": [
        "### Trying VGG16 with all layers frozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IXMli5Z_jfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c75b95-e96c-4075-b257-e9b443f28df3"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_39 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_39 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_39 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_40 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_40 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_40 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_15  (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_41 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_41 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_41 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_42 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_42 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_42 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_16  (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_43 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_43 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_43 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_44 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_44 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_44 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_45 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_45 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_45 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_17  (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_46 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_46 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_46 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_47 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_47 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_47 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_48 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_48 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_48 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_18  (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_49 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_49 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_49 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_50 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_50 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_50 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_51 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_51 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_51 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_19  (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HqTE66BtodW",
        "outputId": "31e77e90-bf67-468f-a569-8b30197bb08a"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7f1186e972d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1186e979d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1186ee1d50> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f11d031a450> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d02b99d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d0314c50> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f11d0098ed0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d00a2fd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d00e2350> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d0076750> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f11d0098ad0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11981b8ed0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d005c5d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f11d00643d0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f1186e9c950> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1186ff5ed0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1186f484d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1198062390> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f1186f94050> False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN36Ny_utrkf",
        "outputId": "0c7ebd11-498f-4d5f-c52f-067839b98819"
      },
      "source": [
        "model.fit(training_set, epochs=70, validation_data=validation_set)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "38/38 [==============================] - 25s 613ms/step - loss: 4.4464 - accuracy: 0.0308 - val_loss: 2.8212 - val_accuracy: 0.2633\n",
            "Epoch 2/70\n",
            "38/38 [==============================] - 24s 621ms/step - loss: 3.8087 - accuracy: 0.0983 - val_loss: 2.4214 - val_accuracy: 0.4267\n",
            "Epoch 3/70\n",
            "38/38 [==============================] - 23s 603ms/step - loss: 3.4491 - accuracy: 0.1583 - val_loss: 2.2143 - val_accuracy: 0.4933\n",
            "Epoch 4/70\n",
            "38/38 [==============================] - 23s 599ms/step - loss: 3.1583 - accuracy: 0.1983 - val_loss: 1.9939 - val_accuracy: 0.5533\n",
            "Epoch 5/70\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 2.9277 - accuracy: 0.2325 - val_loss: 1.8186 - val_accuracy: 0.5867\n",
            "Epoch 6/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 2.7298 - accuracy: 0.3000 - val_loss: 1.6577 - val_accuracy: 0.6300\n",
            "Epoch 7/70\n",
            "38/38 [==============================] - 24s 621ms/step - loss: 2.6236 - accuracy: 0.3000 - val_loss: 1.4804 - val_accuracy: 0.6433\n",
            "Epoch 8/70\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 2.4591 - accuracy: 0.3350 - val_loss: 1.3028 - val_accuracy: 0.7100\n",
            "Epoch 9/70\n",
            "38/38 [==============================] - 23s 601ms/step - loss: 2.3352 - accuracy: 0.3425 - val_loss: 1.2203 - val_accuracy: 0.7300\n",
            "Epoch 10/70\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 2.2420 - accuracy: 0.3842 - val_loss: 1.1135 - val_accuracy: 0.7567\n",
            "Epoch 11/70\n",
            "38/38 [==============================] - 24s 611ms/step - loss: 2.1155 - accuracy: 0.4133 - val_loss: 0.9987 - val_accuracy: 0.7567\n",
            "Epoch 12/70\n",
            "38/38 [==============================] - 24s 617ms/step - loss: 2.0438 - accuracy: 0.4158 - val_loss: 0.9453 - val_accuracy: 0.7633\n",
            "Epoch 13/70\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.9291 - accuracy: 0.4500 - val_loss: 0.8434 - val_accuracy: 0.8233\n",
            "Epoch 14/70\n",
            "38/38 [==============================] - 23s 604ms/step - loss: 1.8626 - accuracy: 0.4558 - val_loss: 0.8075 - val_accuracy: 0.8300\n",
            "Epoch 15/70\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 1.8457 - accuracy: 0.4692 - val_loss: 0.8320 - val_accuracy: 0.8233\n",
            "Epoch 16/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.7422 - accuracy: 0.4925 - val_loss: 0.7581 - val_accuracy: 0.8100\n",
            "Epoch 17/70\n",
            "38/38 [==============================] - 24s 620ms/step - loss: 1.6596 - accuracy: 0.4875 - val_loss: 0.7527 - val_accuracy: 0.8067\n",
            "Epoch 18/70\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.6037 - accuracy: 0.5075 - val_loss: 0.7164 - val_accuracy: 0.8233\n",
            "Epoch 19/70\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 1.6523 - accuracy: 0.4992 - val_loss: 0.6442 - val_accuracy: 0.8433\n",
            "Epoch 20/70\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.5076 - accuracy: 0.5600 - val_loss: 0.6464 - val_accuracy: 0.8300\n",
            "Epoch 21/70\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.4722 - accuracy: 0.5500 - val_loss: 0.6236 - val_accuracy: 0.8100\n",
            "Epoch 22/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.4567 - accuracy: 0.5433 - val_loss: 0.6180 - val_accuracy: 0.8300\n",
            "Epoch 23/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.4250 - accuracy: 0.5525 - val_loss: 0.5467 - val_accuracy: 0.8533\n",
            "Epoch 24/70\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.3974 - accuracy: 0.5800 - val_loss: 0.5323 - val_accuracy: 0.8500\n",
            "Epoch 25/70\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 1.3575 - accuracy: 0.5817 - val_loss: 0.5420 - val_accuracy: 0.8367\n",
            "Epoch 26/70\n",
            "38/38 [==============================] - 24s 622ms/step - loss: 1.3507 - accuracy: 0.5708 - val_loss: 0.5115 - val_accuracy: 0.8333\n",
            "Epoch 27/70\n",
            "38/38 [==============================] - 24s 604ms/step - loss: 1.3213 - accuracy: 0.5792 - val_loss: 0.4814 - val_accuracy: 0.8500\n",
            "Epoch 28/70\n",
            "38/38 [==============================] - 23s 598ms/step - loss: 1.2769 - accuracy: 0.5858 - val_loss: 0.4793 - val_accuracy: 0.8467\n",
            "Epoch 29/70\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.2326 - accuracy: 0.6025 - val_loss: 0.4665 - val_accuracy: 0.8800\n",
            "Epoch 30/70\n",
            "38/38 [==============================] - 24s 611ms/step - loss: 1.2735 - accuracy: 0.5933 - val_loss: 0.4746 - val_accuracy: 0.8633\n",
            "Epoch 31/70\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.1896 - accuracy: 0.6183 - val_loss: 0.4388 - val_accuracy: 0.8533\n",
            "Epoch 32/70\n",
            "38/38 [==============================] - 23s 604ms/step - loss: 1.1711 - accuracy: 0.6142 - val_loss: 0.4389 - val_accuracy: 0.8700\n",
            "Epoch 33/70\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.1544 - accuracy: 0.6108 - val_loss: 0.4164 - val_accuracy: 0.8933\n",
            "Epoch 34/70\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 1.1023 - accuracy: 0.6400 - val_loss: 0.4428 - val_accuracy: 0.8500\n",
            "Epoch 35/70\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 1.1344 - accuracy: 0.6258 - val_loss: 0.4526 - val_accuracy: 0.8500\n",
            "Epoch 36/70\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 1.1462 - accuracy: 0.6125 - val_loss: 0.4605 - val_accuracy: 0.8500\n",
            "Epoch 37/70\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 1.1272 - accuracy: 0.6300 - val_loss: 0.4303 - val_accuracy: 0.8367\n",
            "Epoch 38/70\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 1.0931 - accuracy: 0.6317 - val_loss: 0.4343 - val_accuracy: 0.8600\n",
            "Epoch 39/70\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 1.0219 - accuracy: 0.6692 - val_loss: 0.4029 - val_accuracy: 0.8400\n",
            "Epoch 40/70\n",
            "38/38 [==============================] - 24s 612ms/step - loss: 1.0615 - accuracy: 0.6433 - val_loss: 0.4017 - val_accuracy: 0.8233\n",
            "Epoch 41/70\n",
            "38/38 [==============================] - 24s 613ms/step - loss: 1.0386 - accuracy: 0.6483 - val_loss: 0.4226 - val_accuracy: 0.8500\n",
            "Epoch 42/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.0204 - accuracy: 0.6500 - val_loss: 0.4013 - val_accuracy: 0.8333\n",
            "Epoch 43/70\n",
            "38/38 [==============================] - 24s 618ms/step - loss: 1.0115 - accuracy: 0.6650 - val_loss: 0.3701 - val_accuracy: 0.8700\n",
            "Epoch 44/70\n",
            "38/38 [==============================] - 24s 605ms/step - loss: 1.0331 - accuracy: 0.6550 - val_loss: 0.3753 - val_accuracy: 0.8600\n",
            "Epoch 45/70\n",
            "38/38 [==============================] - 24s 619ms/step - loss: 0.9873 - accuracy: 0.6650 - val_loss: 0.3719 - val_accuracy: 0.8600\n",
            "Epoch 46/70\n",
            "38/38 [==============================] - 24s 606ms/step - loss: 0.9617 - accuracy: 0.6700 - val_loss: 0.3594 - val_accuracy: 0.8733\n",
            "Epoch 47/70\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 0.9611 - accuracy: 0.6717 - val_loss: 0.3959 - val_accuracy: 0.8500\n",
            "Epoch 48/70\n",
            "38/38 [==============================] - 24s 621ms/step - loss: 1.0318 - accuracy: 0.6508 - val_loss: 0.3813 - val_accuracy: 0.8900\n",
            "Epoch 49/70\n",
            "38/38 [==============================] - 24s 609ms/step - loss: 0.9623 - accuracy: 0.6767 - val_loss: 0.3796 - val_accuracy: 0.8633\n",
            "Epoch 50/70\n",
            "38/38 [==============================] - 23s 603ms/step - loss: 0.9434 - accuracy: 0.6625 - val_loss: 0.3720 - val_accuracy: 0.8633\n",
            "Epoch 51/70\n",
            "38/38 [==============================] - 24s 614ms/step - loss: 0.9360 - accuracy: 0.6700 - val_loss: 0.3608 - val_accuracy: 0.8767\n",
            "Epoch 52/70\n",
            "38/38 [==============================] - 24s 623ms/step - loss: 0.9123 - accuracy: 0.6758 - val_loss: 0.3627 - val_accuracy: 0.8600\n",
            "Epoch 53/70\n",
            "38/38 [==============================] - 23s 596ms/step - loss: 0.8993 - accuracy: 0.6808 - val_loss: 0.3554 - val_accuracy: 0.9067\n",
            "Epoch 54/70\n",
            "38/38 [==============================] - 24s 608ms/step - loss: 0.8757 - accuracy: 0.6858 - val_loss: 0.3361 - val_accuracy: 0.8833\n",
            "Epoch 55/70\n",
            "38/38 [==============================] - 23s 599ms/step - loss: 0.8886 - accuracy: 0.6867 - val_loss: 0.3160 - val_accuracy: 0.8900\n",
            "Epoch 56/70\n",
            "38/38 [==============================] - 23s 590ms/step - loss: 0.8768 - accuracy: 0.6858 - val_loss: 0.3347 - val_accuracy: 0.9033\n",
            "Epoch 57/70\n",
            "38/38 [==============================] - 23s 591ms/step - loss: 0.8871 - accuracy: 0.6867 - val_loss: 0.3186 - val_accuracy: 0.8867\n",
            "Epoch 58/70\n",
            "38/38 [==============================] - 24s 611ms/step - loss: 0.8953 - accuracy: 0.6917 - val_loss: 0.3339 - val_accuracy: 0.8900\n",
            "Epoch 59/70\n",
            "38/38 [==============================] - 24s 619ms/step - loss: 0.8401 - accuracy: 0.7142 - val_loss: 0.3348 - val_accuracy: 0.8633\n",
            "Epoch 60/70\n",
            "38/38 [==============================] - 24s 607ms/step - loss: 0.8554 - accuracy: 0.6883 - val_loss: 0.3341 - val_accuracy: 0.8733\n",
            "Epoch 61/70\n",
            "38/38 [==============================] - 23s 603ms/step - loss: 0.8114 - accuracy: 0.7025 - val_loss: 0.3128 - val_accuracy: 0.8900\n",
            "Epoch 62/70\n",
            "38/38 [==============================] - 24s 612ms/step - loss: 0.8243 - accuracy: 0.7033 - val_loss: 0.3178 - val_accuracy: 0.8967\n",
            "Epoch 63/70\n",
            "38/38 [==============================] - 23s 593ms/step - loss: 0.8319 - accuracy: 0.6992 - val_loss: 0.3053 - val_accuracy: 0.8900\n",
            "Epoch 64/70\n",
            "38/38 [==============================] - 23s 583ms/step - loss: 0.8559 - accuracy: 0.6892 - val_loss: 0.3160 - val_accuracy: 0.8933\n",
            "Epoch 65/70\n",
            "38/38 [==============================] - 23s 589ms/step - loss: 0.8129 - accuracy: 0.7008 - val_loss: 0.3486 - val_accuracy: 0.8767\n",
            "Epoch 66/70\n",
            "38/38 [==============================] - 23s 590ms/step - loss: 0.8175 - accuracy: 0.7033 - val_loss: 0.3370 - val_accuracy: 0.8767\n",
            "Epoch 67/70\n",
            "38/38 [==============================] - 23s 591ms/step - loss: 0.8387 - accuracy: 0.7008 - val_loss: 0.3269 - val_accuracy: 0.8633\n",
            "Epoch 68/70\n",
            "38/38 [==============================] - 23s 597ms/step - loss: 0.8251 - accuracy: 0.6975 - val_loss: 0.3398 - val_accuracy: 0.8467\n",
            "Epoch 69/70\n",
            "38/38 [==============================] - 24s 611ms/step - loss: 0.8394 - accuracy: 0.7125 - val_loss: 0.3354 - val_accuracy: 0.8567\n",
            "Epoch 70/70\n",
            "38/38 [==============================] - 23s 602ms/step - loss: 0.7669 - accuracy: 0.7208 - val_loss: 0.3442 - val_accuracy: 0.8633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1186f48610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqqD5DTivIis",
        "outputId": "f27a6c18-040e-4b6f-b4f0-b3ce0376b169"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #70"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 50s 488ms/step - loss: 2.7288 - accuracy: 0.3118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7287697792053223, 0.3118212819099426]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVbGeb5OLrD"
      },
      "source": [
        "### Trying VGG19 with all layers frozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvsKW-QRvUpk",
        "outputId": "c392150e-3dcf-4b52-d70d-04db05afd634"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "80150528/80134624 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_55), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_55), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_56), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_56), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_57), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_57), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_58), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_58), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_59), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_59), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_60), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_60), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_61), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_61), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_62), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_62), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_63), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_63), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_64), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_64), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_65), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_65), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_66), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_66), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_67), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_67), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_52 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_52 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_52 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_53 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_53 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_53 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_20  (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_54 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_54 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_54 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_55 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_55 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_55 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_21  (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_56 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_56 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_56 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_57 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_57 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_57 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_58 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_58 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_58 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_59 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_59 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_59 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_22  (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_60 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_60 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_60 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_61 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_61 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_61 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_62 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_62 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_62 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_63 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_63 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_63 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_23  (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_64 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_64 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_64 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_65 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_65 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_65 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_66 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_66 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_66 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_67 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_67 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_67 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_24  (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Iv81GevicB",
        "outputId": "ffcee2de-fdee-46ee-bf6c-f29a1273b0b3"
      },
      "source": [
        "model.fit(training_set, epochs=70, validation_data=validation_set)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "38/38 [==============================] - 27s 687ms/step - loss: 4.3503 - accuracy: 0.0342 - val_loss: 3.1262 - val_accuracy: 0.2600\n",
            "Epoch 2/70\n",
            "38/38 [==============================] - 26s 682ms/step - loss: 3.8167 - accuracy: 0.0825 - val_loss: 2.4633 - val_accuracy: 0.4533\n",
            "Epoch 3/70\n",
            "38/38 [==============================] - 27s 689ms/step - loss: 3.4653 - accuracy: 0.1542 - val_loss: 2.1619 - val_accuracy: 0.5333\n",
            "Epoch 4/70\n",
            "38/38 [==============================] - 28s 717ms/step - loss: 3.1669 - accuracy: 0.2017 - val_loss: 1.9748 - val_accuracy: 0.5667\n",
            "Epoch 5/70\n",
            "38/38 [==============================] - 28s 718ms/step - loss: 2.9550 - accuracy: 0.2275 - val_loss: 1.6929 - val_accuracy: 0.6667\n",
            "Epoch 6/70\n",
            "38/38 [==============================] - 27s 706ms/step - loss: 2.7984 - accuracy: 0.2608 - val_loss: 1.4983 - val_accuracy: 0.6800\n",
            "Epoch 7/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 2.6175 - accuracy: 0.3125 - val_loss: 1.4255 - val_accuracy: 0.6800\n",
            "Epoch 8/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 2.5041 - accuracy: 0.3183 - val_loss: 1.3198 - val_accuracy: 0.7067\n",
            "Epoch 9/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 2.3636 - accuracy: 0.3542 - val_loss: 1.2196 - val_accuracy: 0.7000\n",
            "Epoch 10/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 2.2522 - accuracy: 0.3792 - val_loss: 1.1120 - val_accuracy: 0.7233\n",
            "Epoch 11/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 2.1317 - accuracy: 0.4017 - val_loss: 1.0700 - val_accuracy: 0.7167\n",
            "Epoch 12/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 2.0471 - accuracy: 0.4108 - val_loss: 0.9932 - val_accuracy: 0.7433\n",
            "Epoch 13/70\n",
            "38/38 [==============================] - 28s 726ms/step - loss: 1.9698 - accuracy: 0.4383 - val_loss: 0.9456 - val_accuracy: 0.7400\n",
            "Epoch 14/70\n",
            "38/38 [==============================] - 27s 707ms/step - loss: 1.8866 - accuracy: 0.4533 - val_loss: 0.8973 - val_accuracy: 0.7433\n",
            "Epoch 15/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 1.8335 - accuracy: 0.4600 - val_loss: 0.8725 - val_accuracy: 0.7467\n",
            "Epoch 16/70\n",
            "38/38 [==============================] - 28s 717ms/step - loss: 1.7694 - accuracy: 0.4775 - val_loss: 0.7976 - val_accuracy: 0.8067\n",
            "Epoch 17/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 1.7504 - accuracy: 0.4750 - val_loss: 0.7656 - val_accuracy: 0.7933\n",
            "Epoch 18/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 1.6227 - accuracy: 0.5167 - val_loss: 0.7137 - val_accuracy: 0.8100\n",
            "Epoch 19/70\n",
            "38/38 [==============================] - 28s 716ms/step - loss: 1.6383 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.8167\n",
            "Epoch 20/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 1.5845 - accuracy: 0.5175 - val_loss: 0.6919 - val_accuracy: 0.8033\n",
            "Epoch 21/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 1.5501 - accuracy: 0.5325 - val_loss: 0.6591 - val_accuracy: 0.8133\n",
            "Epoch 22/70\n",
            "38/38 [==============================] - 28s 727ms/step - loss: 1.4913 - accuracy: 0.5375 - val_loss: 0.6567 - val_accuracy: 0.8000\n",
            "Epoch 23/70\n",
            "38/38 [==============================] - 27s 704ms/step - loss: 1.3759 - accuracy: 0.5758 - val_loss: 0.5986 - val_accuracy: 0.8200\n",
            "Epoch 24/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 1.4212 - accuracy: 0.5333 - val_loss: 0.6108 - val_accuracy: 0.8100\n",
            "Epoch 25/70\n",
            "38/38 [==============================] - 28s 717ms/step - loss: 1.3795 - accuracy: 0.5675 - val_loss: 0.5945 - val_accuracy: 0.8200\n",
            "Epoch 26/70\n",
            "38/38 [==============================] - 27s 708ms/step - loss: 1.2927 - accuracy: 0.5942 - val_loss: 0.5524 - val_accuracy: 0.8200\n",
            "Epoch 27/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 1.2962 - accuracy: 0.5900 - val_loss: 0.5215 - val_accuracy: 0.8433\n",
            "Epoch 28/70\n",
            "38/38 [==============================] - 28s 724ms/step - loss: 1.2416 - accuracy: 0.6050 - val_loss: 0.5263 - val_accuracy: 0.8067\n",
            "Epoch 29/70\n",
            "38/38 [==============================] - 27s 704ms/step - loss: 1.2230 - accuracy: 0.5983 - val_loss: 0.4868 - val_accuracy: 0.8500\n",
            "Epoch 30/70\n",
            "38/38 [==============================] - 28s 721ms/step - loss: 1.2079 - accuracy: 0.6042 - val_loss: 0.5396 - val_accuracy: 0.8433\n",
            "Epoch 31/70\n",
            "38/38 [==============================] - 27s 707ms/step - loss: 1.1739 - accuracy: 0.6042 - val_loss: 0.5046 - val_accuracy: 0.8233\n",
            "Epoch 32/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 1.1733 - accuracy: 0.6067 - val_loss: 0.4787 - val_accuracy: 0.8367\n",
            "Epoch 33/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 1.1363 - accuracy: 0.6275 - val_loss: 0.4457 - val_accuracy: 0.8400\n",
            "Epoch 34/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 1.1190 - accuracy: 0.6333 - val_loss: 0.4647 - val_accuracy: 0.8400\n",
            "Epoch 35/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 1.1014 - accuracy: 0.6350 - val_loss: 0.4379 - val_accuracy: 0.8433\n",
            "Epoch 36/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 1.0878 - accuracy: 0.6433 - val_loss: 0.4481 - val_accuracy: 0.8367\n",
            "Epoch 37/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 1.1055 - accuracy: 0.6475 - val_loss: 0.4443 - val_accuracy: 0.8533\n",
            "Epoch 38/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 1.0796 - accuracy: 0.6383 - val_loss: 0.4145 - val_accuracy: 0.8433\n",
            "Epoch 39/70\n",
            "38/38 [==============================] - 27s 709ms/step - loss: 1.0451 - accuracy: 0.6508 - val_loss: 0.4176 - val_accuracy: 0.8467\n",
            "Epoch 40/70\n",
            "38/38 [==============================] - 27s 710ms/step - loss: 1.0483 - accuracy: 0.6275 - val_loss: 0.3945 - val_accuracy: 0.8767\n",
            "Epoch 41/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 1.0351 - accuracy: 0.6592 - val_loss: 0.4289 - val_accuracy: 0.8567\n",
            "Epoch 42/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 0.9752 - accuracy: 0.6700 - val_loss: 0.3990 - val_accuracy: 0.8800\n",
            "Epoch 43/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 0.9892 - accuracy: 0.6500 - val_loss: 0.3872 - val_accuracy: 0.8467\n",
            "Epoch 44/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 1.0039 - accuracy: 0.6700 - val_loss: 0.3716 - val_accuracy: 0.8700\n",
            "Epoch 45/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 0.9818 - accuracy: 0.6608 - val_loss: 0.3674 - val_accuracy: 0.8667\n",
            "Epoch 46/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 1.0058 - accuracy: 0.6658 - val_loss: 0.3685 - val_accuracy: 0.8700\n",
            "Epoch 47/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 0.9106 - accuracy: 0.7083 - val_loss: 0.3703 - val_accuracy: 0.8700\n",
            "Epoch 48/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 0.9447 - accuracy: 0.6792 - val_loss: 0.3754 - val_accuracy: 0.8700\n",
            "Epoch 49/70\n",
            "38/38 [==============================] - 28s 718ms/step - loss: 0.9009 - accuracy: 0.6850 - val_loss: 0.3900 - val_accuracy: 0.8700\n",
            "Epoch 50/70\n",
            "38/38 [==============================] - 28s 720ms/step - loss: 0.9532 - accuracy: 0.6908 - val_loss: 0.3525 - val_accuracy: 0.8767\n",
            "Epoch 51/70\n",
            "38/38 [==============================] - 27s 705ms/step - loss: 0.9445 - accuracy: 0.6683 - val_loss: 0.3699 - val_accuracy: 0.8733\n",
            "Epoch 52/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 0.9210 - accuracy: 0.6925 - val_loss: 0.3444 - val_accuracy: 0.8900\n",
            "Epoch 53/70\n",
            "38/38 [==============================] - 28s 715ms/step - loss: 0.9405 - accuracy: 0.6683 - val_loss: 0.3355 - val_accuracy: 0.8733\n",
            "Epoch 54/70\n",
            "38/38 [==============================] - 27s 710ms/step - loss: 0.9063 - accuracy: 0.6892 - val_loss: 0.3379 - val_accuracy: 0.8733\n",
            "Epoch 55/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 0.9300 - accuracy: 0.6583 - val_loss: 0.3438 - val_accuracy: 0.8667\n",
            "Epoch 56/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 0.8605 - accuracy: 0.6783 - val_loss: 0.3497 - val_accuracy: 0.8667\n",
            "Epoch 57/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 0.8612 - accuracy: 0.7033 - val_loss: 0.3319 - val_accuracy: 0.8700\n",
            "Epoch 58/70\n",
            "38/38 [==============================] - 28s 711ms/step - loss: 0.8488 - accuracy: 0.6983 - val_loss: 0.3385 - val_accuracy: 0.8833\n",
            "Epoch 59/70\n",
            "38/38 [==============================] - 28s 730ms/step - loss: 0.8409 - accuracy: 0.7117 - val_loss: 0.3036 - val_accuracy: 0.8833\n",
            "Epoch 60/70\n",
            "38/38 [==============================] - 27s 706ms/step - loss: 0.8116 - accuracy: 0.7108 - val_loss: 0.3482 - val_accuracy: 0.8733\n",
            "Epoch 61/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 0.8212 - accuracy: 0.7100 - val_loss: 0.3358 - val_accuracy: 0.8733\n",
            "Epoch 62/70\n",
            "38/38 [==============================] - 28s 721ms/step - loss: 0.7975 - accuracy: 0.7125 - val_loss: 0.3503 - val_accuracy: 0.8733\n",
            "Epoch 63/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 0.7888 - accuracy: 0.7383 - val_loss: 0.2981 - val_accuracy: 0.8833\n",
            "Epoch 64/70\n",
            "38/38 [==============================] - 28s 710ms/step - loss: 0.7883 - accuracy: 0.7125 - val_loss: 0.3204 - val_accuracy: 0.8900\n",
            "Epoch 65/70\n",
            "38/38 [==============================] - 28s 715ms/step - loss: 0.8264 - accuracy: 0.7042 - val_loss: 0.3253 - val_accuracy: 0.8900\n",
            "Epoch 66/70\n",
            "38/38 [==============================] - 28s 716ms/step - loss: 0.8200 - accuracy: 0.6883 - val_loss: 0.3230 - val_accuracy: 0.8533\n",
            "Epoch 67/70\n",
            "38/38 [==============================] - 28s 713ms/step - loss: 0.8148 - accuracy: 0.7158 - val_loss: 0.2982 - val_accuracy: 0.8633\n",
            "Epoch 68/70\n",
            "38/38 [==============================] - 27s 705ms/step - loss: 0.7903 - accuracy: 0.7158 - val_loss: 0.2999 - val_accuracy: 0.8867\n",
            "Epoch 69/70\n",
            "38/38 [==============================] - 28s 712ms/step - loss: 0.8009 - accuracy: 0.7267 - val_loss: 0.2932 - val_accuracy: 0.8933\n",
            "Epoch 70/70\n",
            "38/38 [==============================] - 28s 714ms/step - loss: 0.7556 - accuracy: 0.7383 - val_loss: 0.3009 - val_accuracy: 0.8667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f11863cb590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2JZ5qbevk3y",
        "outputId": "82ec499c-eb85-4097-80db-33da81e41adf"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #70"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 59s 579ms/step - loss: 2.7424 - accuracy: 0.3124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7424423694610596, 0.3124418258666992]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_7HgovIPWWx"
      },
      "source": [
        "### Tried ResNet50 for a few different things but training took way longer to get similar results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX36I7Sjvp2j"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# load model without classifier layers\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size, img_size, channels))\n",
        "\n",
        "# # freeze layers\n",
        "# for layer in model.layers[:45]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = keras.layers.Flatten()(model.layers[-1].output)\n",
        "X = keras.layers.Dense(128, activation='relu')(flat1)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "output = keras.layers.Dense(64, activation='softmax')(X)\n",
        "\n",
        "# define new model\n",
        "model = keras.models.Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "# summarize\n",
        "model.summary()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8pvEw5vP3l3",
        "outputId": "37e02fbf-7893-4b4f-822c-e9579ab71596"
      },
      "source": [
        "history = model.fit(training_set, epochs=40, verbose=2, validation_data=validation_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "38/38 - 56s - loss: 4.4302 - accuracy: 0.0217 - val_loss: 1477.0565 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/40\n",
            "38/38 - 40s - loss: 4.4103 - accuracy: 0.0175 - val_loss: 142.7556 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/40\n",
            "38/38 - 40s - loss: 4.4334 - accuracy: 0.0233 - val_loss: 376.6001 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/40\n",
            "38/38 - 40s - loss: 4.3460 - accuracy: 0.0200 - val_loss: 13.9761 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/40\n",
            "38/38 - 40s - loss: 4.3587 - accuracy: 0.0200 - val_loss: 152.2706 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/40\n",
            "38/38 - 40s - loss: 3.9056 - accuracy: 0.0675 - val_loss: 159.3686 - val_accuracy: 0.0667\n",
            "Epoch 7/40\n",
            "38/38 - 40s - loss: 3.5100 - accuracy: 0.1300 - val_loss: 16.6092 - val_accuracy: 0.0700\n",
            "Epoch 8/40\n",
            "38/38 - 40s - loss: 3.3215 - accuracy: 0.1450 - val_loss: 34.0470 - val_accuracy: 0.0700\n",
            "Epoch 9/40\n",
            "38/38 - 40s - loss: 3.1233 - accuracy: 0.1808 - val_loss: 7.3462 - val_accuracy: 0.0767\n",
            "Epoch 10/40\n",
            "38/38 - 40s - loss: 2.8063 - accuracy: 0.2458 - val_loss: 2.2829 - val_accuracy: 0.4467\n",
            "Epoch 11/40\n",
            "38/38 - 40s - loss: 2.6453 - accuracy: 0.2733 - val_loss: 2.1401 - val_accuracy: 0.5800\n",
            "Epoch 12/40\n",
            "38/38 - 40s - loss: 2.4224 - accuracy: 0.3217 - val_loss: 2.3010 - val_accuracy: 0.6300\n",
            "Epoch 13/40\n",
            "38/38 - 40s - loss: 2.2907 - accuracy: 0.3658 - val_loss: 2.5074 - val_accuracy: 0.5967\n",
            "Epoch 14/40\n",
            "38/38 - 40s - loss: 2.2973 - accuracy: 0.3408 - val_loss: 4.1288 - val_accuracy: 0.0300\n",
            "Epoch 15/40\n",
            "38/38 - 40s - loss: 2.2907 - accuracy: 0.3425 - val_loss: 2.7651 - val_accuracy: 0.2267\n",
            "Epoch 16/40\n",
            "38/38 - 40s - loss: 2.0665 - accuracy: 0.3950 - val_loss: 1.6965 - val_accuracy: 0.6733\n",
            "Epoch 17/40\n",
            "38/38 - 40s - loss: 1.8069 - accuracy: 0.4900 - val_loss: 1.8997 - val_accuracy: 0.6833\n",
            "Epoch 18/40\n",
            "38/38 - 40s - loss: 1.6411 - accuracy: 0.5267 - val_loss: 1.6126 - val_accuracy: 0.6033\n",
            "Epoch 19/40\n",
            "38/38 - 40s - loss: 2.0415 - accuracy: 0.3958 - val_loss: 4.7146 - val_accuracy: 0.1633\n",
            "Epoch 20/40\n",
            "38/38 - 40s - loss: 1.6406 - accuracy: 0.5208 - val_loss: 1.9038 - val_accuracy: 0.5200\n",
            "Epoch 21/40\n",
            "38/38 - 40s - loss: 1.4404 - accuracy: 0.5775 - val_loss: 1.2862 - val_accuracy: 0.7367\n",
            "Epoch 22/40\n",
            "38/38 - 40s - loss: 1.2884 - accuracy: 0.6383 - val_loss: 1.1176 - val_accuracy: 0.7533\n",
            "Epoch 23/40\n",
            "38/38 - 40s - loss: 1.1730 - accuracy: 0.6808 - val_loss: 0.8716 - val_accuracy: 0.7433\n",
            "Epoch 24/40\n",
            "38/38 - 40s - loss: 1.0627 - accuracy: 0.7025 - val_loss: 0.9206 - val_accuracy: 0.7300\n",
            "Epoch 25/40\n",
            "38/38 - 40s - loss: 1.0494 - accuracy: 0.6875 - val_loss: 0.6384 - val_accuracy: 0.7633\n",
            "Epoch 26/40\n",
            "38/38 - 40s - loss: 2.0545 - accuracy: 0.4567 - val_loss: 2.5915 - val_accuracy: 0.3467\n",
            "Epoch 27/40\n",
            "38/38 - 40s - loss: 1.9896 - accuracy: 0.4200 - val_loss: 20.3623 - val_accuracy: 0.0700\n",
            "Epoch 28/40\n",
            "38/38 - 40s - loss: 1.2956 - accuracy: 0.5942 - val_loss: 2.0780 - val_accuracy: 0.4933\n",
            "Epoch 29/40\n",
            "38/38 - 40s - loss: 1.3161 - accuracy: 0.5925 - val_loss: 5.2389 - val_accuracy: 0.1000\n",
            "Epoch 30/40\n",
            "38/38 - 40s - loss: 1.0890 - accuracy: 0.6517 - val_loss: 1.1153 - val_accuracy: 0.7267\n",
            "Epoch 31/40\n",
            "38/38 - 40s - loss: 0.8653 - accuracy: 0.7425 - val_loss: 0.5683 - val_accuracy: 0.8233\n",
            "Epoch 32/40\n",
            "38/38 - 40s - loss: 0.8106 - accuracy: 0.7608 - val_loss: 0.4949 - val_accuracy: 0.8200\n",
            "Epoch 33/40\n",
            "38/38 - 40s - loss: 0.7120 - accuracy: 0.7992 - val_loss: 0.5168 - val_accuracy: 0.8300\n",
            "Epoch 34/40\n",
            "38/38 - 40s - loss: 0.6721 - accuracy: 0.8050 - val_loss: 0.4826 - val_accuracy: 0.8333\n",
            "Epoch 35/40\n",
            "38/38 - 40s - loss: 0.6255 - accuracy: 0.8075 - val_loss: 0.4305 - val_accuracy: 0.8733\n",
            "Epoch 36/40\n",
            "38/38 - 40s - loss: 0.5844 - accuracy: 0.8250 - val_loss: 0.4271 - val_accuracy: 0.8533\n",
            "Epoch 37/40\n",
            "38/38 - 40s - loss: 0.4998 - accuracy: 0.8433 - val_loss: 0.3213 - val_accuracy: 0.8667\n",
            "Epoch 38/40\n",
            "38/38 - 40s - loss: 0.5211 - accuracy: 0.8308 - val_loss: 0.3496 - val_accuracy: 0.8633\n",
            "Epoch 39/40\n",
            "38/38 - 40s - loss: 0.5562 - accuracy: 0.8208 - val_loss: 0.5579 - val_accuracy: 0.7367\n",
            "Epoch 40/40\n",
            "38/38 - 40s - loss: 0.5762 - accuracy: 0.8100 - val_loss: 0.3393 - val_accuracy: 0.8533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZHpZBAZP3l4",
        "outputId": "779975cf-bf6d-4bc3-8f3e-a9187edc66b0"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101/101 [==============================] - 43s 420ms/step - loss: 3.4276 - accuracy: 0.2650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.4275729656219482, 0.2649705111980438]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}