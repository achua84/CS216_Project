{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixing_train_and_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCrqUZLXOyli"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqYr2-DyRzsv",
        "outputId": "6cd87372-84b3-4968-846c-121dc40eb337"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMdCkuVqRwHt"
      },
      "source": [
        "# 64 classes of optotypes. Should correspond to the folder names under images/testing and images/training\n",
        "labels = [\"+blank\", \"+circle\", \"+diamond\", \"+square\", \"2\", \"3\", \"5\", \"6\", \"8\", \"9\", \"apple\", \"bird\", \"C\", \"C-0\", \"C-45\",\n",
        "          \"C-90\", \"C-135\", \"C-180\", \"C-225\", \"C-270\", \"C-315\", \"cake\", \"car\", \"circle\", \"cow\", \"cup\", \"D\",\n",
        "          \"duck\", \"E\", \"E-0\", \"E-90\", \"E-180\", \"E-270\", \"F\", \"flat-line\", \"flat-square\",\n",
        "          \"frown-line\", \"frown-square\", \"H\", \"hand\", \"horse\", \"house\", \"K\", \"L\", \"N\", \"O\", \"P\",\n",
        "          \"panda\", \"phone\", \"R\", \"S\", \"smile-line\", \"smile-square\", \"square\", \"star\", \"T\", \"train\",\n",
        "          \"tree\", \"V\", \"x-blank\", \"x-circle\", \"x-diamond\", \"x-square\", \"Z\"]\n",
        "img_size = 400\n",
        "channels = 3\n",
        "training_dir = 'drive/MyDrive/opt_images/training'\n",
        "testing_dir = 'drive/MyDrive/opt_images/testing'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy3x4HwHR18Q"
      },
      "source": [
        "def create_datasets_test_train():\n",
        "    \"\"\"\n",
        "    Generates two `tf.data.Dataset` from image files in the project.\n",
        "    NOTE: images must be put under project root in directory images/testing and images/training\n",
        "    :return: Two `tf.data.Dataset` objects, one for testing and one for training.\n",
        "    \"\"\"\n",
        "    training_set = image_dataset_from_directory(training_dir,\n",
        "                                                shuffle=True,\n",
        "                                                batch_size=32,\n",
        "                                                image_size=(img_size, img_size))\n",
        "    testing_set = image_dataset_from_directory(testing_dir,\n",
        "                                               shuffle=True,\n",
        "                                               batch_size=32,\n",
        "                                               image_size=(img_size, img_size))\n",
        "    return training_set, testing_set"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM_xIpWrR62y",
        "outputId": "2d9d5b35-2d61-4881-e856-5c17b49e4c06"
      },
      "source": [
        "training_set, testing_set = create_datasets_test_train()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1500 files belonging to 64 classes.\n",
            "Found 3223 files belonging to 64 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngJfl1siSKY1"
      },
      "source": [
        "test_len = len(testing_set)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViKEmxWdbVq4",
        "outputId": "f61bda91-195d-4214-8082-520434ff3614"
      },
      "source": [
        "print(test_len)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdw67OYiQj02"
      },
      "source": [
        "### Mixing Train and Test Data (50% of test data into train, 25% of test for valid and 25% for test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdl5Kps6SAKl"
      },
      "source": [
        "set1 = testing_set.take(test_len // 2)\n",
        "temp = testing_set.skip(test_len // 2)\n",
        "valid_set = temp.skip(test_len // 4)\n",
        "testing_set = temp.take(test_len // 4)\n",
        "training_set = training_set.concatenate(set1)\n",
        "training_set = training_set.shuffle(buffer_size=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1s0V5wea51N",
        "outputId": "a2f74219-7f6d-4882-ed5b-56a0a1ac8cf6"
      },
      "source": [
        "print(len(training_set))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTWQHp_QQuu_"
      },
      "source": [
        "### Trying dataset with VGG16 at different epochs, starting at 40 and increasing by 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii90c-Z8XL_T",
        "outputId": "4407711c-6996-489e-aa68-539846b79b20"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Scd24NXPA_",
        "outputId": "da6d30e0-5201-4f72-93c8-a9e56c126e5e"
      },
      "source": [
        "model.fit(training_set, epochs=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "97/97 [==============================] - 181s 2s/step - loss: 4.2183 - accuracy: 0.0423\n",
            "Epoch 2/40\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 3.5192 - accuracy: 0.1384\n",
            "Epoch 3/40\n",
            "97/97 [==============================] - 54s 550ms/step - loss: 3.1598 - accuracy: 0.1816\n",
            "Epoch 4/40\n",
            "97/97 [==============================] - 54s 547ms/step - loss: 2.8825 - accuracy: 0.2306\n",
            "Epoch 5/40\n",
            "97/97 [==============================] - 53s 543ms/step - loss: 2.6623 - accuracy: 0.2677\n",
            "Epoch 6/40\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 2.4989 - accuracy: 0.3029\n",
            "Epoch 7/40\n",
            "97/97 [==============================] - 54s 550ms/step - loss: 2.3407 - accuracy: 0.3365\n",
            "Epoch 8/40\n",
            "97/97 [==============================] - 55s 564ms/step - loss: 2.2498 - accuracy: 0.3468\n",
            "Epoch 9/40\n",
            "97/97 [==============================] - 54s 548ms/step - loss: 2.1254 - accuracy: 0.3823\n",
            "Epoch 10/40\n",
            "97/97 [==============================] - 54s 555ms/step - loss: 2.0834 - accuracy: 0.3829\n",
            "Epoch 11/40\n",
            "97/97 [==============================] - 54s 545ms/step - loss: 2.0099 - accuracy: 0.3935\n",
            "Epoch 12/40\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 1.9452 - accuracy: 0.4142\n",
            "Epoch 13/40\n",
            "97/97 [==============================] - 54s 550ms/step - loss: 1.8949 - accuracy: 0.4248\n",
            "Epoch 14/40\n",
            "97/97 [==============================] - 53s 542ms/step - loss: 1.8339 - accuracy: 0.4352\n",
            "Epoch 15/40\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.7953 - accuracy: 0.4542\n",
            "Epoch 16/40\n",
            "97/97 [==============================] - 53s 535ms/step - loss: 1.7623 - accuracy: 0.4442\n",
            "Epoch 17/40\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.7150 - accuracy: 0.4587\n",
            "Epoch 18/40\n",
            "97/97 [==============================] - 53s 542ms/step - loss: 1.7114 - accuracy: 0.4726\n",
            "Epoch 19/40\n",
            "97/97 [==============================] - 54s 548ms/step - loss: 1.6885 - accuracy: 0.4665\n",
            "Epoch 20/40\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 1.6520 - accuracy: 0.4800\n",
            "Epoch 21/40\n",
            "97/97 [==============================] - 55s 564ms/step - loss: 1.6312 - accuracy: 0.4729\n",
            "Epoch 22/40\n",
            "97/97 [==============================] - 55s 559ms/step - loss: 1.5980 - accuracy: 0.4935\n",
            "Epoch 23/40\n",
            "97/97 [==============================] - 54s 554ms/step - loss: 1.5628 - accuracy: 0.4923\n",
            "Epoch 24/40\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.5979 - accuracy: 0.4806\n",
            "Epoch 25/40\n",
            "97/97 [==============================] - 52s 526ms/step - loss: 1.5145 - accuracy: 0.5006\n",
            "Epoch 26/40\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.5227 - accuracy: 0.5081\n",
            "Epoch 27/40\n",
            "97/97 [==============================] - 52s 526ms/step - loss: 1.5260 - accuracy: 0.5106\n",
            "Epoch 28/40\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.5113 - accuracy: 0.5077\n",
            "Epoch 29/40\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.4908 - accuracy: 0.5119\n",
            "Epoch 30/40\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.4601 - accuracy: 0.5219\n",
            "Epoch 31/40\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4304 - accuracy: 0.5339\n",
            "Epoch 32/40\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4660 - accuracy: 0.5184\n",
            "Epoch 33/40\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.4104 - accuracy: 0.5384\n",
            "Epoch 34/40\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.4426 - accuracy: 0.5452\n",
            "Epoch 35/40\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.4033 - accuracy: 0.5197\n",
            "Epoch 36/40\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.3739 - accuracy: 0.5410\n",
            "Epoch 37/40\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.3908 - accuracy: 0.5374\n",
            "Epoch 38/40\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.3675 - accuracy: 0.5471\n",
            "Epoch 39/40\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4067 - accuracy: 0.5423\n",
            "Epoch 40/40\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3799 - accuracy: 0.5377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79170e2a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eErKj9QXeY4",
        "outputId": "b0513bc7-360f-4bc7-c0b4-c5aa91b742fc"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 104s 2s/step - loss: 1.4920 - accuracy: 0.5152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4920461177825928, 0.5151883363723755]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtilZz-oXR-d",
        "outputId": "82e313be-1946-45ed-811f-c11bb963d59b"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.4164 - accuracy: 0.5287\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 1.3872 - accuracy: 0.5348\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.3860 - accuracy: 0.5394\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.3612 - accuracy: 0.5487\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.3264 - accuracy: 0.5503\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 505ms/step - loss: 1.3589 - accuracy: 0.5458\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.3306 - accuracy: 0.5619\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.2979 - accuracy: 0.5503\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.3222 - accuracy: 0.5539\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3283 - accuracy: 0.5539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc773dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWu8vFb0XhpY",
        "outputId": "5a568778-210d-4132-957e-6a3cee606d5b"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 20s 453ms/step - loss: 1.4385 - accuracy: 0.5043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.438458800315857, 0.5042527318000793]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAuXgN9XYOs",
        "outputId": "5f26543e-1649-4766-eca3-c625c14d33f6"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.2935 - accuracy: 0.5706\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2901 - accuracy: 0.5706\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 505ms/step - loss: 1.2830 - accuracy: 0.5655\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.3089 - accuracy: 0.5523\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.3262 - accuracy: 0.5555\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.2746 - accuracy: 0.5632\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.2998 - accuracy: 0.5716\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.2985 - accuracy: 0.5706\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2960 - accuracy: 0.5703\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.2529 - accuracy: 0.5694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc7086d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-ZYI0QqXiMy",
        "outputId": "1b5899f7-2ce1-46e2-9ce2-90604e290e5a"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #60"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 20s 467ms/step - loss: 1.3998 - accuracy: 0.5298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3997935056686401, 0.5297691226005554]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3V1RakVXY4t",
        "outputId": "90a8001c-42ef-4c3b-f238-d41655f058fc"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.3071 - accuracy: 0.5555\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2660 - accuracy: 0.5639\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 505ms/step - loss: 1.2693 - accuracy: 0.5739\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 503ms/step - loss: 1.2409 - accuracy: 0.5835\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 503ms/step - loss: 1.2703 - accuracy: 0.5694\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.2671 - accuracy: 0.5652\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.2915 - accuracy: 0.5597\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 513ms/step - loss: 1.2586 - accuracy: 0.5684\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2712 - accuracy: 0.5742\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 52s 528ms/step - loss: 1.2178 - accuracy: 0.5848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc708e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InR-QWrFXkfy",
        "outputId": "3164f4da-f117-42b7-cd76-36d5a8fb66b8"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 471ms/step - loss: 1.3265 - accuracy: 0.5759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3264529705047607, 0.5759416818618774]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-AVimRLXalp"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXs7mFteXk4E",
        "outputId": "34dce13a-d497-490a-e908-6006749ccbc5"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 20s 458ms/step - loss: 1.3679 - accuracy: 0.5541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3678925037384033, 0.5540704727172852]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVYbHgA1Xa2M",
        "outputId": "02a75c0d-eeaa-4b14-f766-f88a47b6b980"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 1.2576 - accuracy: 0.5739\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.2368 - accuracy: 0.5774\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 506ms/step - loss: 1.2547 - accuracy: 0.5684\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.2401 - accuracy: 0.5813\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.2132 - accuracy: 0.5819\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 513ms/step - loss: 1.2586 - accuracy: 0.5674\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.2208 - accuracy: 0.5787\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2479 - accuracy: 0.5916\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.1972 - accuracy: 0.5910\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2288 - accuracy: 0.5781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7916e72f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uICOuMq9XlPN",
        "outputId": "fbe61f40-aa54-4171-a6c9-f5625a8ae98b"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #90"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 470ms/step - loss: 1.3605 - accuracy: 0.5431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3605055809020996, 0.543134868144989]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mniGAYSpYVwK",
        "outputId": "92a66cd1-e53e-48cf-8c14-19006c81efe6"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 49s 497ms/step - loss: 1.2360 - accuracy: 0.5794\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2090 - accuracy: 0.5961\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2370 - accuracy: 0.5739\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2150 - accuracy: 0.5890\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2028 - accuracy: 0.5932\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2226 - accuracy: 0.5855\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2049 - accuracy: 0.5861\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.2195 - accuracy: 0.5890\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2065 - accuracy: 0.5797\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.1898 - accuracy: 0.5903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc69ff50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1r1GBXrIQsh",
        "outputId": "4eff9b63-9b00-4434-ad66-9072a36f2c4b"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 470ms/step - loss: 1.3361 - accuracy: 0.5541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3360763788223267, 0.5540704727172852]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO-Bqi-rISJW",
        "outputId": "037743e0-3370-4fc0-9467-846c77eb9ae7"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.1735 - accuracy: 0.5990\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.1882 - accuracy: 0.5897\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.1822 - accuracy: 0.5997\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 52s 530ms/step - loss: 1.1982 - accuracy: 0.5855\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.1827 - accuracy: 0.5971\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.1901 - accuracy: 0.5884\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 52s 530ms/step - loss: 1.1691 - accuracy: 0.6035\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.1957 - accuracy: 0.5826\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.1657 - accuracy: 0.6039\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.1724 - accuracy: 0.5987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc6795d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QKfn7gkIVFw",
        "outputId": "a77f09b7-cb40-40ce-c5f6-8c81cc1ccde3"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #110"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 476ms/step - loss: 1.3689 - accuracy: 0.5371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3689066171646118, 0.5370595455169678]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3YJ1L8DIS2e",
        "outputId": "5f142551-3845-483a-9d5f-17d602531973"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.1867 - accuracy: 0.5894\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.2367 - accuracy: 0.5784\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.2240 - accuracy: 0.5765\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.1556 - accuracy: 0.6055\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1506 - accuracy: 0.5952\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.1686 - accuracy: 0.6023\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1477 - accuracy: 0.6074\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.1569 - accuracy: 0.5919\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.1648 - accuracy: 0.6068\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.1716 - accuracy: 0.5977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc69f650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTU9xJCrIVyt",
        "outputId": "ffd0cf1e-d3bd-4040-c0ad-19639bc2ab54"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #120"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 480ms/step - loss: 1.3420 - accuracy: 0.5383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3420027494430542, 0.5382745862007141]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEL9xCaHITY9",
        "outputId": "44e47f49-f6cb-44c8-bbe7-1dc388400351"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.1537 - accuracy: 0.6006\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1603 - accuracy: 0.6029\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.1589 - accuracy: 0.6090\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.1276 - accuracy: 0.6090\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.1187 - accuracy: 0.6055\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.1396 - accuracy: 0.6045\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.1373 - accuracy: 0.5997\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.1218 - accuracy: 0.6071\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.1378 - accuracy: 0.6168\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1336 - accuracy: 0.5948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc69f790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJUtdXWZIWkJ",
        "outputId": "0c141014-7c70-493c-efa2-499fa267b2d8"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #130"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 472ms/step - loss: 1.3075 - accuracy: 0.5443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3074806928634644, 0.5443499684333801]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W-Ch660IT6C",
        "outputId": "31d7eabc-34cd-4c48-a14a-dee9dfb00a28"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1428 - accuracy: 0.6106\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1436 - accuracy: 0.6084\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.1216 - accuracy: 0.6113\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.1338 - accuracy: 0.6071\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.1247 - accuracy: 0.6061\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.1143 - accuracy: 0.6106\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.1046 - accuracy: 0.6161\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 51s 514ms/step - loss: 1.1132 - accuracy: 0.6123\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.1646 - accuracy: 0.6026\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.1456 - accuracy: 0.5942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc679a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsmcNKiTIXGe",
        "outputId": "c945c051-1160-4c2c-b289-b17b24577a1d"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #140"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 474ms/step - loss: 1.5156 - accuracy: 0.4775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5155620574951172, 0.47752127051353455]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4KEZbQRCzA"
      },
      "source": [
        "### Restarted training and saving weights for when epoch=60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzPXSgl3YUsh",
        "outputId": "21331d37-f310-4f32-d28d-3eb8df3f5422"
      },
      "source": [
        "model.fit(training_set, epochs=60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "97/97 [==============================] - 53s 532ms/step - loss: 4.1598 - accuracy: 0.0452\n",
            "Epoch 2/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 3.4927 - accuracy: 0.1348\n",
            "Epoch 3/60\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 3.1485 - accuracy: 0.1932\n",
            "Epoch 4/60\n",
            "97/97 [==============================] - 52s 528ms/step - loss: 2.8653 - accuracy: 0.2465\n",
            "Epoch 5/60\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 2.6344 - accuracy: 0.2890\n",
            "Epoch 6/60\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 2.4442 - accuracy: 0.3184\n",
            "Epoch 7/60\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 2.3280 - accuracy: 0.3390\n",
            "Epoch 8/60\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 2.2255 - accuracy: 0.3545\n",
            "Epoch 9/60\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 2.1057 - accuracy: 0.3871\n",
            "Epoch 10/60\n",
            "97/97 [==============================] - 51s 524ms/step - loss: 2.0566 - accuracy: 0.3952\n",
            "Epoch 11/60\n",
            "97/97 [==============================] - 53s 540ms/step - loss: 1.9760 - accuracy: 0.3981\n",
            "Epoch 12/60\n",
            "97/97 [==============================] - 53s 535ms/step - loss: 1.9364 - accuracy: 0.3987\n",
            "Epoch 13/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.8666 - accuracy: 0.4274\n",
            "Epoch 14/60\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.8063 - accuracy: 0.4455\n",
            "Epoch 15/60\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.7937 - accuracy: 0.4410\n",
            "Epoch 16/60\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.7394 - accuracy: 0.4523\n",
            "Epoch 17/60\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.7197 - accuracy: 0.4623\n",
            "Epoch 18/60\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.7090 - accuracy: 0.4448\n",
            "Epoch 19/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.6676 - accuracy: 0.4758\n",
            "Epoch 20/60\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 1.6244 - accuracy: 0.4913\n",
            "Epoch 21/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.6020 - accuracy: 0.4955\n",
            "Epoch 22/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.5949 - accuracy: 0.4839\n",
            "Epoch 23/60\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.5620 - accuracy: 0.5087\n",
            "Epoch 24/60\n",
            "97/97 [==============================] - 53s 534ms/step - loss: 1.5387 - accuracy: 0.4958\n",
            "Epoch 25/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.5079 - accuracy: 0.5135\n",
            "Epoch 26/60\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.5142 - accuracy: 0.5135\n",
            "Epoch 27/60\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.4846 - accuracy: 0.5152\n",
            "Epoch 28/60\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.4492 - accuracy: 0.5229\n",
            "Epoch 29/60\n",
            "97/97 [==============================] - 52s 535ms/step - loss: 1.4899 - accuracy: 0.5148\n",
            "Epoch 30/60\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.4813 - accuracy: 0.5106\n",
            "Epoch 31/60\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.4724 - accuracy: 0.5139\n",
            "Epoch 32/60\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.4442 - accuracy: 0.5226\n",
            "Epoch 33/60\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.4225 - accuracy: 0.5290\n",
            "Epoch 34/60\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.4100 - accuracy: 0.5358\n",
            "Epoch 35/60\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 1.3810 - accuracy: 0.5487\n",
            "Epoch 36/60\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3800 - accuracy: 0.5403\n",
            "Epoch 37/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.3614 - accuracy: 0.5406\n",
            "Epoch 38/60\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.3634 - accuracy: 0.5465\n",
            "Epoch 39/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.3580 - accuracy: 0.5442\n",
            "Epoch 40/60\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3415 - accuracy: 0.5445\n",
            "Epoch 41/60\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.3416 - accuracy: 0.5445\n",
            "Epoch 42/60\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 1.3644 - accuracy: 0.5394\n",
            "Epoch 43/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.3553 - accuracy: 0.5523\n",
            "Epoch 44/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.3060 - accuracy: 0.5668\n",
            "Epoch 45/60\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.3356 - accuracy: 0.5474\n",
            "Epoch 46/60\n",
            "97/97 [==============================] - 52s 526ms/step - loss: 1.3347 - accuracy: 0.5403\n",
            "Epoch 47/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.2955 - accuracy: 0.5619\n",
            "Epoch 48/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.2799 - accuracy: 0.5626\n",
            "Epoch 49/60\n",
            "97/97 [==============================] - 52s 528ms/step - loss: 1.3057 - accuracy: 0.5635\n",
            "Epoch 50/60\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.2763 - accuracy: 0.5629\n",
            "Epoch 51/60\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.2763 - accuracy: 0.5600\n",
            "Epoch 52/60\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2251 - accuracy: 0.5810\n",
            "Epoch 53/60\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.2524 - accuracy: 0.5687\n",
            "Epoch 54/60\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.2510 - accuracy: 0.5635\n",
            "Epoch 55/60\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.2232 - accuracy: 0.5916\n",
            "Epoch 56/60\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 1.2681 - accuracy: 0.5758\n",
            "Epoch 57/60\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.2518 - accuracy: 0.5735\n",
            "Epoch 58/60\n",
            "97/97 [==============================] - 52s 530ms/step - loss: 1.2073 - accuracy: 0.5755\n",
            "Epoch 59/60\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.2145 - accuracy: 0.5845\n",
            "Epoch 60/60\n",
            "97/97 [==============================] - 51s 524ms/step - loss: 1.2508 - accuracy: 0.5719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f961215c6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWbpV_A0og_T",
        "outputId": "40f6c327-d4c8-4561-9dee-5b59e9a672ce"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #60"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 20s 485ms/step - loss: 1.4730 - accuracy: 0.5407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.472959280014038, 0.5407047271728516]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAZjLKCzc4ZO"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg16-60.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSuHcUgHXIC0"
      },
      "source": [
        "weights_dir = 'drive/MyDrive/vgg16-60'\n",
        "model.load_weights(os.path.join(weights_dir, 'vgg16-60.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kldiswhEYP8E",
        "outputId": "3077a994-34b5-4ac0-f6d2-10e3eda7cfbb"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 631s 6s/step - loss: 1.3573 - accuracy: 0.5500\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 48s 483ms/step - loss: 1.3405 - accuracy: 0.5532\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.3113 - accuracy: 0.5681\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.2978 - accuracy: 0.5655\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 49s 499ms/step - loss: 1.3037 - accuracy: 0.5658\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.2928 - accuracy: 0.5681\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.2854 - accuracy: 0.5613\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.2872 - accuracy: 0.5655\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.2916 - accuracy: 0.5706\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 50s 514ms/step - loss: 1.2806 - accuracy: 0.5610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f58ffb1cd10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9-dFnrdYiIX",
        "outputId": "56fe8fcd-dbe2-4a26-9a96-9eca3dfbde6f"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 159s 6s/step - loss: 1.2517 - accuracy: 0.6100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.251744031906128, 0.6100000143051147]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLWnXMgsROw8"
      },
      "source": [
        "### Trying VGG19 with new dataset with different epochs, starting at 40 and increasing by 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNuobomzYNRW",
        "outputId": "a0262fe9-4c48-473a-ddd3-7be409a2ab96"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_13 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_14 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_5 ( (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_15 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_16 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_16 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_16 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_6 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_17 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_17 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_17 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_18 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_18 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_18 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_19 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_19 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_19 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_20 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_20 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_20 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_7 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_21 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_21 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_21 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_22 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_22 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_22 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_23 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_23 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_23 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_24 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_24 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_24 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_8 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_25 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_25 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_25 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_26 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_26 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_26 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_27 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_27 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_27 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_28 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_28 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_28 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_9 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L30VsCwHYTQj",
        "outputId": "3d9d82e0-8fe0-4ac6-80bb-41db88c73643"
      },
      "source": [
        "model.fit(training_set, epochs=60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "97/97 [==============================] - 58s 585ms/step - loss: 4.1802 - accuracy: 0.0477\n",
            "Epoch 2/60\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 3.4836 - accuracy: 0.1413\n",
            "Epoch 3/60\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 3.1175 - accuracy: 0.1994\n",
            "Epoch 4/60\n",
            "97/97 [==============================] - 64s 655ms/step - loss: 2.8433 - accuracy: 0.2506\n",
            "Epoch 5/60\n",
            "97/97 [==============================] - 63s 640ms/step - loss: 2.6060 - accuracy: 0.2832\n",
            "Epoch 6/60\n",
            "97/97 [==============================] - 64s 658ms/step - loss: 2.4311 - accuracy: 0.3229\n",
            "Epoch 7/60\n",
            "97/97 [==============================] - 63s 643ms/step - loss: 2.2722 - accuracy: 0.3365\n",
            "Epoch 8/60\n",
            "97/97 [==============================] - 64s 658ms/step - loss: 2.1659 - accuracy: 0.3671\n",
            "Epoch 9/60\n",
            "97/97 [==============================] - 64s 656ms/step - loss: 2.0530 - accuracy: 0.3926\n",
            "Epoch 10/60\n",
            "97/97 [==============================] - 63s 640ms/step - loss: 1.9796 - accuracy: 0.4035\n",
            "Epoch 11/60\n",
            "97/97 [==============================] - 65s 658ms/step - loss: 1.9227 - accuracy: 0.4200\n",
            "Epoch 12/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.8577 - accuracy: 0.4397\n",
            "Epoch 13/60\n",
            "97/97 [==============================] - 63s 641ms/step - loss: 1.8350 - accuracy: 0.4397\n",
            "Epoch 14/60\n",
            "97/97 [==============================] - 64s 657ms/step - loss: 1.7305 - accuracy: 0.4606\n",
            "Epoch 15/60\n",
            "97/97 [==============================] - 63s 640ms/step - loss: 1.7011 - accuracy: 0.4577\n",
            "Epoch 16/60\n",
            "97/97 [==============================] - 64s 654ms/step - loss: 1.6767 - accuracy: 0.4755\n",
            "Epoch 17/60\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.6374 - accuracy: 0.4819\n",
            "Epoch 18/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.6213 - accuracy: 0.4790\n",
            "Epoch 19/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.6012 - accuracy: 0.4861\n",
            "Epoch 20/60\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.5730 - accuracy: 0.4926\n",
            "Epoch 21/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.5123 - accuracy: 0.5068\n",
            "Epoch 22/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.5078 - accuracy: 0.5145\n",
            "Epoch 23/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.4465 - accuracy: 0.5297\n",
            "Epoch 24/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.4676 - accuracy: 0.5213\n",
            "Epoch 25/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.4474 - accuracy: 0.5235\n",
            "Epoch 26/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.4320 - accuracy: 0.5203\n",
            "Epoch 27/60\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.4337 - accuracy: 0.5271\n",
            "Epoch 28/60\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.4278 - accuracy: 0.5339\n",
            "Epoch 29/60\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.3871 - accuracy: 0.5523\n",
            "Epoch 30/60\n",
            "97/97 [==============================] - 64s 654ms/step - loss: 1.3898 - accuracy: 0.5439\n",
            "Epoch 31/60\n",
            "97/97 [==============================] - 64s 654ms/step - loss: 1.4143 - accuracy: 0.5168\n",
            "Epoch 32/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.3558 - accuracy: 0.5406\n",
            "Epoch 33/60\n",
            "97/97 [==============================] - 64s 654ms/step - loss: 1.3227 - accuracy: 0.5571\n",
            "Epoch 34/60\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 1.3560 - accuracy: 0.5477\n",
            "Epoch 35/60\n",
            "97/97 [==============================] - 62s 630ms/step - loss: 1.3022 - accuracy: 0.5742\n",
            "Epoch 36/60\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.3199 - accuracy: 0.5619\n",
            "Epoch 37/60\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2783 - accuracy: 0.5729\n",
            "Epoch 38/60\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.2922 - accuracy: 0.5587\n",
            "Epoch 39/60\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.2623 - accuracy: 0.5752\n",
            "Epoch 40/60\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 1.2925 - accuracy: 0.5597\n",
            "Epoch 41/60\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2659 - accuracy: 0.5700\n",
            "Epoch 42/60\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.2485 - accuracy: 0.5726\n",
            "Epoch 43/60\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.2158 - accuracy: 0.5932\n",
            "Epoch 44/60\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.2938 - accuracy: 0.5700\n",
            "Epoch 45/60\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.2757 - accuracy: 0.5626\n",
            "Epoch 46/60\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2566 - accuracy: 0.5700\n",
            "Epoch 47/60\n",
            "97/97 [==============================] - 60s 606ms/step - loss: 1.2170 - accuracy: 0.5858\n",
            "Epoch 48/60\n",
            "97/97 [==============================] - 60s 606ms/step - loss: 1.2139 - accuracy: 0.5787\n",
            "Epoch 49/60\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.2210 - accuracy: 0.5823\n",
            "Epoch 50/60\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.1943 - accuracy: 0.5871\n",
            "Epoch 51/60\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.2179 - accuracy: 0.5913\n",
            "Epoch 52/60\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 1.2145 - accuracy: 0.5852\n",
            "Epoch 53/60\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.1899 - accuracy: 0.5961\n",
            "Epoch 54/60\n",
            "97/97 [==============================] - 60s 607ms/step - loss: 1.1954 - accuracy: 0.5955\n",
            "Epoch 55/60\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.2163 - accuracy: 0.5803\n",
            "Epoch 56/60\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.1901 - accuracy: 0.5939\n",
            "Epoch 57/60\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.1655 - accuracy: 0.5926\n",
            "Epoch 58/60\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.1600 - accuracy: 0.5971\n",
            "Epoch 59/60\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.1918 - accuracy: 0.5858\n",
            "Epoch 60/60\n",
            "97/97 [==============================] - 60s 606ms/step - loss: 1.1603 - accuracy: 0.5958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78cc770650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yusDuArWYTQk",
        "outputId": "5501e2bf-d68a-4d59-ecf3-b94b737ba3ff"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #60"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 563ms/step - loss: 1.1785 - accuracy: 0.5759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1785175800323486, 0.5759416818618774]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By8PqwnBYTQk",
        "outputId": "68c1e4dc-8f8f-4e29-cdc2-60ac5c316d37"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.1226 - accuracy: 0.6065\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.1586 - accuracy: 0.5929\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.1777 - accuracy: 0.5971\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.1888 - accuracy: 0.5919\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.1769 - accuracy: 0.5910\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.1620 - accuracy: 0.5965\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 606ms/step - loss: 1.1361 - accuracy: 0.6026\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.1452 - accuracy: 0.6065\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.1485 - accuracy: 0.6184\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.1505 - accuracy: 0.6077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78b9e5a2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2wwLB7EYTQl",
        "outputId": "0e84ead3-1b41-445b-955d-9ebe3f01bd28"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #70"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 561ms/step - loss: 1.2092 - accuracy: 0.5687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.20916748046875, 0.5686512589454651]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20_F-2JgYTQl",
        "outputId": "2e4cd704-ecf7-4129-da04-d5fe65478747"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.1724 - accuracy: 0.5948\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.1292 - accuracy: 0.6058\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.1752 - accuracy: 0.5984\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.1414 - accuracy: 0.6000\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.1297 - accuracy: 0.6081\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.0904 - accuracy: 0.6229\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.0984 - accuracy: 0.6187\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.0679 - accuracy: 0.6252\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.1057 - accuracy: 0.6197\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.0903 - accuracy: 0.6190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78b9de2c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81qn7LT4YTQl",
        "outputId": "f18a9aec-1732-4bc6-cfa7-6d36d1cd9a4f"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 572ms/step - loss: 1.1415 - accuracy: 0.5881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.141545057296753, 0.5880923271179199]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlN0sbHdYTQm",
        "outputId": "f1006c87-11e2-428d-94b6-4ccf65a9c7d5"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 60s 607ms/step - loss: 1.0704 - accuracy: 0.6261\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 60s 607ms/step - loss: 1.1191 - accuracy: 0.6068\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.0844 - accuracy: 0.6274\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.0822 - accuracy: 0.6168\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.0956 - accuracy: 0.6158\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.1242 - accuracy: 0.6184\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.0831 - accuracy: 0.6203\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.0960 - accuracy: 0.6203\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 1.0612 - accuracy: 0.6248\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.1343 - accuracy: 0.6084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78b9de7f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1A_tr-pYTQm",
        "outputId": "d10e0c37-aacf-4bcf-dc15-1e3aa2111b16"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #90"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 567ms/step - loss: 1.1658 - accuracy: 0.5820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1657918691635132, 0.5820170044898987]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LebrPOMYTQm",
        "outputId": "154518e1-b9aa-4dba-87db-99d5d3c0855d"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0934 - accuracy: 0.6245\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 59s 607ms/step - loss: 1.1083 - accuracy: 0.6045\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0884 - accuracy: 0.6294\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.0667 - accuracy: 0.6248\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0718 - accuracy: 0.6245\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.0594 - accuracy: 0.6206\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.0427 - accuracy: 0.6368\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 1.0639 - accuracy: 0.6210\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 618ms/step - loss: 1.0790 - accuracy: 0.6139\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.1128 - accuracy: 0.6145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9612377110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLMulFMYTQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac524ac-88fa-44a2-96fd-c4f69cfb52cd"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 209s 4s/step - loss: 1.1471 - accuracy: 0.6015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.147148609161377, 0.6014580726623535]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ISwwghYTQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f48dbf-f6d8-458e-844e-372a6b8aaf91"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 627ms/step - loss: 1.0404 - accuracy: 0.6277\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.0594 - accuracy: 0.6271\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 1.0705 - accuracy: 0.6274\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0377 - accuracy: 0.6416\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.0765 - accuracy: 0.6258\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.0514 - accuracy: 0.6190\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 0.9936 - accuracy: 0.6523\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0193 - accuracy: 0.6387\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.0157 - accuracy: 0.6452\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0777 - accuracy: 0.6281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9612377150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cssA9TyzYTQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a0c800-6ab0-4096-94f6-48593202a9da"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #110"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 587ms/step - loss: 1.2127 - accuracy: 0.6015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2127381563186646, 0.6014580726623535]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdyook45Klw",
        "outputId": "4cf2b5a6-ef61-4db3-aab1-1ad24f22a90a"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0577 - accuracy: 0.6235\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 1.0439 - accuracy: 0.6332\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 62s 629ms/step - loss: 1.0585 - accuracy: 0.6206\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 62s 629ms/step - loss: 1.0308 - accuracy: 0.6310\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 62s 628ms/step - loss: 1.0187 - accuracy: 0.6494\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0132 - accuracy: 0.6342\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.0464 - accuracy: 0.6342\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 1.0389 - accuracy: 0.6348\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0289 - accuracy: 0.6365\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0259 - accuracy: 0.6358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96122f0e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYEPMPV35MY6",
        "outputId": "7ef3e516-f716-4fec-d3e4-8578935bf790"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #120"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 24s 608ms/step - loss: 1.1569 - accuracy: 0.6173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1569312810897827, 0.6172539591789246]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Ts_zFl8zTN",
        "outputId": "9a592622-338e-488d-a7fa-e09b1ee7de0a"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0217 - accuracy: 0.6410\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0234 - accuracy: 0.6374\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 1.0186 - accuracy: 0.6400\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0220 - accuracy: 0.6432\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 1.0335 - accuracy: 0.6348\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 1.0158 - accuracy: 0.6358\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 1.0046 - accuracy: 0.6384\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 1.0060 - accuracy: 0.6384\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 0.9592 - accuracy: 0.6626\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0452 - accuracy: 0.6284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96122f0610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU2tFz8K80ot",
        "outputId": "bba0e4b4-a946-4c85-baac-90c9fb5280a8"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #130"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 593ms/step - loss: 1.1954 - accuracy: 0.5978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.195421814918518, 0.5978128910064697]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1YulJ4HAsqq",
        "outputId": "6a0ffcd4-be06-4fa8-ec84-5345646069ee"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0312 - accuracy: 0.6261\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9899 - accuracy: 0.6526\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0200 - accuracy: 0.6319\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0061 - accuracy: 0.6423\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0194 - accuracy: 0.6429\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.0397 - accuracy: 0.6323\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 1.0176 - accuracy: 0.6400\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0223 - accuracy: 0.6300\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.0086 - accuracy: 0.6390\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 0.9981 - accuracy: 0.6481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9612302790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XclXgwT6CQRU",
        "outputId": "51dddf67-3f4d-49b2-a940-1e8c32cd355b"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #140"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 588ms/step - loss: 1.1591 - accuracy: 0.6245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1591356992721558, 0.6245443224906921]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V4QICPlEfih",
        "outputId": "b33bc412-afbe-45a8-c37f-2cb2f0c39737"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 1.0066 - accuracy: 0.6423\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0178 - accuracy: 0.6400\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0321 - accuracy: 0.6426\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9907 - accuracy: 0.6423\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 0.9937 - accuracy: 0.6448\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.0253 - accuracy: 0.6413\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0140 - accuracy: 0.6384\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9645 - accuracy: 0.6497\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9826 - accuracy: 0.6494\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9429 - accuracy: 0.6606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f962403f9d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-MWIR4FEhR6",
        "outputId": "ceba8b6c-1580-439d-ec77-bbf42e8d9ff8"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #150 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 572ms/step - loss: 1.1326 - accuracy: 0.6185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1325528621673584, 0.6184689998626709]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhPlTPt3HPP-",
        "outputId": "1b42f7ba-4e89-4a41-9866-abea340af2df"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0061 - accuracy: 0.6403\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 0.9705 - accuracy: 0.6590\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 0.9666 - accuracy: 0.6497\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 0.9532 - accuracy: 0.6652\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9702 - accuracy: 0.6581\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9783 - accuracy: 0.6574\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 0.9711 - accuracy: 0.6529\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 0.9372 - accuracy: 0.6642\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 0.9700 - accuracy: 0.6555\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 0.9686 - accuracy: 0.6532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96122f0850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-wXBL4TKl5j",
        "outputId": "5399f122-400b-45a1-ba7d-d20a9b678ef5"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 581ms/step - loss: 1.1017 - accuracy: 0.6330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1016980409622192, 0.6330498456954956]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP_XMkzmK1CF",
        "outputId": "feffffb0-e8ab-454f-9514-77c42b449944"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 0.9562 - accuracy: 0.6458\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 1.0023 - accuracy: 0.6471\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 0.9261 - accuracy: 0.6648\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 0.9480 - accuracy: 0.6587\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 62s 629ms/step - loss: 0.9745 - accuracy: 0.6523\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 0.9613 - accuracy: 0.6568\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 0.9335 - accuracy: 0.6768\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 0.9406 - accuracy: 0.6632\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9632 - accuracy: 0.6561\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9651 - accuracy: 0.6642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96122dde90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmELKXNuN4wc",
        "outputId": "0bdb1470-d11a-4ff4-8363-3a5039f237e0"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #170"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 24s 606ms/step - loss: 1.0601 - accuracy: 0.6209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0600800514221191, 0.6208991408348083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JodbRzVTOTC6",
        "outputId": "1b4d22ee-98de-43de-f3ec-516c83250a69"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 627ms/step - loss: 0.9185 - accuracy: 0.6590\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 0.9511 - accuracy: 0.6613\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9628 - accuracy: 0.6516\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 0.9738 - accuracy: 0.6671\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9531 - accuracy: 0.6587\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 0.9343 - accuracy: 0.6574\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 0.9770 - accuracy: 0.6497\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9285 - accuracy: 0.6500\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 0.9413 - accuracy: 0.6565\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 0.9548 - accuracy: 0.6597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9612267950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dG7VV4CRP-I",
        "outputId": "858baf8d-5b8a-4933-add5-eeed89d3ac62"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #180"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 588ms/step - loss: 1.1084 - accuracy: 0.6136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1083630323410034, 0.6136087775230408]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOlE70YORe8l",
        "outputId": "f14df816-c641-4f10-aba7-cda9de040edc"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9490 - accuracy: 0.6577\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 0.9245 - accuracy: 0.6577\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 0.9550 - accuracy: 0.6587\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 61s 626ms/step - loss: 0.9680 - accuracy: 0.6571\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9288 - accuracy: 0.6703\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 0.9377 - accuracy: 0.6623\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9421 - accuracy: 0.6619\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 0.9417 - accuracy: 0.6745\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9352 - accuracy: 0.6694\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 621ms/step - loss: 0.9645 - accuracy: 0.6503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96123cd6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_z3RzrqUiL4",
        "outputId": "eb6656e3-91b5-422c-9bef-b3969159b4c0"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #180"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 581ms/step - loss: 1.1066 - accuracy: 0.6124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1065982580184937, 0.6123936772346497]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhbfZimOUvxu",
        "outputId": "84702274-c548-4f0d-8852-3ad3f1cfaf72"
      },
      "source": [
        "model.fit(training_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 0.9653 - accuracy: 0.6552\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 0.9329 - accuracy: 0.6652\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 0.9343 - accuracy: 0.6600\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 0.9330 - accuracy: 0.6697\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - 61s 619ms/step - loss: 0.9603 - accuracy: 0.6642\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9567 - accuracy: 0.6594\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - 61s 625ms/step - loss: 0.9265 - accuracy: 0.6623\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 0.9552 - accuracy: 0.6635\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 0.9344 - accuracy: 0.6716\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - 61s 627ms/step - loss: 0.9495 - accuracy: 0.6606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96122c3590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_3K9hmvUs7o",
        "outputId": "a1e5323a-1cd8-4290-dc33-99e3a037eac4"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #190"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 592ms/step - loss: 1.1400 - accuracy: 0.6100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1399586200714111, 0.6099635362625122]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFt6kQUi9a7R"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-180'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvgshKoXc7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febe590b-7598-4ab2-bda2-23e25f472b58"
      },
      "source": [
        "model.fit(training_set, epochs=80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "97/97 [==============================] - 534s 5s/step - loss: 4.2084 - accuracy: 0.0477\n",
            "Epoch 2/80\n",
            "97/97 [==============================] - 55s 562ms/step - loss: 3.4617 - accuracy: 0.1423\n",
            "Epoch 3/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 3.0599 - accuracy: 0.2068\n",
            "Epoch 4/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 2.7859 - accuracy: 0.2548\n",
            "Epoch 5/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 2.5130 - accuracy: 0.3000\n",
            "Epoch 6/80\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 2.3666 - accuracy: 0.3261\n",
            "Epoch 7/80\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 2.2258 - accuracy: 0.3542\n",
            "Epoch 8/80\n",
            "97/97 [==============================] - 58s 597ms/step - loss: 2.1170 - accuracy: 0.3832\n",
            "Epoch 9/80\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 2.0196 - accuracy: 0.3897\n",
            "Epoch 10/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.9206 - accuracy: 0.4123\n",
            "Epoch 11/80\n",
            "97/97 [==============================] - 59s 600ms/step - loss: 1.8626 - accuracy: 0.4323\n",
            "Epoch 12/80\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.7971 - accuracy: 0.4381\n",
            "Epoch 13/80\n",
            "97/97 [==============================] - 59s 597ms/step - loss: 1.7306 - accuracy: 0.4626\n",
            "Epoch 14/80\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.6832 - accuracy: 0.4590\n",
            "Epoch 15/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.6366 - accuracy: 0.4787\n",
            "Epoch 16/80\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 1.6235 - accuracy: 0.4771\n",
            "Epoch 17/80\n",
            "97/97 [==============================] - 58s 596ms/step - loss: 1.5879 - accuracy: 0.5006\n",
            "Epoch 18/80\n",
            "97/97 [==============================] - 59s 600ms/step - loss: 1.5479 - accuracy: 0.5039\n",
            "Epoch 19/80\n",
            "97/97 [==============================] - 59s 602ms/step - loss: 1.5248 - accuracy: 0.5171\n",
            "Epoch 20/80\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.5110 - accuracy: 0.5113\n",
            "Epoch 21/80\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.5152 - accuracy: 0.5213\n",
            "Epoch 22/80\n",
            "97/97 [==============================] - 59s 601ms/step - loss: 1.4789 - accuracy: 0.5219\n",
            "Epoch 23/80\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.4461 - accuracy: 0.5306\n",
            "Epoch 24/80\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.4128 - accuracy: 0.5365\n",
            "Epoch 25/80\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.4341 - accuracy: 0.5235\n",
            "Epoch 26/80\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 1.3908 - accuracy: 0.5455\n",
            "Epoch 27/80\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.3694 - accuracy: 0.5468\n",
            "Epoch 28/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.3686 - accuracy: 0.5429\n",
            "Epoch 29/80\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.3813 - accuracy: 0.5342\n",
            "Epoch 30/80\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.3371 - accuracy: 0.5574\n",
            "Epoch 31/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.3564 - accuracy: 0.5494\n",
            "Epoch 32/80\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.3214 - accuracy: 0.5616\n",
            "Epoch 33/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.3347 - accuracy: 0.5490\n",
            "Epoch 34/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.3259 - accuracy: 0.5652\n",
            "Epoch 35/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.2919 - accuracy: 0.5700\n",
            "Epoch 36/80\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.2752 - accuracy: 0.5648\n",
            "Epoch 37/80\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.2563 - accuracy: 0.5765\n",
            "Epoch 38/80\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.2425 - accuracy: 0.5687\n",
            "Epoch 39/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.2195 - accuracy: 0.5884\n",
            "Epoch 40/80\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.2593 - accuracy: 0.5645\n",
            "Epoch 41/80\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.2527 - accuracy: 0.5765\n",
            "Epoch 42/80\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 1.2229 - accuracy: 0.5806\n",
            "Epoch 43/80\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.1917 - accuracy: 0.6006\n",
            "Epoch 44/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.2322 - accuracy: 0.5813\n",
            "Epoch 45/80\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.1943 - accuracy: 0.5906\n",
            "Epoch 46/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.2325 - accuracy: 0.5819\n",
            "Epoch 47/80\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.1799 - accuracy: 0.5923\n",
            "Epoch 48/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.1876 - accuracy: 0.5877\n",
            "Epoch 49/80\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.1891 - accuracy: 0.5935\n",
            "Epoch 50/80\n",
            "97/97 [==============================] - 58s 586ms/step - loss: 1.1869 - accuracy: 0.5894\n",
            "Epoch 51/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1973 - accuracy: 0.5916\n",
            "Epoch 52/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.1979 - accuracy: 0.5887\n",
            "Epoch 53/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.2226 - accuracy: 0.5806\n",
            "Epoch 54/80\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 1.2263 - accuracy: 0.5839\n",
            "Epoch 55/80\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.2139 - accuracy: 0.5839\n",
            "Epoch 56/80\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.1870 - accuracy: 0.5906\n",
            "Epoch 57/80\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.1768 - accuracy: 0.5929\n",
            "Epoch 58/80\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 1.1650 - accuracy: 0.5945\n",
            "Epoch 59/80\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.1427 - accuracy: 0.5965\n",
            "Epoch 60/80\n",
            "97/97 [==============================] - 58s 592ms/step - loss: 1.1622 - accuracy: 0.5942\n",
            "Epoch 61/80\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 1.1632 - accuracy: 0.5955\n",
            "Epoch 62/80\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.1297 - accuracy: 0.6126\n",
            "Epoch 63/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1347 - accuracy: 0.6003\n",
            "Epoch 64/80\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.1278 - accuracy: 0.6158\n",
            "Epoch 65/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1440 - accuracy: 0.6019\n",
            "Epoch 66/80\n",
            "97/97 [==============================] - 58s 592ms/step - loss: 1.1115 - accuracy: 0.6055\n",
            "Epoch 67/80\n",
            "97/97 [==============================] - 58s 594ms/step - loss: 1.1347 - accuracy: 0.6081\n",
            "Epoch 68/80\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.1628 - accuracy: 0.6087\n",
            "Epoch 69/80\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.1141 - accuracy: 0.6071\n",
            "Epoch 70/80\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0965 - accuracy: 0.6158\n",
            "Epoch 71/80\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.1164 - accuracy: 0.6142\n",
            "Epoch 72/80\n",
            "97/97 [==============================] - 61s 619ms/step - loss: 1.0855 - accuracy: 0.6177\n",
            "Epoch 73/80\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.1088 - accuracy: 0.6087\n",
            "Epoch 74/80\n",
            "97/97 [==============================] - 61s 622ms/step - loss: 1.0851 - accuracy: 0.6306\n",
            "Epoch 75/80\n",
            "97/97 [==============================] - 60s 607ms/step - loss: 1.0782 - accuracy: 0.6335\n",
            "Epoch 76/80\n",
            "97/97 [==============================] - 61s 623ms/step - loss: 1.0895 - accuracy: 0.6235\n",
            "Epoch 77/80\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.0785 - accuracy: 0.6171\n",
            "Epoch 78/80\n",
            "97/97 [==============================] - 60s 610ms/step - loss: 1.1161 - accuracy: 0.6123\n",
            "Epoch 79/80\n",
            "97/97 [==============================] - 61s 624ms/step - loss: 1.0727 - accuracy: 0.6245\n",
            "Epoch 80/80\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.0745 - accuracy: 0.6184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f966e7a2350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5o163DxaSfx"
      },
      "source": [
        "weights_dir = 'drive/MyDrive/160'\n",
        "model.load_weights(os.path.join(weights_dir, 'vgg19-160.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRhztZyoejNr",
        "outputId": "dfeca819-eade-46c0-fb7a-a0d4ad5f4656"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 20s 562ms/step - loss: 0.8253 - accuracy: 0.7525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8252878785133362, 0.7524999976158142]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOryWvfEGtbQ"
      },
      "source": [
        "### VGG-16 Freeze all layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7iJ46PherlC",
        "outputId": "dfa01108-2eca-4937-91c6-1f14dcb34d17"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_29 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_29 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_29 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_30 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_30 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_30 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_10  (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_31 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_31 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_31 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_32 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_32 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_32 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_11  (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_33 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_33 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_33 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_34 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_34 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_34 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_35 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_35 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_35 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_12  (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_36 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_36 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_36 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_37 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_37 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_37 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_38 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_38 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_38 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_13  (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_39 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_39 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_39 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_40 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_40 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_40 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_41 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_41 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_41 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_14  (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOrDQTpTfECz",
        "outputId": "802a68c5-1394-4590-d038-03632abf79a7"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 48s 484ms/step - loss: 4.2183 - accuracy: 0.0484\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 3.5286 - accuracy: 0.1300\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 49s 496ms/step - loss: 3.1739 - accuracy: 0.1887\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 50s 505ms/step - loss: 2.9180 - accuracy: 0.2306\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 2.7090 - accuracy: 0.2652\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 2.5071 - accuracy: 0.2984\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 2.4083 - accuracy: 0.3181\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 2.2937 - accuracy: 0.3365\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 2.1672 - accuracy: 0.3684\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 2.0847 - accuracy: 0.3848\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 2.0311 - accuracy: 0.3990\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.9681 - accuracy: 0.4090\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.8842 - accuracy: 0.4258\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 49s 499ms/step - loss: 1.8357 - accuracy: 0.4445\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.7943 - accuracy: 0.4448\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 50s 504ms/step - loss: 1.7726 - accuracy: 0.4532\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.7412 - accuracy: 0.4513\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 1.6999 - accuracy: 0.4655\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 1.6845 - accuracy: 0.4690\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 50s 514ms/step - loss: 1.6529 - accuracy: 0.4719\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.6053 - accuracy: 0.4806\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.5639 - accuracy: 0.4952\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.5793 - accuracy: 0.4832\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.5440 - accuracy: 0.5039\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.5324 - accuracy: 0.4961\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.5300 - accuracy: 0.5019\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.5059 - accuracy: 0.5032\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.5102 - accuracy: 0.5132\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.4691 - accuracy: 0.5074\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.4560 - accuracy: 0.5197\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.4528 - accuracy: 0.5242\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4929 - accuracy: 0.5168\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 50s 513ms/step - loss: 1.4511 - accuracy: 0.5223\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.4398 - accuracy: 0.5297\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.4301 - accuracy: 0.5281\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.4274 - accuracy: 0.5277\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3887 - accuracy: 0.5310\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.3616 - accuracy: 0.5471\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3912 - accuracy: 0.5258\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.3581 - accuracy: 0.5539\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.3322 - accuracy: 0.5590\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.3670 - accuracy: 0.5416\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3608 - accuracy: 0.5452\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.3571 - accuracy: 0.5461\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.3297 - accuracy: 0.5529\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.3227 - accuracy: 0.5497\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.3438 - accuracy: 0.5500\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.3324 - accuracy: 0.5555\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.3277 - accuracy: 0.5623\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3370 - accuracy: 0.5465\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 50s 513ms/step - loss: 1.3456 - accuracy: 0.5429\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.3504 - accuracy: 0.5490\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3241 - accuracy: 0.5513\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.3642 - accuracy: 0.5426\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3196 - accuracy: 0.5581\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.2839 - accuracy: 0.5671\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.3367 - accuracy: 0.5481\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.3177 - accuracy: 0.5597\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.3255 - accuracy: 0.5665\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.2953 - accuracy: 0.5600\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.3007 - accuracy: 0.5690\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2898 - accuracy: 0.5768\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.2819 - accuracy: 0.5774\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.3193 - accuracy: 0.5561\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.2774 - accuracy: 0.5652\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.2808 - accuracy: 0.5513\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.2760 - accuracy: 0.5726\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.2625 - accuracy: 0.5710\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 1.2629 - accuracy: 0.5694\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.2501 - accuracy: 0.5732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f58a2b5f190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp80wPJAfQEe",
        "outputId": "ddbdb722-0fb3-419e-ed85-4cc6e313b700"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 123s 4s/step - loss: 1.3869 - accuracy: 0.5140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3868820667266846, 0.5139732956886292]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eASCo_80DGK",
        "outputId": "2a418332-9c37-4b7c-b0c6-27f1151b4118"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 18s 522ms/step - loss: 1.3781 - accuracy: 0.5437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3781497478485107, 0.543749988079071]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaRVJxM001p"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg16-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-JEa-ibG2lb"
      },
      "source": [
        "### VGG-16 Freeze half of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOevnydn0FLf",
        "outputId": "fb1dbcff-b7dc-4c22-adb5-43fc8076aa31"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "len = len(base_model.layers) // 2\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:len]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_42 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_42 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_42 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_43 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_43 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_43 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_15  (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_44 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_44 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_44 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_45 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_45 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_45 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_16  (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_46 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_46 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_46 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_47 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_47 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_47 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_48 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_48 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_48 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_17  (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_49 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_49 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_49 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_50 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_50 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_50 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_51 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_51 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_51 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_18  (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_52 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_52 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_52 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_53 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_53 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_53 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_54 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_54 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_54 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_19  (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpQe_mpa1Kwi",
        "outputId": "b178b432-f306-4cfe-9baf-e58648f0791e"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 4.1919 - accuracy: 0.0468\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 3.5439 - accuracy: 0.1252\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 51s 519ms/step - loss: 3.1411 - accuracy: 0.1871\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 2.8582 - accuracy: 0.2355\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 2.6655 - accuracy: 0.2713\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 2.5038 - accuracy: 0.2997\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 2.3625 - accuracy: 0.3303\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 2.2594 - accuracy: 0.3435\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 2.1194 - accuracy: 0.3706\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 2.0394 - accuracy: 0.3887\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 48s 491ms/step - loss: 2.0082 - accuracy: 0.4045\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 48s 491ms/step - loss: 1.9145 - accuracy: 0.4155\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 48s 491ms/step - loss: 1.8754 - accuracy: 0.4265\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 49s 494ms/step - loss: 1.8511 - accuracy: 0.4277\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.7874 - accuracy: 0.4400\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.7521 - accuracy: 0.4590\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.7045 - accuracy: 0.4600\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 49s 494ms/step - loss: 1.6855 - accuracy: 0.4635\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.6871 - accuracy: 0.4694\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 48s 491ms/step - loss: 1.6301 - accuracy: 0.4845\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 48s 490ms/step - loss: 1.6008 - accuracy: 0.4819\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.5727 - accuracy: 0.4952\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 48s 491ms/step - loss: 1.5868 - accuracy: 0.4852\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 48s 490ms/step - loss: 1.5698 - accuracy: 0.4823\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 48s 490ms/step - loss: 1.5612 - accuracy: 0.5074\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.5278 - accuracy: 0.5026\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 50s 513ms/step - loss: 1.5128 - accuracy: 0.5229\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 49s 501ms/step - loss: 1.4966 - accuracy: 0.5194\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.4829 - accuracy: 0.5184\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.4915 - accuracy: 0.5052\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.4643 - accuracy: 0.5300\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.4381 - accuracy: 0.5297\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.4181 - accuracy: 0.5284\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.3966 - accuracy: 0.5394\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.4090 - accuracy: 0.5365\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.3946 - accuracy: 0.5358\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.4021 - accuracy: 0.5368\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.3801 - accuracy: 0.5361\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 49s 498ms/step - loss: 1.4075 - accuracy: 0.5329\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4007 - accuracy: 0.5413\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.3570 - accuracy: 0.5377\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 49s 496ms/step - loss: 1.3613 - accuracy: 0.5487\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.3207 - accuracy: 0.5506\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.3499 - accuracy: 0.5519\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.3262 - accuracy: 0.5565\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.3083 - accuracy: 0.5619\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 50s 507ms/step - loss: 1.3164 - accuracy: 0.5635\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 49s 500ms/step - loss: 1.3209 - accuracy: 0.5500\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 49s 499ms/step - loss: 1.3131 - accuracy: 0.5594\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 49s 497ms/step - loss: 1.3456 - accuracy: 0.5590\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 49s 494ms/step - loss: 1.3358 - accuracy: 0.5471\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 48s 493ms/step - loss: 1.3178 - accuracy: 0.5471\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.2967 - accuracy: 0.5697\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 49s 496ms/step - loss: 1.2896 - accuracy: 0.5648\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 48s 492ms/step - loss: 1.3181 - accuracy: 0.5506\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 49s 502ms/step - loss: 1.2999 - accuracy: 0.5629\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.2527 - accuracy: 0.5726\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.2472 - accuracy: 0.5732\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 51s 524ms/step - loss: 1.2621 - accuracy: 0.5723\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2812 - accuracy: 0.5581\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.2547 - accuracy: 0.5665\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.2382 - accuracy: 0.5784\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2317 - accuracy: 0.5829\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 51s 520ms/step - loss: 1.2481 - accuracy: 0.5729\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.2303 - accuracy: 0.5800\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 51s 522ms/step - loss: 1.2442 - accuracy: 0.5874\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 1.2838 - accuracy: 0.5661\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.2690 - accuracy: 0.5635\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 51s 523ms/step - loss: 1.2658 - accuracy: 0.5648\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 51s 518ms/step - loss: 1.2705 - accuracy: 0.5639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f58a0755c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pkgwo2d1RPA",
        "outputId": "0924c039-4a2a-44ab-d93b-640cff7a97fb"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 21s 498ms/step - loss: 1.4390 - accuracy: 0.5298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4390194416046143, 0.5297691226005554]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSQIehVMKfes",
        "outputId": "12024ab8-3dd0-467e-85ad-a3618a8cb001"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 19s 524ms/step - loss: 1.3928 - accuracy: 0.5612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.392838954925537, 0.5612499713897705]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMk133q4KiAU"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg16-half-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9tf9HwkG9D5"
      },
      "source": [
        "### VGG-16 Freeze a fourth of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch1uj50vKljH",
        "outputId": "986a5f6e-3f1b-4dec-c72d-01613525e3d1"
      },
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "len = len(base_model.layers) // 4\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:len]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj_9WpwAKs_d",
        "outputId": "639b37fa-c7d7-43b6-b9ba-c00051801635"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 84s 688ms/step - loss: 4.2022 - accuracy: 0.0442\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 53s 540ms/step - loss: 3.5476 - accuracy: 0.1345\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 3.1795 - accuracy: 0.1897\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 2.8851 - accuracy: 0.2316\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 2.6660 - accuracy: 0.2674\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 52s 532ms/step - loss: 2.5105 - accuracy: 0.2965\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 2.3593 - accuracy: 0.3297\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 2.2389 - accuracy: 0.3365\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 2.1464 - accuracy: 0.3603\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 2.0630 - accuracy: 0.3865\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.9663 - accuracy: 0.4081\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.8972 - accuracy: 0.4294\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 52s 530ms/step - loss: 1.8905 - accuracy: 0.4139\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.8339 - accuracy: 0.4277\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 53s 534ms/step - loss: 1.7982 - accuracy: 0.4335\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 1.7234 - accuracy: 0.4590\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 51s 514ms/step - loss: 1.6900 - accuracy: 0.4648\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.6400 - accuracy: 0.4774\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.6410 - accuracy: 0.4723\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.6333 - accuracy: 0.4765\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 51s 516ms/step - loss: 1.6409 - accuracy: 0.4816\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.5979 - accuracy: 0.4797\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.5634 - accuracy: 0.4990\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 52s 533ms/step - loss: 1.5488 - accuracy: 0.4990\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.5067 - accuracy: 0.5100\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.5127 - accuracy: 0.5116\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.5021 - accuracy: 0.5055\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 50s 511ms/step - loss: 1.5053 - accuracy: 0.5126\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 50s 508ms/step - loss: 1.4487 - accuracy: 0.5181\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.4647 - accuracy: 0.5277\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.4645 - accuracy: 0.5142\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 51s 517ms/step - loss: 1.4172 - accuracy: 0.5339\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.4149 - accuracy: 0.5287\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.4029 - accuracy: 0.5329\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.4231 - accuracy: 0.5274\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.3829 - accuracy: 0.5439\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.3747 - accuracy: 0.5452\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.3693 - accuracy: 0.5419\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 52s 530ms/step - loss: 1.3191 - accuracy: 0.5581\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.3367 - accuracy: 0.5487\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.3621 - accuracy: 0.5510\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.3329 - accuracy: 0.5552\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 51s 515ms/step - loss: 1.3609 - accuracy: 0.5487\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3385 - accuracy: 0.5600\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.3623 - accuracy: 0.5452\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 50s 512ms/step - loss: 1.3625 - accuracy: 0.5484\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3286 - accuracy: 0.5539\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 50s 509ms/step - loss: 1.3296 - accuracy: 0.5481\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 50s 514ms/step - loss: 1.3089 - accuracy: 0.5610\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.3088 - accuracy: 0.5648\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 50s 510ms/step - loss: 1.2809 - accuracy: 0.5687\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.3064 - accuracy: 0.5506\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.3216 - accuracy: 0.5516\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 52s 525ms/step - loss: 1.3021 - accuracy: 0.5639\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.3320 - accuracy: 0.5503\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 52s 529ms/step - loss: 1.2748 - accuracy: 0.5765\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 53s 537ms/step - loss: 1.2837 - accuracy: 0.5658\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 52s 526ms/step - loss: 1.2833 - accuracy: 0.5716\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 52s 531ms/step - loss: 1.2929 - accuracy: 0.5716\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 53s 537ms/step - loss: 1.2686 - accuracy: 0.5752\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.2866 - accuracy: 0.5700\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.2467 - accuracy: 0.5726\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 53s 537ms/step - loss: 1.2340 - accuracy: 0.5706\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 52s 526ms/step - loss: 1.2572 - accuracy: 0.5739\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 53s 538ms/step - loss: 1.2581 - accuracy: 0.5710\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 52s 534ms/step - loss: 1.2281 - accuracy: 0.5768\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 53s 536ms/step - loss: 1.1920 - accuracy: 0.5929\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 52s 527ms/step - loss: 1.2090 - accuracy: 0.5874\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 53s 535ms/step - loss: 1.2000 - accuracy: 0.5865\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 53s 537ms/step - loss: 1.1733 - accuracy: 0.5935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f780e4190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czfqS-mbLQty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emy5TwJ7cDAf",
        "outputId": "26ed4545-f72b-4af6-ec3e-2e636ab6bb6c"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 35s 1s/step - loss: 1.3851 - accuracy: 0.5213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3850960731506348, 0.5212636590003967]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eIs5J9IcDAp",
        "outputId": "14a3854e-d4a4-4ba9-b8fa-de5072847df4"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 19s 538ms/step - loss: 1.3317 - accuracy: 0.5475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3317179679870605, 0.5475000143051147]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEmJC6uncDAq"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg16-fourth-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSHFOC6VHAgt"
      },
      "source": [
        "### VGG-19 Freeze all of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ZN3KObfZhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d322b9bd-2fd6-4888-f938-0b0d9d99dcd7"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_13 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_14 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_15 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ef2lbgQ8luV",
        "outputId": "f616e079-5f8c-4465-9e6c-ac6c1342768e"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 498s 4s/step - loss: 4.1682 - accuracy: 0.0506\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 3.4912 - accuracy: 0.1326\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 3.0721 - accuracy: 0.2106\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 2.7983 - accuracy: 0.2513\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 60s 616ms/step - loss: 2.5948 - accuracy: 0.2787\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 2.4450 - accuracy: 0.3094\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 2.2850 - accuracy: 0.3413\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 2.1661 - accuracy: 0.3645\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 2.0748 - accuracy: 0.3742\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 2.0009 - accuracy: 0.3981\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.9292 - accuracy: 0.4081\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.8476 - accuracy: 0.4339\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.8154 - accuracy: 0.4348\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.7393 - accuracy: 0.4558\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.6901 - accuracy: 0.4713\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.7111 - accuracy: 0.4600\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.6402 - accuracy: 0.4819\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.6079 - accuracy: 0.4868\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.5820 - accuracy: 0.4868\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 60s 617ms/step - loss: 1.5779 - accuracy: 0.4926\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.5301 - accuracy: 0.4968\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.5028 - accuracy: 0.5077\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.4973 - accuracy: 0.5129\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.4818 - accuracy: 0.5152\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.4685 - accuracy: 0.5190\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.4480 - accuracy: 0.5203\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.4386 - accuracy: 0.5406\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.4357 - accuracy: 0.5255\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.4515 - accuracy: 0.5316\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.3978 - accuracy: 0.5342\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.3931 - accuracy: 0.5400\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.3531 - accuracy: 0.5642\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.3561 - accuracy: 0.5555\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.3266 - accuracy: 0.5561\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.3015 - accuracy: 0.5481\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.3142 - accuracy: 0.5677\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 61s 619ms/step - loss: 1.3219 - accuracy: 0.5626\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.2974 - accuracy: 0.5671\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 60s 613ms/step - loss: 1.2652 - accuracy: 0.5768\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.2888 - accuracy: 0.5606\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 61s 617ms/step - loss: 1.2689 - accuracy: 0.5690\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.2623 - accuracy: 0.5810\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2771 - accuracy: 0.5755\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 60s 608ms/step - loss: 1.2687 - accuracy: 0.5690\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.2452 - accuracy: 0.5823\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.2699 - accuracy: 0.5710\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.2589 - accuracy: 0.5710\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.2513 - accuracy: 0.5865\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 1.2281 - accuracy: 0.5865\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.2313 - accuracy: 0.5761\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.2503 - accuracy: 0.5784\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.2242 - accuracy: 0.5787\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.2375 - accuracy: 0.5703\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 61s 617ms/step - loss: 1.2061 - accuracy: 0.5977\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 60s 614ms/step - loss: 1.2176 - accuracy: 0.5845\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 60s 615ms/step - loss: 1.1924 - accuracy: 0.5887\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 59s 605ms/step - loss: 1.1913 - accuracy: 0.5971\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 61s 617ms/step - loss: 1.1898 - accuracy: 0.5900\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1868 - accuracy: 0.5935\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 58s 586ms/step - loss: 1.2139 - accuracy: 0.5900\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.1944 - accuracy: 0.5906\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 1.2098 - accuracy: 0.5832\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.1456 - accuracy: 0.5971\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1952 - accuracy: 0.5932\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 1.1801 - accuracy: 0.5965\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.1576 - accuracy: 0.5997\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.1559 - accuracy: 0.6103\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.1887 - accuracy: 0.5945\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.1843 - accuracy: 0.5984\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.1900 - accuracy: 0.5987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd1eb974950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqZxA3K8o7t",
        "outputId": "833ec5a6-41dd-4d2a-b099-5b0110978be1"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 195s 3s/step - loss: 1.2316 - accuracy: 0.5747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2315784692764282, 0.5747265815734863]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etLx1f3pSGCI",
        "outputId": "86047058-700b-4630-df8d-99b92f69d743"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 20s 559ms/step - loss: 1.1372 - accuracy: 0.6275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.13724946975708, 0.6274999976158142]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0wrUtULSHd_"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ierrwn-HELX"
      },
      "source": [
        "### VGG-19 Freeze half of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xyGSxlHSVLS",
        "outputId": "5e72ac89-e6e4-4adb-e1e2-3222f7ef36f1"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "len = len(base_model.layers) // 2\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:len]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_16 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_16 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_16 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_17 (TFOpLa (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_17 (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_17 (TFOpLambda)   (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_5 ( (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_18 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_18 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_18 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_19 (TFOpLa (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_19 (TFOpLambd (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_19 (TFOpLambda)   (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_6 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_20 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_20 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_20 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_21 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_21 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_21 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_22 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_22 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_22 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_23 (TFOpLa (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_23 (TFOpLambd (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_23 (TFOpLambda)   (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_7 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_24 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_24 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_24 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_25 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_25 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_25 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_26 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_26 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_26 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_27 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_27 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_27 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_8 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_28 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_28 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_28 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_29 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_29 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_29 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_30 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_30 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_30 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_31 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_31 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_31 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_9 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt_otxCOSXms",
        "outputId": "81aa2311-50df-4225-c35c-162fbdb2bfee"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 56s 560ms/step - loss: 4.1766 - accuracy: 0.0426\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 55s 564ms/step - loss: 3.5091 - accuracy: 0.1252\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 3.1543 - accuracy: 0.1881\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 2.8553 - accuracy: 0.2300\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 56s 571ms/step - loss: 2.6449 - accuracy: 0.2665\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 56s 571ms/step - loss: 2.4560 - accuracy: 0.2945\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 2.2874 - accuracy: 0.3387\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 58s 594ms/step - loss: 2.1927 - accuracy: 0.3584\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 2.1095 - accuracy: 0.3648\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 2.0008 - accuracy: 0.3903\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.9072 - accuracy: 0.4210\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.8636 - accuracy: 0.4303\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.7990 - accuracy: 0.4332\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.7642 - accuracy: 0.4471\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.7476 - accuracy: 0.4516\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.6992 - accuracy: 0.4690\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.6198 - accuracy: 0.4823\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.5932 - accuracy: 0.4894\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.5713 - accuracy: 0.4935\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.5381 - accuracy: 0.5052\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.5329 - accuracy: 0.5003\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.5228 - accuracy: 0.5123\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.5252 - accuracy: 0.4977\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.4557 - accuracy: 0.5239\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.4469 - accuracy: 0.5219\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 1.4444 - accuracy: 0.5190\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.4247 - accuracy: 0.5300\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.4140 - accuracy: 0.5297\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.3939 - accuracy: 0.5384\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.3869 - accuracy: 0.5381\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.4031 - accuracy: 0.5281\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.3759 - accuracy: 0.5403\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.3418 - accuracy: 0.5555\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.3174 - accuracy: 0.5597\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.3347 - accuracy: 0.5548\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.2953 - accuracy: 0.5639\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.3060 - accuracy: 0.5677\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.2958 - accuracy: 0.5652\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2925 - accuracy: 0.5665\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.2851 - accuracy: 0.5635\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.2652 - accuracy: 0.5774\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.2650 - accuracy: 0.5761\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.2509 - accuracy: 0.5884\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.2594 - accuracy: 0.5681\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.2592 - accuracy: 0.5806\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 57s 586ms/step - loss: 1.2585 - accuracy: 0.5716\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.2318 - accuracy: 0.5732\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.2252 - accuracy: 0.5842\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.2223 - accuracy: 0.5903\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 1.2385 - accuracy: 0.5816\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.2418 - accuracy: 0.5726\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.1862 - accuracy: 0.5913\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.2436 - accuracy: 0.5806\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.2479 - accuracy: 0.5677\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.2092 - accuracy: 0.5835\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.2163 - accuracy: 0.5858\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.1932 - accuracy: 0.5974\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 1.1803 - accuracy: 0.6084\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.2088 - accuracy: 0.5910\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1750 - accuracy: 0.5974\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.1664 - accuracy: 0.6042\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.1704 - accuracy: 0.5952\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.1577 - accuracy: 0.6094\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 57s 575ms/step - loss: 1.1626 - accuracy: 0.5910\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.1509 - accuracy: 0.5971\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.1552 - accuracy: 0.5984\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.1493 - accuracy: 0.6000\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.1693 - accuracy: 0.5897\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.1428 - accuracy: 0.6055\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1349 - accuracy: 0.6229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd18ebe0510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwQu42GYmMn9",
        "outputId": "c964aee4-2a06-4a46-924b-45a6cca16025"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 23s 554ms/step - loss: 1.2025 - accuracy: 0.5711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2025425434112549, 0.5710813999176025]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZeyllM7mMoJ",
        "outputId": "35df04d5-a7a5-4bc3-d430-2fb3ed2c6cf8"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 20s 582ms/step - loss: 1.1398 - accuracy: 0.6212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1398231983184814, 0.6212499737739563]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfICIHvKmMoK"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-half-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOfgOKiHH1F"
      },
      "source": [
        "### VGG-19 Freeze a fourth of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qT4_oOCmY3m",
        "outputId": "0f9880d5-c125-4b25-b632-94d1288803c5"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "len = len(base_model.layers) // 4\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:len]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_13 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_14 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_15 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaW72fHmmqw1",
        "outputId": "2ca44a77-e1e3-4bf4-ab70-0f288d1f2ff3"
      },
      "source": [
        "model.fit(training_set, epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "97/97 [==============================] - 85s 710ms/step - loss: 4.1722 - accuracy: 0.0545\n",
            "Epoch 2/70\n",
            "97/97 [==============================] - 56s 566ms/step - loss: 3.5033 - accuracy: 0.1252\n",
            "Epoch 3/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 3.0933 - accuracy: 0.1906\n",
            "Epoch 4/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 2.8175 - accuracy: 0.2432\n",
            "Epoch 5/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 2.6011 - accuracy: 0.2739\n",
            "Epoch 6/70\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 2.4214 - accuracy: 0.3171\n",
            "Epoch 7/70\n",
            "97/97 [==============================] - 58s 589ms/step - loss: 2.2601 - accuracy: 0.3419\n",
            "Epoch 8/70\n",
            "97/97 [==============================] - 59s 602ms/step - loss: 2.1605 - accuracy: 0.3723\n",
            "Epoch 9/70\n",
            "97/97 [==============================] - 58s 588ms/step - loss: 2.0626 - accuracy: 0.3871\n",
            "Epoch 10/70\n",
            "97/97 [==============================] - 58s 587ms/step - loss: 1.9453 - accuracy: 0.4052\n",
            "Epoch 11/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.8882 - accuracy: 0.4142\n",
            "Epoch 12/70\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 1.8600 - accuracy: 0.4261\n",
            "Epoch 13/70\n",
            "97/97 [==============================] - 58s 592ms/step - loss: 1.7755 - accuracy: 0.4435\n",
            "Epoch 14/70\n",
            "97/97 [==============================] - 59s 600ms/step - loss: 1.7042 - accuracy: 0.4710\n",
            "Epoch 15/70\n",
            "97/97 [==============================] - 58s 592ms/step - loss: 1.6805 - accuracy: 0.4765\n",
            "Epoch 16/70\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.6389 - accuracy: 0.4735\n",
            "Epoch 17/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.6034 - accuracy: 0.4916\n",
            "Epoch 18/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.5816 - accuracy: 0.5000\n",
            "Epoch 19/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.5627 - accuracy: 0.4961\n",
            "Epoch 20/70\n",
            "97/97 [==============================] - 58s 594ms/step - loss: 1.5088 - accuracy: 0.5097\n",
            "Epoch 21/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.4927 - accuracy: 0.5126\n",
            "Epoch 22/70\n",
            "97/97 [==============================] - 58s 594ms/step - loss: 1.4779 - accuracy: 0.5174\n",
            "Epoch 23/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.4642 - accuracy: 0.5148\n",
            "Epoch 24/70\n",
            "97/97 [==============================] - 58s 591ms/step - loss: 1.4475 - accuracy: 0.5242\n",
            "Epoch 25/70\n",
            "97/97 [==============================] - 57s 575ms/step - loss: 1.4161 - accuracy: 0.5403\n",
            "Epoch 26/70\n",
            "97/97 [==============================] - 57s 575ms/step - loss: 1.4286 - accuracy: 0.5171\n",
            "Epoch 27/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.4023 - accuracy: 0.5358\n",
            "Epoch 28/70\n",
            "97/97 [==============================] - 56s 575ms/step - loss: 1.3921 - accuracy: 0.5365\n",
            "Epoch 29/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.3616 - accuracy: 0.5429\n",
            "Epoch 30/70\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.3152 - accuracy: 0.5506\n",
            "Epoch 31/70\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.3272 - accuracy: 0.5565\n",
            "Epoch 32/70\n",
            "97/97 [==============================] - 58s 596ms/step - loss: 1.3229 - accuracy: 0.5568\n",
            "Epoch 33/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.3644 - accuracy: 0.5345\n",
            "Epoch 34/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.3268 - accuracy: 0.5529\n",
            "Epoch 35/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.3137 - accuracy: 0.5442\n",
            "Epoch 36/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.3013 - accuracy: 0.5539\n",
            "Epoch 37/70\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.3127 - accuracy: 0.5513\n",
            "Epoch 38/70\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.2948 - accuracy: 0.5645\n",
            "Epoch 39/70\n",
            "97/97 [==============================] - 59s 604ms/step - loss: 1.2410 - accuracy: 0.5777\n",
            "Epoch 40/70\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.2773 - accuracy: 0.5713\n",
            "Epoch 41/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.2915 - accuracy: 0.5626\n",
            "Epoch 42/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.2946 - accuracy: 0.5652\n",
            "Epoch 43/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.2608 - accuracy: 0.5668\n",
            "Epoch 44/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.2479 - accuracy: 0.5774\n",
            "Epoch 45/70\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.2624 - accuracy: 0.5729\n",
            "Epoch 46/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.2375 - accuracy: 0.5826\n",
            "Epoch 47/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.2430 - accuracy: 0.5661\n",
            "Epoch 48/70\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.2559 - accuracy: 0.5690\n",
            "Epoch 49/70\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.2210 - accuracy: 0.5897\n",
            "Epoch 50/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.2358 - accuracy: 0.5655\n",
            "Epoch 51/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.2239 - accuracy: 0.5819\n",
            "Epoch 52/70\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1902 - accuracy: 0.5884\n",
            "Epoch 53/70\n",
            "97/97 [==============================] - 58s 590ms/step - loss: 1.1266 - accuracy: 0.6103\n",
            "Epoch 54/70\n",
            "97/97 [==============================] - 58s 592ms/step - loss: 1.1499 - accuracy: 0.5984\n",
            "Epoch 55/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.1681 - accuracy: 0.6003\n",
            "Epoch 56/70\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.1591 - accuracy: 0.6097\n",
            "Epoch 57/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1937 - accuracy: 0.5819\n",
            "Epoch 58/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.1856 - accuracy: 0.5903\n",
            "Epoch 59/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1651 - accuracy: 0.5923\n",
            "Epoch 60/70\n",
            "97/97 [==============================] - 57s 583ms/step - loss: 1.1697 - accuracy: 0.5874\n",
            "Epoch 61/70\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.1641 - accuracy: 0.5958\n",
            "Epoch 62/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1734 - accuracy: 0.5935\n",
            "Epoch 63/70\n",
            "97/97 [==============================] - 59s 600ms/step - loss: 1.1427 - accuracy: 0.6032\n",
            "Epoch 64/70\n",
            "97/97 [==============================] - 59s 606ms/step - loss: 1.1347 - accuracy: 0.6029\n",
            "Epoch 65/70\n",
            "97/97 [==============================] - 57s 584ms/step - loss: 1.1790 - accuracy: 0.5919\n",
            "Epoch 66/70\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.1613 - accuracy: 0.5994\n",
            "Epoch 67/70\n",
            "97/97 [==============================] - 58s 586ms/step - loss: 1.1522 - accuracy: 0.5997\n",
            "Epoch 68/70\n",
            "97/97 [==============================] - 57s 585ms/step - loss: 1.1541 - accuracy: 0.6094\n",
            "Epoch 69/70\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.1336 - accuracy: 0.5961\n",
            "Epoch 70/70\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.1631 - accuracy: 0.6068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0f3e552990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGwhijEsmu58",
        "outputId": "2a5f7514-7458-4a5d-b67f-b90e533a610d"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 35s 1s/step - loss: 1.2539 - accuracy: 0.5881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2539360523223877, 0.5880923271179199]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiymOiEP-OfF",
        "outputId": "b22cb6c5-d520-49e1-b185-4af1cd930b4f"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 20s 561ms/step - loss: 1.1267 - accuracy: 0.5987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1266990900039673, 0.5987499952316284]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxlsVQl-QqB"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights/article'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-fourth-froze.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27S-eDBKHNSw"
      },
      "source": [
        "### Final model, VGG-19 with a fourth of the layers frozen, trained at 160 epochs (trained in two different 80 epoch sessions because of Colab limits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiME7J3J-UV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b545fc-ffca-4d2f-cb0b-60043a302ee8"
      },
      "source": [
        "vgg = VGG19(include_top=False, weights=\"imagenet\", input_shape=(img_size, img_size, channels))\n",
        "vgg.trainable = True\n",
        "base_model = vgg\n",
        "\n",
        "len = len(base_model.layers) // 4\n",
        "\n",
        "# freeze layers\n",
        "for layer in base_model.layers[:len]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(img_size, img_size, channels))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "X = keras.layers.Dense(128, activation='relu')(x)\n",
        "X = keras.layers.Dropout(0.5)(X)\n",
        "X = keras.layers.BatchNormalization()(X)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(64, activation=tf.keras.activations.softmax)(X)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 400, 400, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambd (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add (TFOpLambda)  (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)      (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLam (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_1 (TFOpLambda (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)    (None, 400, 400, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TF (None, 200, 200, 64)      0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_2 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLam (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_3 (TFOpLambda (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)    (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 ( (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_4 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_5 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_6 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLam (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_7 (TFOpLambda (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)    (None, 100, 100, 256)     0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_2 ( (None, 50, 50, 256)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_8 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLam (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_9 (TFOpLambda (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)    (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_10 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLa (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_11 (TFOpLambd (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)   (None, 50, 50, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_3 ( (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_12 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_13 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_14 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLa (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.bias_add_15 (TFOpLambd (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)   (None, 25, 25, 512)       0         \n",
            "_________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_4 ( (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 74,432\n",
            "Trainable params: 74,176\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2cNC9ha97on",
        "outputId": "b09ed483-188e-4a3b-de63-eb1c9b3a9de1"
      },
      "source": [
        "model.fit(training_set, epochs=80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "97/97 [==============================] - 640s 6s/step - loss: 4.1646 - accuracy: 0.0513\n",
            "Epoch 2/80\n",
            "97/97 [==============================] - 64s 653ms/step - loss: 3.4758 - accuracy: 0.1400\n",
            "Epoch 3/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 3.0628 - accuracy: 0.2206\n",
            "Epoch 4/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 2.7875 - accuracy: 0.2561\n",
            "Epoch 5/80\n",
            "97/97 [==============================] - 63s 643ms/step - loss: 2.5740 - accuracy: 0.2939\n",
            "Epoch 6/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 2.3994 - accuracy: 0.3097\n",
            "Epoch 7/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 2.2869 - accuracy: 0.3226\n",
            "Epoch 8/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 2.1616 - accuracy: 0.3690\n",
            "Epoch 9/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 2.0360 - accuracy: 0.3906\n",
            "Epoch 10/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.9389 - accuracy: 0.4119\n",
            "Epoch 11/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.8888 - accuracy: 0.4119\n",
            "Epoch 12/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.8101 - accuracy: 0.4439\n",
            "Epoch 13/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.8023 - accuracy: 0.4394\n",
            "Epoch 14/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.7326 - accuracy: 0.4529\n",
            "Epoch 15/80\n",
            "97/97 [==============================] - 62s 637ms/step - loss: 1.6805 - accuracy: 0.4758\n",
            "Epoch 16/80\n",
            "97/97 [==============================] - 63s 642ms/step - loss: 1.6478 - accuracy: 0.4913\n",
            "Epoch 17/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.6221 - accuracy: 0.4874\n",
            "Epoch 18/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.5718 - accuracy: 0.4919\n",
            "Epoch 19/80\n",
            "97/97 [==============================] - 62s 634ms/step - loss: 1.5264 - accuracy: 0.5081\n",
            "Epoch 20/80\n",
            "97/97 [==============================] - 62s 630ms/step - loss: 1.5041 - accuracy: 0.5074\n",
            "Epoch 21/80\n",
            "97/97 [==============================] - 62s 628ms/step - loss: 1.4911 - accuracy: 0.5190\n",
            "Epoch 22/80\n",
            "97/97 [==============================] - 60s 611ms/step - loss: 1.5003 - accuracy: 0.5110\n",
            "Epoch 23/80\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.4334 - accuracy: 0.5248\n",
            "Epoch 24/80\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.4118 - accuracy: 0.5271\n",
            "Epoch 25/80\n",
            "97/97 [==============================] - 62s 636ms/step - loss: 1.4026 - accuracy: 0.5355\n",
            "Epoch 26/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.3967 - accuracy: 0.5439\n",
            "Epoch 27/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.3798 - accuracy: 0.5477\n",
            "Epoch 28/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.3495 - accuracy: 0.5532\n",
            "Epoch 29/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.3858 - accuracy: 0.5494\n",
            "Epoch 30/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.3623 - accuracy: 0.5452\n",
            "Epoch 31/80\n",
            "97/97 [==============================] - 63s 647ms/step - loss: 1.3389 - accuracy: 0.5500\n",
            "Epoch 32/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.3460 - accuracy: 0.5435\n",
            "Epoch 33/80\n",
            "97/97 [==============================] - 63s 647ms/step - loss: 1.3444 - accuracy: 0.5506\n",
            "Epoch 34/80\n",
            "97/97 [==============================] - 62s 637ms/step - loss: 1.3294 - accuracy: 0.5587\n",
            "Epoch 35/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.3045 - accuracy: 0.5677\n",
            "Epoch 36/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.2945 - accuracy: 0.5648\n",
            "Epoch 37/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.2719 - accuracy: 0.5616\n",
            "Epoch 38/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.2648 - accuracy: 0.5784\n",
            "Epoch 39/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.2422 - accuracy: 0.5700\n",
            "Epoch 40/80\n",
            "97/97 [==============================] - 61s 620ms/step - loss: 1.2557 - accuracy: 0.5758\n",
            "Epoch 41/80\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.2853 - accuracy: 0.5684\n",
            "Epoch 42/80\n",
            "97/97 [==============================] - 60s 609ms/step - loss: 1.2947 - accuracy: 0.5755\n",
            "Epoch 43/80\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.2644 - accuracy: 0.5774\n",
            "Epoch 44/80\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.2268 - accuracy: 0.5852\n",
            "Epoch 45/80\n",
            "97/97 [==============================] - 62s 637ms/step - loss: 1.2182 - accuracy: 0.5871\n",
            "Epoch 46/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.2210 - accuracy: 0.5813\n",
            "Epoch 47/80\n",
            "97/97 [==============================] - 63s 648ms/step - loss: 1.2435 - accuracy: 0.5732\n",
            "Epoch 48/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.2164 - accuracy: 0.5897\n",
            "Epoch 49/80\n",
            "97/97 [==============================] - 63s 640ms/step - loss: 1.1990 - accuracy: 0.5987\n",
            "Epoch 50/80\n",
            "97/97 [==============================] - 63s 641ms/step - loss: 1.2033 - accuracy: 0.5784\n",
            "Epoch 51/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.1895 - accuracy: 0.5881\n",
            "Epoch 52/80\n",
            "97/97 [==============================] - 63s 647ms/step - loss: 1.1715 - accuracy: 0.5965\n",
            "Epoch 53/80\n",
            "97/97 [==============================] - 63s 647ms/step - loss: 1.2077 - accuracy: 0.5797\n",
            "Epoch 54/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.2052 - accuracy: 0.5835\n",
            "Epoch 55/80\n",
            "97/97 [==============================] - 62s 637ms/step - loss: 1.1999 - accuracy: 0.5826\n",
            "Epoch 56/80\n",
            "97/97 [==============================] - 63s 639ms/step - loss: 1.1878 - accuracy: 0.5894\n",
            "Epoch 57/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.1845 - accuracy: 0.5929\n",
            "Epoch 58/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.1917 - accuracy: 0.5877\n",
            "Epoch 59/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.1985 - accuracy: 0.5900\n",
            "Epoch 60/80\n",
            "97/97 [==============================] - 64s 650ms/step - loss: 1.1712 - accuracy: 0.6029\n",
            "Epoch 61/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.2085 - accuracy: 0.5787\n",
            "Epoch 62/80\n",
            "97/97 [==============================] - 63s 647ms/step - loss: 1.1946 - accuracy: 0.6000\n",
            "Epoch 63/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.1824 - accuracy: 0.5890\n",
            "Epoch 64/80\n",
            "97/97 [==============================] - 64s 648ms/step - loss: 1.1741 - accuracy: 0.6019\n",
            "Epoch 65/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.1817 - accuracy: 0.6000\n",
            "Epoch 66/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.1783 - accuracy: 0.5990\n",
            "Epoch 67/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.1517 - accuracy: 0.5997\n",
            "Epoch 68/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.1786 - accuracy: 0.5929\n",
            "Epoch 69/80\n",
            "97/97 [==============================] - 64s 649ms/step - loss: 1.1521 - accuracy: 0.6055\n",
            "Epoch 70/80\n",
            "97/97 [==============================] - 62s 637ms/step - loss: 1.1417 - accuracy: 0.6055\n",
            "Epoch 71/80\n",
            "97/97 [==============================] - 64s 652ms/step - loss: 1.1986 - accuracy: 0.5835\n",
            "Epoch 72/80\n",
            "97/97 [==============================] - 63s 639ms/step - loss: 1.1500 - accuracy: 0.6029\n",
            "Epoch 73/80\n",
            "97/97 [==============================] - 63s 640ms/step - loss: 1.1575 - accuracy: 0.6026\n",
            "Epoch 74/80\n",
            "97/97 [==============================] - 64s 651ms/step - loss: 1.1227 - accuracy: 0.6029\n",
            "Epoch 75/80\n",
            "97/97 [==============================] - 63s 638ms/step - loss: 1.1362 - accuracy: 0.5939\n",
            "Epoch 76/80\n",
            "97/97 [==============================] - 62s 627ms/step - loss: 1.1091 - accuracy: 0.6132\n",
            "Epoch 77/80\n",
            "97/97 [==============================] - 61s 627ms/step - loss: 1.1224 - accuracy: 0.6126\n",
            "Epoch 78/80\n",
            "97/97 [==============================] - 62s 628ms/step - loss: 1.1463 - accuracy: 0.6023\n",
            "Epoch 79/80\n",
            "97/97 [==============================] - 62s 631ms/step - loss: 1.1472 - accuracy: 0.6026\n",
            "Epoch 80/80\n",
            "97/97 [==============================] - 60s 612ms/step - loss: 1.1364 - accuracy: 0.6065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa7e6062bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R15fGun8Z8a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Q0RikV-Y7v"
      },
      "source": [
        "import os\n",
        "weights_dir = 'weights'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-new-final.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQp0YSI11wMY"
      },
      "source": [
        "weights_dir = 'drive/MyDrive'\n",
        "model.load_weights(os.path.join(weights_dir, 'vgg19-new-final-final.h5'))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5dZybpWMkOz",
        "outputId": "204803db-c840-4716-8349-1db8808c22b1"
      },
      "source": [
        "model.fit(training_set, epochs=80)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "97/97 [==============================] - 642s 6s/step - loss: 1.2143 - accuracy: 0.5832\n",
            "Epoch 2/80\n",
            "97/97 [==============================] - 56s 568ms/step - loss: 1.2024 - accuracy: 0.5977\n",
            "Epoch 3/80\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.1514 - accuracy: 0.6029\n",
            "Epoch 4/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1705 - accuracy: 0.5971\n",
            "Epoch 5/80\n",
            "97/97 [==============================] - 59s 601ms/step - loss: 1.1545 - accuracy: 0.6061\n",
            "Epoch 6/80\n",
            "97/97 [==============================] - 58s 594ms/step - loss: 1.1578 - accuracy: 0.6042\n",
            "Epoch 7/80\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.1589 - accuracy: 0.6029\n",
            "Epoch 8/80\n",
            "97/97 [==============================] - 58s 593ms/step - loss: 1.1318 - accuracy: 0.6058\n",
            "Epoch 9/80\n",
            "97/97 [==============================] - 58s 595ms/step - loss: 1.1359 - accuracy: 0.6103\n",
            "Epoch 10/80\n",
            "97/97 [==============================] - 58s 596ms/step - loss: 1.1021 - accuracy: 0.6174\n",
            "Epoch 11/80\n",
            "97/97 [==============================] - 59s 601ms/step - loss: 1.1055 - accuracy: 0.6090\n",
            "Epoch 12/80\n",
            "97/97 [==============================] - 58s 595ms/step - loss: 1.1229 - accuracy: 0.6074\n",
            "Epoch 13/80\n",
            "97/97 [==============================] - 59s 603ms/step - loss: 1.1043 - accuracy: 0.6235\n",
            "Epoch 14/80\n",
            "97/97 [==============================] - 59s 598ms/step - loss: 1.1053 - accuracy: 0.6219\n",
            "Epoch 15/80\n",
            "97/97 [==============================] - 59s 601ms/step - loss: 1.1230 - accuracy: 0.6161\n",
            "Epoch 16/80\n",
            "97/97 [==============================] - 59s 602ms/step - loss: 1.1050 - accuracy: 0.6110\n",
            "Epoch 17/80\n",
            "97/97 [==============================] - 59s 601ms/step - loss: 1.0767 - accuracy: 0.6365\n",
            "Epoch 18/80\n",
            "97/97 [==============================] - 57s 580ms/step - loss: 1.1140 - accuracy: 0.6206\n",
            "Epoch 19/80\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.1376 - accuracy: 0.6103\n",
            "Epoch 20/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 1.0799 - accuracy: 0.6248\n",
            "Epoch 21/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0976 - accuracy: 0.6171\n",
            "Epoch 22/80\n",
            "97/97 [==============================] - 57s 581ms/step - loss: 1.0886 - accuracy: 0.6265\n",
            "Epoch 23/80\n",
            "97/97 [==============================] - 59s 597ms/step - loss: 1.0946 - accuracy: 0.6145\n",
            "Epoch 24/80\n",
            "97/97 [==============================] - 57s 575ms/step - loss: 1.1256 - accuracy: 0.6042\n",
            "Epoch 25/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.1131 - accuracy: 0.6100\n",
            "Epoch 26/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 1.0932 - accuracy: 0.6190\n",
            "Epoch 27/80\n",
            "97/97 [==============================] - 56s 567ms/step - loss: 1.0972 - accuracy: 0.6168\n",
            "Epoch 28/80\n",
            "97/97 [==============================] - 56s 567ms/step - loss: 1.0810 - accuracy: 0.6145\n",
            "Epoch 29/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0642 - accuracy: 0.6310\n",
            "Epoch 30/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0631 - accuracy: 0.6248\n",
            "Epoch 31/80\n",
            "97/97 [==============================] - 56s 567ms/step - loss: 1.0551 - accuracy: 0.6394\n",
            "Epoch 32/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.0547 - accuracy: 0.6271\n",
            "Epoch 33/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0772 - accuracy: 0.6281\n",
            "Epoch 34/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.0483 - accuracy: 0.6374\n",
            "Epoch 35/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 1.0390 - accuracy: 0.6323\n",
            "Epoch 36/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0671 - accuracy: 0.6290\n",
            "Epoch 37/80\n",
            "97/97 [==============================] - 56s 571ms/step - loss: 1.0320 - accuracy: 0.6348\n",
            "Epoch 38/80\n",
            "97/97 [==============================] - 56s 567ms/step - loss: 1.0524 - accuracy: 0.6335\n",
            "Epoch 39/80\n",
            "97/97 [==============================] - 56s 565ms/step - loss: 1.0806 - accuracy: 0.6377\n",
            "Epoch 40/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0723 - accuracy: 0.6139\n",
            "Epoch 41/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0394 - accuracy: 0.6397\n",
            "Epoch 42/80\n",
            "97/97 [==============================] - 56s 568ms/step - loss: 1.0394 - accuracy: 0.6397\n",
            "Epoch 43/80\n",
            "97/97 [==============================] - 55s 564ms/step - loss: 1.0671 - accuracy: 0.6239\n",
            "Epoch 44/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0317 - accuracy: 0.6448\n",
            "Epoch 45/80\n",
            "97/97 [==============================] - 56s 568ms/step - loss: 1.0525 - accuracy: 0.6394\n",
            "Epoch 46/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0246 - accuracy: 0.6465\n",
            "Epoch 47/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 1.0214 - accuracy: 0.6461\n",
            "Epoch 48/80\n",
            "97/97 [==============================] - 56s 568ms/step - loss: 1.0567 - accuracy: 0.6258\n",
            "Epoch 49/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.0724 - accuracy: 0.6329\n",
            "Epoch 50/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0269 - accuracy: 0.6435\n",
            "Epoch 51/80\n",
            "97/97 [==============================] - 56s 567ms/step - loss: 1.0312 - accuracy: 0.6368\n",
            "Epoch 52/80\n",
            "97/97 [==============================] - 56s 565ms/step - loss: 1.0134 - accuracy: 0.6455\n",
            "Epoch 53/80\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.0449 - accuracy: 0.6297\n",
            "Epoch 54/80\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.0254 - accuracy: 0.6377\n",
            "Epoch 55/80\n",
            "97/97 [==============================] - 56s 571ms/step - loss: 1.0215 - accuracy: 0.6426\n",
            "Epoch 56/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.0219 - accuracy: 0.6416\n",
            "Epoch 57/80\n",
            "97/97 [==============================] - 56s 576ms/step - loss: 1.0058 - accuracy: 0.6494\n",
            "Epoch 58/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0338 - accuracy: 0.6323\n",
            "Epoch 59/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0373 - accuracy: 0.6371\n",
            "Epoch 60/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.0367 - accuracy: 0.6384\n",
            "Epoch 61/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.0435 - accuracy: 0.6355\n",
            "Epoch 62/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0249 - accuracy: 0.6387\n",
            "Epoch 63/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.0056 - accuracy: 0.6461\n",
            "Epoch 64/80\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.0364 - accuracy: 0.6394\n",
            "Epoch 65/80\n",
            "97/97 [==============================] - 56s 568ms/step - loss: 1.0052 - accuracy: 0.6361\n",
            "Epoch 66/80\n",
            "97/97 [==============================] - 56s 576ms/step - loss: 1.0079 - accuracy: 0.6358\n",
            "Epoch 67/80\n",
            "97/97 [==============================] - 56s 570ms/step - loss: 0.9957 - accuracy: 0.6477\n",
            "Epoch 68/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.0133 - accuracy: 0.6361\n",
            "Epoch 69/80\n",
            "97/97 [==============================] - 56s 571ms/step - loss: 1.0441 - accuracy: 0.6368\n",
            "Epoch 70/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0222 - accuracy: 0.6390\n",
            "Epoch 71/80\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.0064 - accuracy: 0.6519\n",
            "Epoch 72/80\n",
            "97/97 [==============================] - 57s 579ms/step - loss: 1.0062 - accuracy: 0.6500\n",
            "Epoch 73/80\n",
            "97/97 [==============================] - 56s 572ms/step - loss: 1.0354 - accuracy: 0.6429\n",
            "Epoch 74/80\n",
            "97/97 [==============================] - 57s 578ms/step - loss: 1.0430 - accuracy: 0.6335\n",
            "Epoch 75/80\n",
            "97/97 [==============================] - 57s 577ms/step - loss: 1.0476 - accuracy: 0.6229\n",
            "Epoch 76/80\n",
            "97/97 [==============================] - 57s 582ms/step - loss: 1.0552 - accuracy: 0.6429\n",
            "Epoch 77/80\n",
            "97/97 [==============================] - 57s 576ms/step - loss: 1.0225 - accuracy: 0.6355\n",
            "Epoch 78/80\n",
            "97/97 [==============================] - 56s 573ms/step - loss: 1.0222 - accuracy: 0.6471\n",
            "Epoch 79/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 1.0212 - accuracy: 0.6452\n",
            "Epoch 80/80\n",
            "97/97 [==============================] - 56s 569ms/step - loss: 0.9990 - accuracy: 0.6526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdccd814e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmZpJMr1CiWE"
      },
      "source": [
        "import os\n",
        "weights_dir = 'drive/MyDrive'\n",
        "# Save model weights\n",
        "model.save_weights(os.path.join(weights_dir, 'vgg19-new-final-final.h5'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzE6r8mTCnUb",
        "outputId": "08869e5e-28a2-44be-c5f3-4645ecb7ac6b"
      },
      "source": [
        "model.evaluate(valid_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 261s 4s/step - loss: 1.2660 - accuracy: 0.5565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.26601243019104, 0.5565006136894226]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02yyml52Mgao",
        "outputId": "4bdcef51-53fb-4df3-b121-268c94f01f20"
      },
      "source": [
        "model.evaluate(testing_set, verbose=1) #160"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 20s 577ms/step - loss: 1.1321 - accuracy: 0.6288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1321409940719604, 0.6287500262260437]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnX-9G3qHdNB"
      },
      "source": [
        "### Some data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAj-Ho7p-k5h"
      },
      "source": [
        "import os\n",
        "\n",
        "def count_files(path):\n",
        "  dirs = []\n",
        "  for file in os.listdir(path):\n",
        "    dirs.append(file)\n",
        "\n",
        "  dirs = sorted(dirs)\n",
        "\n",
        "  num_files = []\n",
        "  for dir in dirs:\n",
        "    count = 0\n",
        "    for file in os.listdir(path + '/' + dir):\n",
        "      count += 1\n",
        "    num_files.append(count)\n",
        "\n",
        "  return num_files"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHqacdUeAwRB"
      },
      "source": [
        "sorted_labels = sorted(labels)\n",
        "num_training = count_files(training_dir)\n",
        "num_testing = count_files(testing_dir)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZJxNZ-DGeoj",
        "outputId": "72018b27-2a49-46e2-f8d0-a96bb4556b61"
      },
      "source": [
        "for file in os.listdir(training_dir + '/+circle'):\n",
        "  print(file)\n",
        "  img = plt.imread(training_dir + '/E/' + file)\n",
        "  print(img.shape)\n",
        "  break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2-1-45.Tif\n",
            "(400, 400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "P5lFBDHMA8sw",
        "outputId": "03d5ab50-3dc2-4f7e-f00b-7ace659f802f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "gs = gridspec.GridSpec(2, 1, figure=fig)\n",
        "# Display original image\n",
        "plt.subplot(gs[0]) \n",
        "plt.bar(sorted_labels, num_training) \n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title(\"Training Data Labels Count\")\n",
        "\n",
        "plt.subplot(gs[1]) \n",
        "plt.bar(sorted_labels, num_testing) \n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title(\"Testing Data Labels Count\")\n",
        "\n",
        "fig.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "XCzEmcHl7-Xl",
        "outputId": "29d38d2a-faa0-4dce-cc3d-d3370a13213f"
      },
      "source": [
        "epochs = [i + 1 for i in range(160)]\n",
        "epoch_ticks = [i for i in range(0, 165, 10)] \n",
        "epoch_ticks[0] = 1\n",
        "loss = [4.1646, 3.4758, 3.0628, 2.7875, 2.5740, 2.3994, 2.2869, \n",
        "        2.1616, 2.0360, 1.9389, 1.8888, 1.8101, 1.8023, 1.7326, \n",
        "        1.6805, 1.6478, 1.6221, 1.5718, 1.5264, 1.5041, 1.4911, \n",
        "        1.5003, 1.4334, 1.4118, 1.4026, 1.3967, 1.3798, 1.3495, \n",
        "        1.3858, 1.3623, 1.3389, 1.3460, 1.3444, 1.3294, 1.3045, \n",
        "        1.2945, 1.2719, 1.2648, 1.2422, 1.2557, 1.2853, 1.2947, \n",
        "        1.2644, 1.2268, 1.2182, 1.2210, 1.2435, 1.2164, 1.1990, \n",
        "        1.2033, 1.1895, 1.1715, 1.2077, 1.2052, 1.1999, 1.1878, \n",
        "        1.1845, 1.1917, 1.1985, 1.1712, 1.2085, 1.1946, 1.1824, \n",
        "        1.1741, 1.1817, 1.1783, 1.1517, 1.1786, 1.1521, 1.1417, \n",
        "        1.1986, 1.1500, 1.1575, 1.1227, 1.1362, 1.1091, 1.1224, \n",
        "        1.1463, 1.1472, 1.1364, 1.2143,  1.2024, 1.1514, 1.1705, \n",
        "        1.1545, 1.1578, 1.1589, 1.1318, 1.1359, 1.1021, 1.1055, \n",
        "        1.1229, 1.1043, 1.1053, 1.1230, 1.1050, 1.0767, 1.1140, \n",
        "        1.1376, 1.0799, 1.0976, 1.0886, 1.0946, 1.1256, 1.1131, \n",
        "        1.0932, 1.0972, 1.0810, 1.0642, 1.0631, 1.0551, 1.0547, \n",
        "        1.0772, 1.0483, 1.0390, 1.0671, 1.0320, 1.0524, 1.0806, \n",
        "        1.0723, 1.0394, 1.0394, 1.0671, 1.0317, 1.0525, 1.0246, \n",
        "        1.0214, 1.0567, 1.0724, 1.0269, 1.0312, 1.0134, 1.0449, \n",
        "        1.0254, 1.0215, 1.0219, 1.0058, 1.0338, 1.0373, 1.0367, \n",
        "        1.0435, 1.0249, 1.0056, 1.0364, 1.0052, 1.0079, 0.9957, \n",
        "        1.0133, 1.0441, 1.0222, 1.0064, 1.0062, 1.0354, 1.0430, \n",
        "        1.0476, 1.0552, 1.0225, 1.0222, 1.0212, 0.9990]\n",
        "\n",
        "# Graphs the learning curve\n",
        "plt.plot(epochs, loss)\n",
        "plt.title(\"Learning Curve for VGG-19 Mixing Data\")\n",
        "plt.xticks(epoch_ticks, rotation='vertical')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "cYtiGSOVHPVQ",
        "outputId": "40aaa97a-3a52-4714-ce57-8f292a90cb2c"
      },
      "source": [
        "import numpy as np\n",
        "img = plt.imread(training_dir + '/E/0-3-90.Png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "img = img.reshape(1, 400, 400, 3)\n",
        "print(\"Predict: \" + labels[np.argmax(np.array(model.predict(img)))])\n",
        "print(\"Truth label: K\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9TdMtOW4mBvDce+ujq0td/aXuUXhCljQRnoU3s/bCv9v+E/ZCoZFn0xGOsaW21B9SV3fVve8hvCABPABBZr5VZY8Wxar3nnMykyQI4uMByMxkEaHvy/fl+/Jvr7T/1gR8X74v35e6fK+c35fvy7/R8r1yfl++L/9Gy/fK+X35vvwbLd8r5/fl+/JvtLw5nfxf/15ERIiZhxZDYleISJiImYmIiFlISOx3LgzVGf61E/pT7GsqQrcTy8yjL2iIhQgbYB4/B7mZOiEdRs5m5/EpfxZqZ71xSuaxQGLoT79jU0qj9us0rUPW67mNFlmYdGizd6u3VBfxcQjjYRhHPSs4Fmv/O1gBKPtjola4E+zvTWOS3ie/eJ6PbSFf6yI0+DXrd6I+5ZuoEcmdNmJ/g476/P/0VzVzj8r5IKJnd0GlJFxMTAyaxbTomn/nSumEWJToWVMK4dGrk6AI6hP0SSJDxpDe1FbvqKB+oZBsjMNKh9KCCuqnopBr63ouGgYn1gWpWxuj7SEszMp7Whilx4aB83Gws5aIiJ5xNFGQaQhjOW7ox4UT5l/ngZl6P0vuyaBhf6vZXCqFNnUu1Jg5jWWVggak0Q04t2i4KnrvtYlykQQ4laNyfuhiDDIFQ48iU0F5CILIMNPcEjusgTnY8c3/nVIlkxMmePNPmkrykMox8Tz7BGFVRqjCgYUwURX3+jQVVI1xV4VgJp6E+gTjAPSgiX/gWxCmyaNI4PjdkuBEgW3OI3NJffKHFyFl4HEns3jGb7FrdLRibQj007gRP+BsFyJpZlxFhvnS9vR7gkXEj2GArDchIpbBVx3OtBpjPNPAoZdBFhNlNsPAfdwy+ctu/YxWBiTgiASmlEyqqIu4MWBl/ZRvnBMb32qkEqFw3b1yVE4VSrfcgwCEbF06MTG1wNjC1U4OMdfwNHif+S8TUxdRmRwCItNCYl2aHk+Y2FwmU75IZgMdBRaJmJ2ICHGb42RQ7HHRVH52AUpDpXRIhRfpZvXRsptMAe8KPJN4TfDYCw/ZPSYPtKCGksFEOuKA5k1aMyyZ/AEDUUFf5U0idyKaKfgTlZkC2AXGgeEBkaTZJsLbpX/tVy3T5LtM+N6aG76uMj2VrwNDehfi1oJCS+/TIVEoQx7wuIcKJislvbXXJLpQzgjVVgvvsIxAkIhE+rSaUwVUcYWo66GWIEsarVspUHoTLhdxMxw0GNumR3Qr6AIoVmfUbvOsOekZp/GcLGI0NLFkA8PGo1LtZ1tRwwT4Vyl3hMxSXoYC3MUJC1BK/wWb2UFAVHCJkEangUiGYofxrwSXx4wGHSd4fiYIP9ToR0VTeKy80LGpErngg5bPjkW9chdTBG48x6JccQjuCIXMEkifMiB9eH5SumbtNM0YriDKFDPo0dCfHOlROa0JJsowy7VejHFKgCUv+jBhw1oNX2HEg1XlRiFGYR4MHUx1GKZWUAc2mCSm6MRD6NQaC9A2WzDBUNQrMoL9cQFGasmjIb2MkxAvQIMVeCU0LQfUAF1VURHZJaPYJxiOkQghklnjcjGCHQF5+5VCYd0q4bVLgp1KFzW4QnFUnAzXOnalw+Vrzvs0+jne9wiIi3bJpgdSDUFOxoWDOvPyhkJU6RxJWp9GtysmGlhvPtK7K7eUczBFqE/zia7eCfLvKHTSJ0Tsk7ltDNCZKyQvo8KjtQljZephMyULk2o8cS0P0GOwxw0EOVqKtJLy3xRLlUOELCY1uDW1WdSzNkBOiCCAd6tgr/x1x4heDxUgJp3qEg1lLqWQoiutWgxGd0//NlHnV9gY7Leoko0jrYEx2dCJhi97pUQ59ZmxLahdQxL1M6mNQAuMzY6zzRg5MonG0+lbnZgbynpeX+E5iVprAQLZ1BYxB5FCjzFyDbAbwtQZu7TJrN4lWESERAYocH7VenUak93ErqfezYgw0Uw+rAWtnzM+Cxl4W/ZkiNIiosfEPOxQcHWx7m1rT7ROT7wmKl3wiiQmtDH8UALHtY9Hm4rgln3Qo/Mbj7uhGlAsKoAakZPp53AdZsTJwhG8BmknOBYNiBnOBH+Rnjp77mgvOxBFXNq+Q3iG9vAQk8y4x8NcVFBvT2kMKEUNPtHIAm/Kbc/Z2FIYxK2ZF43XRYKckQ5peu+Gu3Vgw8o1sCjGhWBpLMCmwVPp068KU1NoBFpj1pNRDIDOqUi4PouTzdqHQifxJZjBCCHuk062fxxWs8z1VRvO4tmWJEr4joaiL9e7kKdOkkWWSXe20q6YHK51Hu0EB+eoNsxOn8y5FcI9L1kZMzRX2rLHjvaAjc68DozefjW+s04wYNlIZHrzOEeYFw2BUDYWOyQT+VCXS+XM1qgPUDcR3tygMD2Uwd4k6MHTmpeJQmbfzMr4Ol9ryXILzZR8HhmbEdC2uioF0GFtgJuoPNpg/4SwKoQ6pXM+JzghktEXE9FTxIxZ45FB7l2IHnGSlWeNtdU4ltWD5xjQ1z7R81WKZogkFckLwtr7FqKO9rMnxVgLjXIcT+47XuTeHPvmJOTKv2Z0uiisiGQdA/DV0I3XzXGk0jNWGRyVaH96PPMRveU+tDkZwMullGLShCAoLlw2EJBlQT1gtIau/DHwH+eG4rs1JZobFxJdmhgS0ZPzc3a2i0FEZkbOlE//FXqq8tlEzL5mHSam1sYSzlM8Jrb4s4+1tzabtMV9dt5qNlFA4QK/E83REkcFYu4kAgm0QuH92oPJpuw5a++IicFME9Lbe6RFgGib02QMstdBGYsJq5VmZgrtZVnUtlrKndQ8IBvjOjb7Fj69zbYxVJHWXTkrZ/fKT3JBbZPhzbKsPDvqG4Y57b5Antd/fLsUkSty724tNfYRtc6zUZUjT5l7n42SUHXPx1qckT5F+kQJHudGJo/tjCKahZwXgTEg0U0GrviKh5WPNvni8MnWVzPdpErg/LImaY0Ld4bIBcgVLwqut439eP3RnxvYVfKQfl1y6GqQQE/ZwpuJP7ZCrEadl+P+vcHYcn1HGGPcnvH18ewz08GwhLYpnEOPWnvtdUyncgPWEkCV0ah6oR6WOvy81tlbJdz3iFb85OZxKaeGMbl9hSTuLSZEnjuflu1YWpszPM5CgvxxfOuJA7aMs9W3610x1TM0g6IQK0qcWDPeMEZtryo7KFfx6gxB1djtoJnTh8aLSEAehITbFKS1B2+zFvRdn8P7rYv8EZm5UUM5yBtlhvJGw47eLbd/cLiXhS/mTsvthJA2G61qTFnj3s+6IW0D4EaLioYDj1AJmViPCmPTtY0Rwz6nZxi7hZwu9FixAU/zr/xwz8Uy4ksSsnRfn9CpTwTCmpSZqMDi5qbb1yiwN8Mxdc6x73O5a6XPZU1i5XajF3JZsJBg0l+Et0N555xwG3zKZR8zz08wgIZ2NqNB42g5EG2LlW71xun64vc3KQtAKcqtmFMHQRQJRkUUMbBndXfEBx2YWRScyB3VBrNDNsY3kYegfFFg3A/EQD9Yc4hltHhSYFSQiXExc6yb930zxPSILWYncdyNlYdxkRr5Fjw0gbCdAEYq304pYzsxOQh8SX2VkG6jlBrPMQnBytPS7g7S9t5H7Dh3hem8hptXChZwYi5sNQCUU2fxkQ/Hdk9ls/MMy63te0Q+cILgnSyGqJRpxpCweE/qrdiZYIhudqIeJCh32m+pUMV38ZnKuQykWKXjxlIbUc3gUcb1fXpOsb69baK5QVrYvOZzTjCbARhjHtdNdgjeFbMaM+WzeqzW3MILiwWzZvHHSI/gNSdYdGloOgu4juIPY1dsvcvaV6jLPi5MvCwxHdBPMjgMu+sITo1DnVKc4JBVQvsuKQsvpmfODkGbneq4JJGcHIYeMg9gNjjXBbrJjdxu1m7BWm0o8B7+JVohli5ej8q5hlupwPmSkzq4CTeUfeVkKIXNpU49lsBGBi68OnptdgOg/KysqUPxLHBgvGbdTkQPnJswr+6xx9LUQAEYwxNND7HYF11/ltBWLpUHKAtIdNjIzeQ3IMw5w/25GI5gF24Q1n6Z3eghglBe5GuGyOvqJEo5md0nIjeCepV4fwuLzHGcAcnqOU985AvY6sjoFHfegrUyW+J0jGlVynHSoQoaC45SlSsBVBr1q4SAMxgFGmEJrEGFWAkUOjZblHgS42qEWN62e7kxfJn8SiZMiLgxLm1vKcB0P9JhBqk5pdWaJBe/Q/uB7kSOCjvIshDB3WZiClnSDj2anVObmqSW2VGHLiuRJXkSqugRXYT8hgKjaejZDJejlTBGNPR+gOiQta3WorNEa95AyMfKpjvOF6t/WNK6hLV9ekzJGwzse2zcvJzZN2fwENhVNNl+uKedYmPHTHHz8JA7zIGaCjKO33ZfBLbkn+YCnA/rGOFch3EmRdVGHhqvit4rgW1Hr4E3AdSwW+ZdM7o+mrKVk0/IUzCzRhYasrULVDECxscZ8Hif3BADCx/zh0z4wWbRhRSqm2ebX3oacxXvxbVOVwAXIbcsvmS1qFKAMmwGLxtfclqtjySJwfjMsdIcq9AibTC67ZnrpRSKzAmoyoR44gidKT0X4I0nkoZk6D1Z2mCbEywmOEYA+aAxTjPTYMKIzJK5b3GqQpoX3EKIeqSmR3x6Lxk07QIp1cozzUIaiyiSYX3FYM3QgEI6H6izTKU9JDYVuoPAheWd5BHR2i82ILBSK/j8jrZVObSdwUCN7x2qJgMpNitbnpKOf97ZxE2RkNPs40PoPI0jK6+EniL0htkmwUXT6SOgMdMamDKhgMkh4zr7ashR+QcL47rywmwol8rJFJXBMpRgLQdlxo1YHydke29kWmNSa2yDYuc7WCaZzO5gmbS/sSVuJui1bTOtuKsJabKgyqz9KUZjvc7IU3RQjBE+VWFb4pfDIB+LsQyExxMXAUOEvvDMGEcSCtB6vGcyUx3zl0gngVLM413GFkbbWjd6yI8sUaXWTd9q15saKh4HuxBx53mPrhtTLA6vcZPB7He21SK8SjGxjtLdTs4pRKZt9sua3VL6c/21TrB1RbmlnE7WaLFhiyAJINd1W4GQFYIqc10xKbpHmkkmZYJ5rThC9+w8oUpNEIavmeY7KXHkP+zMK6FoOaGB4hrKVTtk3GjM31P42+QLQ5MxQzvbKx54lUc2NvyrV4EJkVgHl31ERtKLtE+JimzVmf1JBmZVZSogKA8moIqCRsUMFgTKPHnTVIGDUMH4zbn4+A1hzHrBUWA/2gawyLuKZjLKwHnrHtFlzGmkj67M7Zt3tw70cQ/XJeMzOGOOM2sKfKqiskI6Ni83aNY4ZtLamfTWkADPzWKiR6/pPytWvA4/Y3zk3+u2ooLaTT9TiPP65+JlicLSEo4n0BOQjCznwxxTC0KKSxAuXM6z1qYXns6D1ymetxEOreUG7YrQc7bVnrpBQOv02b4bJOSlCJHYBuZp9mWyLnnNzMvx23kphgDFZYTZ9pTH691LY2hk4VmBFHXuGODxrhyfW7v1JtPQBCt16CUsw1x4VwvOdI1Ut37ow8Ns0J5c0vhMPUNmPlNWCoBiNrY9xAjQLSWDlIHNjIT/hfELJZqGMkaIFaFZYAvQF/IUOIAQKLIJkf1NKZKe+b/SG8MRAtiG/cgUMpltzJGpwS6ewOfjHYZz3AsrRGkuMbb0G7GJBP7T80p7Y6UHDH0SPLdParAjXTh3AvWghTBnLtvOr/FZCZPTrDScjP41rDWBXxUwZCR1wNudD7ZB69wfMQnLyA4zESf7gWunDHVMAZEuZcCoOI+BclmM4+th6EVOWVr9rUe6WlIVlEmdHtdJf0DMO7v0fy1jM2BykzWxUvKMZz82ZPCwC8936EDHNHmAMSpkKkOCSUln/2LKAZmq7LlNmeZ2y0atcDQRibU2blfsejvCvPuh6ZZRwf7IgEhwnDBOw00St33a0gY2qIQxGS93xt4uXIoLrMxdFldo7PKJ7yJobdaiiE1hDKv1Kq6/FcdNL8mtWXvaT6yukCIv1tNINKhFV68LQp5pqujaHduNYfGWYBXxM18Xx6WQqvL2Pg/ehoynP8zhJqywIprk2bRvnePKAIf2KAq3nSJKCCeuBYfsc0Zj5Nlloejp0AMjbXyEOcOAm5PP0BF47V52p2TL8OPhAHtwAup2Bnwn6/M0DKK7jymxPaPARXUSoYPuDFEIsplwO2TufbVyBJMZ7pYET6Lth5gsNBP7XlLltHrJymuWfAHlWRMECoIkeIXOfiuZCquNgch2tui/LTyaMXoTVeTJKlqQy8EwY7yIjys1/qQY2JoUkAc4hlzXudQnIATjFC81I0QkcNOXGsJRRz0bC9Ebfow2GhreyBK1ydpJ7xQeAhdYA8OUwEun404xWeCCptkRGtbW9K4uZEgst7fvEVMQxCWu45lJHZzTKrt+iWhO9Jwht6ziDIPHfmCsKtMI2EQnaKKd2+4UimK2U8Sdom1ZwhT7hPaHgIMHRAo4AU1VrCnYQmwP6qtoUmFm8vsU8bGNrsRxnMoJp9uFA3eqKKydv5BSwuck2Wl270iigLGF9jT+bI1tzsZGKqBVFJz78UVhFD0k0hTC63YgJjcaDlVhKNoAoAfPody/42TVgTmwTGBRp4X9nGs5K2eRcUJoodpPBAJhghaJxkeYiEINm8ioHF4S3JBBkyBeFyFOj5jXBIYqwdzjRESrpzsp5u6YwtvjKwck/kA/hItI3v/cxmaxeWyKMz0mTO7pZPLHtW9VaBR2EdhHLti+G1Y2hu6HqHI47CqD8oJsgIHSe131pgHWeH3S26dMcHd6rM9puxvpjQXjeMcNACa2aGQSiksAgyHvQAJDnvLsnlzpiLIRk2h7mbqp80R0FXMGgY/7PBWS6c2ueNwse3mcjAEM1n4pc67Qa7r3nIpGyocVv6uCqmKu8dtK30JCYPx6TEvmAWoAkg4qPie6kz6A22Iz0g0UZGhFDVtrbJlLVQJVWoXH51uROCiBehshN19CQs/efdsmDEljPx2NEPsGEHajMfgR+TjCfjCIPPIKVg/nQCG+VWGjoRJ0sNPkFoXMgyoqi6sL8c+civUHrpqwfR+TH5fw6bQuEwBjcaS0KxewNm4IqD3JGEAVc/mamaRzEtp9XTE/MvmIE4cM4QBZdtlXpE/X0Sr47uOMz51RD4ptN56ZRen+1Pt5fJDFwdip0KmQdFWW2feb1hQ9KnggmsrDPPbtmiFDjVoQjAtYN34lAzovjJDdja1CcBQ+NRR6qSnWvL1XvR0SEnicEIhxjN2r2VCqB0gR+S4tyzKfVwZ24YKhj+L6XV5hkSsi6zueGoyQDjfwH2zp5SYEREdRoATivNWNZ8EPg5tCKJu726+KxjTMq7VSulknklYrV9GnY40OMNPHi2JW6XCdnNaieMiEceotcmKLaO5LhbgMGw0CpYKBnWrPDD9BQc3DkPhuy0y+Kj7zfGi2ws7ooRRKq8NpbSi87ZSaNPTJQ032dCfWZIgbe5sm8NNby1h+cl82Jgk3RDAP5dTHTTETPJ9omZ443MVx5Ap1A1nWw/XO/FAfgWPv8VhVzrDWFpgTLDCi0uUgcPHYOrAMUfDcufgdL9hHiCFESEgfNrYy/5SNzdB37d3HtMJkgthQFHgPP8hk28iUq1266oLxqLVmt1DpIzNHn85fhZwqB10UHg0V0Jvcr1iJ8ZMKP3pTVMYcXyuUV973JIvLfl2J/ek5h8pAF7kBWI+PMTKmdufJMQa2d5Oq4ahKludvWipI6+fi7y7r41a+BawlwszVuj7HcYLndbijY15NizIV5VoxCaDqEPFHg7aZxnY96AcVdAdDIq01XQr5mOItTcHai0MuRB16q2F7+HYzZn8awlDo+cJXGjDvgYYQ6Ovg9QzRKF80bgXvQ7NtYgNa5l1Mn8AIjPYch3bpw3PxOqcmmKzwU9uDGLgscY4U5qMhr1BL7ktYec64PgAQeNc/tPoNFbTSifUaJEa1oJbJqhw958jIxiQQnA3WLTJWSTm3ndtEz3ulqJWXHif6WNOC9u9OQL5Ox36qH2mY3mS+toLmu160qMBiUmZWG1CN6thmGeLEowbjmysbxq7c4A8XydVF6legxRGA8w8TYnHs0dNqe0SauIpb4XJiDes6Up/Gw6CsVnAUZzwBOYNeSL/eMvbfUVE5C2yyGwcKBKbXH0Tz8kkIJ+HOLt1jvnonzq6NvNaIipXPE0x2rlNl0E7eEmnDfnaTqlOv/O7TMvoaIc/YZ14lT2qazDEo7O1h/O7vA51bEMp9qTyWEMiFdxCFij7WPst40qjkSl5mH+m7OEJC+kVkvFO3VF73wcb/2SDLeGM6Klas4x5D1U5YJq+9XQNjgFSIYEnpUE5hzWtL2Q4cEkhgOp2IavZt337A12tLFkRNJuV49FS2MSN1t6qFQt6l/QqanJTUm3fGV79R4UUIdoYg9OTwicmRNUM4mrdb4dB7CHiaQLr3lbi5jBe7E43jJm1LPK57bpkC9F56SSwaih55JvYv8BZ5LLmeKiiMrWsLkoeW6DnnHb5JCQjSewIPT4RBcLf+9zTc3yFEaHEGp1h43T1VeAg9vgT4BYN23i/EtNdbgss2lnN9LHW4PRbL1DeeSyIz+dJTOz3Q3+wYiQsYPnDbx1GP32LKuUAvasj0BUbEdh/t4G/a+pVthB1zQZXyImhDL5/K9ijO69y2xoYc9HWIOA5sa15lXjgbAJmPzx/DQUkWU768FZSFwmNHNJQSwl1PZ7T0/2UxuVPLRZPzdjP4YimXcqGcfmvB7jamYP9A0HZQFQnP36tyB5beqV+fa9Fu2eQDTC8E6qq/u544P6waGnEYCMLozAZIi5BpubF8WmaEfgFyxcuXLZDJqqtihhADbAMicY0nTRYoykdADgYApjLiYMFTtvmwXxGez2QalUVhOvvOKifxu/WQu1I5GQH+zCHMD33Rx3fkOZGZ9liRBZ5U9agkoFLQNu9EeT77/H59W83dUsWyDgIcQgoTNXi68bhViSyjqPV37S/0Crhi6HsUPQbJHaGRgVSde7B5sgBPBbK9QUEwPvc6dqAQFiQVx6N97QRIDQQTE/Xwwjx0ehYrW72iudGWDjRdIGQIp3eBB3KP8fj9GHuoeCc7+l2XXTza5usuv6Xn9Ce4E+FOGbJjA+IoMbi4GuFaZk6OqbJVzZB2B4HvKq9tiRtkT9kfFqwBJOsy7o5AL9KY5msc4lgyLUgPwpqHvs2b1AjFcZhiQlv4blFV1JxEaAndD53OkF8oCGuhkKUMsSIgPb8K92kHzmh8aI7aJ9ooCOMXiT/cW4+jLd9dEmJ68MY2jm+HvL7zImvItys3PKczySdohEJ+10dURKMjeavQKmcBrT3PSfnG2qD2W7dHRHarHXpM24RNulVuXNOmPepTg+0ca8ZyDDobEaTVbnHisbumJ5pMGXkdn8XVuoyj0JqikLrCwobtJRmEYQn2Ad+LxJJ2UKOiDJvT2RQL83y3DWerQApIK7p9kNF4wO4ucrkKcw10VOf/W5SIlMDoEM2tizVtr3riu39XwZuYX+oOcpy5eklZ3pu5i00r5rJu3iwmPlwvYk96G5PVx6MWG7ymgeddE0WsPH85fAzCEvvVrCwR2d5SmxCqbrWzb8E7avtmaQOyQERCKwpUBWFrItTV7qIgc1KszFPnyWRX3Tn0g0ilaieO9YbyiBtVpxvn2gf4b8VTBuOLOlQgkVxurnMS6eSFwiu0OVmpHVTNHmjA473CibuKsqzt4YZtJtu83qfCQL8oJK78yNcI7fNyTEAYI8Cg/jxNROIf/itzm+2CMrSelDLtUDB6JFVYFBEgem2K42VwaVFwM3y+qDKsPgSFvisV6/h2hrz6fXX8TtmFMN+ohHrXbVyuc3rCodrMHi3ZVVtKn8O6oZjjJbwu5IsNAGUmInvbmKAQt/w6whjTjnPa//T604N2HUsyLFnhWAN5KFEgY/2uz0Hi/CIfH2M4DoojQrYhfDuNrHB3VQYG54e7gwxO+cDGcaVHrxei7e1n1nZWjuSdt3Sjto9eB71ZE2NbO4Tm53aKIz7vaq+O8703AK8ta/tsBim/HCqXWy/Pnd+SYt5TSiSsijOJXDFPdbG0mWDJE9eSYtp3IsOYtm9U0MPWa7NErkz52uxRY31obxqwaDRojruvkzcVzjy9wWFXLCIXWlNmwmkWQ5whRg0Dy3xV46kVYXDZu+vzZMMd/2h8Y9wZjVwmGKyCfjHl97XNq1zEVTiEcuyJo/28x7bjb0du14rrBkXRlA6N0/m1vGoTQihc35WiHe4yqxavgldDhR8vm+1L/aAELqJL/KU3KzAPuDpf/QMbpiELO7+0KYhPWic9xspDeOMlI4aN25T3d93EsVSeQgfDRDxi5S5Oq+rCeAWfukNXEoWGzEKdBR4cN8VCyGEsAxoiNwzqWndekK3NgHiX8/hbUdZ+8R0710bcAFehys5BYAxay6Ir6B0fg3y4CqnKsE4A3YR2zuVVb7ZG+JbLXfdfJ3ZWRQ7MR4jFK7T2ek6azJPcME8YM6W7hMbOCmt/WWG9XV9a0ut2Yx6GCLfyidE7W6L47skY3S8tMo31UT2pxkF5MmEjel+kjUH63Ptl2AXtE7azkzS/Xo1GHADMzLLD6Lx7rPo9jqFcVGgtIi5U1JMMG3IRvY95HfMuPkUpQ0X3kLEut55bq436m6P9AcCngBmRT3gI9aE41AXvOSWKW7yrAa/HdUXisab4UG9N0dKySai2odJcjd8nTmnCYys0jJ61svg+1vkF1zRxLOyTyeoZm4TrlqQPkjKPZ3BamVc0hHr7GraLNIc+okPw64q+wvSboSfCfMI3jfHysk80mok2cd7mOnuPpslE5GJFx4Z+k9uhqlzISi433s+JpnclqoojbQH/VYM/QRWx26KIO5Ho7qEIzSYQmrrMQTBFnoRGRX0ResKTWDCv29tixnlVwF3slcc8zpHdmWIZZOmz0DkAACAASURBVPQu5fZJ6Mfdk5+HL9behJcZWgeDGuYh0tyKPbTYmRkMgCiooGv8fe21fLybvmk97s2pIOZkEBrmWH+nuKe+xzW7MdQyPSrt27sZc2Z4M5sHaxe/j87dQ6F3qzetS+KMJXcWOh7z+krYPQZlbdPIzt5exivip8cSFrtNSwBW4wLyLsbG33mJKL5sh8O1vXd7GJbGYhr/jXe80Li5AMYhnew5OWagVPAO6DLPXlZmhnkKbhfHKCicpukJBm94g+QkRuYw4lrI1Vm448h0Ve+/iZBZaM1yC9TNTodoPof/VryIZQ2TUDH32nlxs3WerLvFmciBgBr8qKCZcOdmqqbpbKmYEowK18Myi81xhLzq9cZSj08IJrN2UKxSRB2nfeKzUSvrPZdYmfQVdtAecWwTvF5VmHCY0wCtwHb2555MEQFzT5GnEOkxQAxHxZoEXHlIzaBqT/oGgQqZRERHlDmwi+myZxzX9PJaVdjRJ25nvV/KsCZ49rocPae/76MutaX0uPD8AK8Im5RhD/U2jQsBmjWKwapFEuGwfa7PR0/iOyMRZpqHE4nTHKCqxgmDJx2MzlWstCru7B6fwKbeZ7hKuJjmsoW7PnuZFsjl0WsW9AiJbST3BEX0pKGk+za9nUGzAH/uxI44ZxoGEbnCXHumtW0NVTAuHGOTdE2U0+hxoxFVBLgqvfZzh1ZHVPo950t25QxrJwrVpa8z3tYK3ml8jydOvlsj5rQbCCFpJufGMWWYPg+WCusZs70x3hRMpxGljQDKhLkowA4tQ1uMQqrn5h0V8MRCm/ZF1uBAeu+M20AXGlUTsjBiLUE4DCPHduoy5i3fZknkDh8TLHcwll0b4N6xBoRKq9K40ilczQrm86TXYh2cPy9CPtjYv57fvQy6HvN6A8cVijgnhNS00rDgMf6wUc+fAFGVmKc+JW5N4WMzKOBO9N29R2DNCZ7Cznmr9R6im6KqkqpyMfumeWisEdNT1qSKtoWxzUjVsEG6UEcf3ZlkQpdRBpICRUIrDvDINpBLdeeQ93dKSAUCbCzrdaEN8Uw41r0SukzGSTHzIr0rptO792LKj/VuqjiW6fmDeK/WaO0jjrfKu+zk7U4C7OLRmDTWzvQ+MDXQYyj+mnC9XOMUmkKd4ro44CjQo22tT9P91WRV0Elhmh6DFzqRW7zp9UKix4dGqAzzYIzffRIw9jQamsei/nR2sbbs/kOUZdH7YvD82NKoEFjzE1kI7NU0YPg84ZDGdwN/ne7NLeun8SN/vkkZSKSm349V52oF58lAnPuoE9gWGiUBPlI4n9vC3yhfK32v58mNZwiNRsMzA2Y/4Z2QpI9DjHcU2sOGBdYoZ9t6F8g4Nvua8LdR06EGegIsIAJlRsIpPNBZs6aNcelEWxh1GwhajKWnnZjKFY1NVJjVkzLcGOxj104VTWBIgLDLaU1WNqEQXaf1ZA5CsUzTVYljX84GZXxdubuWGU97GLC5OkDYFdypIq2G45Qn2GWQv+l67GuvJ7q1CcFUj/AODi2NUMJm9AMM6F3CEh1PqWvTJTwnulM7INPDmudFb0UEAkigDKrEsKQgnZptO9FtfUOxRvJHlzrmgrsiAoXU6vC670gSNSCgcDmO0DGaoDDAbjMOZFhWl0lcSZV/h8dyJiNi0C+8XStCKjS0WqpQYuHrweCrEUYYn4PoK4+xxv9aX+BT5xYNTofzaqDiGKsseqVc1ZJgGGfLNy6cxqL043zux38qNzYhiCVFnoFA2CfJ6gnJecNV9s49ir5HRDqNB0Or0xwaMt9EpY3TNAyggMoDGzijTJji2FiIxgO99HGUY4CBVhtzNib6CcojxTkOTaKAIT+j1/fzcC14xnDF1HgcVSLXhGN9/foK3waPskdxHquy7BIfQSlT2y6gq8fCfvIOqxzTscrWIuBRCHDPMybm8NqTYtqVBb2770EW07jGtfnc68r1JoTWxpPnmAyi2eIKk23LA5JMgIac4IJ+NzjLrY1Weh9Lu4/RmDGw6RPv5vtZBISN9ekFsHdWFUe/5ze8zrqdiejRLKTl+V6SPq2DqQ47oDYLqDELQHcmAheCsEr8w9Ldo13N2Ea4lH5TJn+cw2UtDYvMCDKTRtvjtQXoCdEL+QuUsMTM86hzSmxATaOvahfbD7yxcVUGYIzF+3fvmG8+EPE3m2UIenfZ4uRlV7pQubFOvv7Es+ty45axaGmxU3w/p1+zMkYFpLV45wYR20O9tGCbSwIkxQurJcyT5okbTdKE+C/ADvBa5v4IXJjMC2MOrdF86tz0+kOWhhIGflnVesJyLJdEIMB5pMtjWvckS4ia4FZUNr2jxqEjKk/2ZDX9Pj8RDWzGt3hl7STfjbS2k5eo8LrXLr/l2LKi+bSbydvLdbZNzmtWY1yVe6+d55l9TYKPZjx2FLNiWWH8GIK3PbEIH/Ga+KoIb4fZXxhTWUQiGhvIJwlDnvS3CygzkehNsXaDtjY0j5OL9grzyNocylsvv2jXhbxpmsgU0p8aqW4zGsBVaFUQ/dmuvhYodo17Ll8n9PFoO1FxXaH12rh3Gb2h06ftgLEhPO78qZd+8Fc2IpVnxrrnu0l2Mem+uKFApURen+i/6uPyXSlOPDyJQD0S+bshUUiU+cwEf+P37lmt6MVwmaLyAk7T0kpQZC24dGLCMM64nOhtQ/D2qjHsDGFy+xyVZip6dADKv3LoQI+OkebtX9F4xYn17/ZC3UVwo/AgP31vaWRwRaPP4WhTlU/bQ75Eo1spQqavUsA7ilEXHGvV5ikTW51fz3m9uv9qLryI1Dyuyq1Hp6NwDwPJJI3mC2JBvkVgE4DHbBio7wNtPx8ZoGr09DtTKCppXPgnygKHiu+GxnomjwtpbLZY2lwZLmlPIxNRe4x24hor1Enjz+eQF92c9FwHDXX3fNwVjflFnyDPTL6mSKBw+D2vC2Lx97y4ksWRI5KK4U9s9wpe5nK61ue4OhfXuKtw6ao/S5JujED24Pt12+tyI1s7H/YsKsSOqoQiHMlxk7+8dhwZt2gpjFph585bjtqedfG4Zn2MRYB37IS2uVdXjE73SmNbHc2n8TktZl6G5dH8YtR2G6wfyv5oJ0yc27ETvmwzgAkukWTIWZfKc+RlC6fZhS2fW4ZaIpe8Q0f5m+Lui3LyaKflDg93zn3dNQDa993Y1Okb14yQSii/juO15fY6p64RKg0IZ3JAPCsQ83zIFU3l6B7PRMZjXaHn03+7cLBag5HH6+KvtyMUmgQpjGe6sV2ti9PZHlPR7dk45IjXnkyAU66bEZioKy/GFbif2PfNRyVeJpqdv+QmgZqOx3irlJwFrVayO4/1uHeciKk1fBRLzAWgEdHra8W55/0rlLCDrXeK0xoNzK7PXG/vcYcMZOR3x5hW5XbMKWAF/QnvGMMgQYNQS0BQmzAQNxz7n8c0cUucDgqhSh7oeG25K1WGnhi6aZwcLLyRmyeb/LV8iSc584i8snGQ6prQXbEREd+qR5LusVxL5hfSUVn9nSfaxWhZuXMmNuYPVEnNlIT4yr/Do1eWNdZI2yk5cwoRMi9yHwi3K/7GnMc613hdpaAuy9GgvrZcZ2sNpk2m0hB6XBVRC2EWCTwTo2iasVLmg1LNzixkneuPNtW6UaGv1kkh6PBmk560GVozrdqnEFHTrBocJ6CDC7XCBW5A+nW2drj0UQcZABDIOeXb+QjOcqLirpfbWXg9jrteTuuB1XJV9jLjPHoW7Ev5sofK2FeVsDmV7M3y2LNyaS4BPT7SVLXv7biR8/zIlemVNP6Kz3Ub17BWJxCzcxShqCuqewwWps5iDDHBNAWdCjaxXwcm+2tv9PrJlKbWNjJsO4Gw3GEiBWREN7qugbqXXwt2KU5ivGb5NXc9pYdMicSJMzFg1e/4VIadMAZjBMp2goB31vHWdrJCwBZHMMzrtOS+KP2OCOA1nvG6RFozBH1tH85zLnlcx+SaMLtH97XnnDOOfQ/YphvdQUgoWnpjgVrQTr5UIbE9HZBBQv1njkW9FFY8Wtop3QvcxSDO1SBN0rVgx7gFtv9RZL1Y43uBCJae9uWkODa0jdeo6lSCtRvzKSGzqydhjolqj7un8WRgTu3ES1dlcIeRcQqVx+r+cZPNjjYmQ0TokJY+63L9OgZKcSAptIzNb72M2A1RI0ZRZEfroB0ScWCeQWXgG0KFRSABOQ5dlDxjShrh40JM6GATtSKGNt9wlD2TjVqwUYKJQ+irXnyNQ52/6rpR6WS5Ln/H6+56lEpBT79P8BdGoq3TToDvLP2EFm8aHO8XP7H/neG5r/jaNnPcHXdvfIjOKsMQy42YU0iUOep1hjucHpWh/eIG6cqlzDY8dnP83phDKzYxKuXEnlXFhA6SjN6srQI+h2Xt63kzDkzqhkyn3VhEoUUva8ehvUmx1XXFrC1zTtTsYsClz4RuxvEKWsZxZ4gaDHHxeYq1fAxU1sex7hS9EvKrY7lEnk2+C/5eapALdg07q9gU6d8ppDoRNQ5DjVTuz0bh8n5Of17OoIyJ/LauuenchICJcBucLp4zM+UlnxE/rrtmhPax1Tgo0zOCZ7LfYsqo96GpJ9IlneDRRG87yhliNULuxfKk7mCoaDX2uvYaP7AlV3DyCspVcWf2GogulJgoFIUR3JTKSFSeO9Kxr3/Hu5+QQR1zV20i0kBjVdGpsqAKXfEnGqBdEX31ITiumo59ub3OqY1qPyjM44s5m/l79RCR+HWyTomLUWce19s0O0HyV+YT69gU/7Xt5+Ma5+7GEeRQ64QLADiYsoJ31g8GOFv1QXEyr2OdiRw2QpAVqILqVZ06DFmzpZnO0zV3Ie4dZXb0wCXfNGQC+KdntJd5nd9yd6LhtfA8ytD1eC48pyuciD+gConzXRARbpzgh0FS8olzD4gPwUIkPb4ws22dGy80IjuuVKhDzLHTfpwb4brY4KG00aThcqo4fACIurL8+90yeIyRmJpi0lvJ/Loert9BTPy995j7chU/57aqMZ5iz1pxaog6axAZSqvoqOnK40G6TmEIet67Ov3Kd6W4ReodY4d5vqGynZWSDSquMGFlDAUFHPBQYWsmdj+O18NE7b86LuHDz6tBU2Mh5Hvped5Nlry6VlyPvqqMfmNsFMe1b7MSthwXnpTjDHP9N95JpLKz8+4Yy+W4935B5auNX5iv1N/JGKDBqejz+0+1Tva25zk++ga1PiJiMZsrKlGlCVWQr9lT94a+sZzZic7WVbBS6GSjaDoPjExdx6Njwr88Fjy3KqpQJM1d6Dg/NpjLxMXjiLfZdSIp+KzAv2XAF8cijIsJCjeWg3Z/SLMQPg5lZ1Tj2OP3K4/eezdeXYUR+L0yiFfINiZmmIhaQna6zhjn++SFd0mfnRLH65TPKP9MeeZ35TIhtBJlZ0ljJVQEpwswHBMVED60hx5J4a3VvwExmJksy0oubKjbOysdx+XX2pv1DtcachAFqLDmOa2PL5yMLXlxKdrRRxXf78MD9AgAkHkVIL3jVPnhQrLvYyewO9gW+bFHAWo8MK5Hw1TlCtxr07HUMBlpUOOE9LweocT2I+2R1vq+zioersrFOuf4e+hjPQyyTlirCmoyUuH+SRAcO8covrxiseh8Zi7GpcqQpfYiVOixi95KgUBqiPytWxnqaH9iKmmb1RUREI1kEPRhDxvTRaOBa49wLfBsGTZvvsdrPOaJQvpaqHjybtmYo3W96iLD6N25u8mhTFucv2o+r7cO7mQ3x65jylvpEO6y+tYtY9lTlJ5j/AoEZudXeYiqT2QSjlmEqD/RK+7jW6TB48BE10YoRYjYFA7gCnk986mGHkonX87EHNn8d7jnHL/vhDEY4jAP69hCn6WQ7KHdOozKI3vdXcw5zq8vFl7b9/P4+yRfFY117KfH1xmqw5ZzOV/D5Et0+37ulFtLKTixWaAjpMxexeg1nuRJuMq6hRIUDBMU2Wpxuga94X6CvJW5BxZqxC/uwZvMwarQBSEYHeO2inFUIPLIdK9ZP7XKwxNPZaazkKwxkZ+rlOikqOV8b86vZUUbrVzqQlSEIQn2PX5fJaZORWFmGRZ9wxLly44SpczsCfFV5fJuUPWUCDM9eQAdTckeRlIM7vrtXNNr2poD0xWU08JJKRCSBAiavGhMTKx7gfN1PNvTXxmccYIuYzuiW2ozUkzUicfOKq8dTYg9rp0IHgw0xyjhcx4dcyCcL1/GimWFmKfz+2N3zq/H883zDHOw7qpxeVIjJale3ed1qLQvezgez1f9jk8iN0AqDq10OlcoL5dXPEOo1nxnpgsbPMvZ6ondC0Z+EqALMkdEvK0E6cZPrzwYsjKjgl74nbNSWb05FLA5hS5431iPKD2ACyn3wY+kkYR2XfjOiY9BKhqniBJeM/l3LPlrBBbr6Nu+RPRtHg6jRBqN9dbQqv1Fg7tC9ZMi7pQAFfib8qjitSM5F2zeGNvd71255Tnjb5rELlemz3ExCl28t1MovKNSq3Qie+Vc1Se8T2Qcq2/7cc++o/e6yIw88S/0Yf8U/VfeDQ6NB/pVEHtcqS/fqSZWbz3aGZhsfEqvcPBC+j3QfhGXuYGNnYTwZkuAC7bKi8ecqNSVt4107TzglULvfmN+Yw0VqvgZ6yqMX1HenXIZc+pzZf1ZskTKRPdiQJcweDieCJd9z6uCO7vPMgfk8bIsRbWHvLL8UVCh8U1h+DhM6PTwIcsYW0D7Se4dYPu7EOF2+JIOIsItZWtMvfLohHa0aQaBv4orw7jhunEMtlmwK9SQkYrXAtehYiLX7mWvryD2KVm14x1+v0OD07x3XtFJ3NPS8zpng+1NrC/7Uegn4Ao8W0msXmNqr25IF4i/0qKn3ucpiBPJ292VHePc0s3+gBz9TlQL4LeJW4wuHl6Xbejin0rLVEd0sGjsskCNR1lGgauu02O5VFb/NAY8rhtQsG+vj/Vyv9MQgcWNYyKKht6znPm18CrUw6uy8eOkQDnbfVK0avwnHsc6el29J9diJSKT/THms6yddwh1JpJhFRs86AphpQvW6itgGEAcdgDWtuvvM3zS67XPHVNPzK7bvI5FVsi0YrZFPNEWgdNmOF/RuBOG6i9fc1XuxpFXtIR5HUfnOWshGMhdsi5yTb2qkG7XiDFcSkQSLbzYzWXFtyt+veZaN0SoCwIBnY/zzjzd2Pg+OjMr35H51d5WE7tFUtG7jkvELmVCSHBtDWsj8P9/MWOFcGYyi9XhcJyu8OR8ZQjEWUvR98GIP56Tec7JRtF2nrJaMqmWUnaxp8pDFEIOCmSvJGQ1lGv4UdGxV959uYsAqnp3k4gVWkAkpuO2SEdwbVbtMRquvHJQ03ypnPoEOrtBNF5B+cjOXfskwX2V8HwfHIq2UcVBlbDUE3Kl3HXZTVCmJVxD5hDn+b1nJKqnwumKN1lbVJCiV21nF1NtE0FLf3U5x3M+Lg0R8NyoH/u5iuUqPq/HnMt3Ys3duerYKW5f67gixm7izfmrIxG7YWRHI5br7Xvz+bDDa4o9BS8TWtfPakfE3ALuHkTOs9OLWD19XXiR1UVtqK3gNU1XGcg7xb3gtIboJq1d3+RAzPMJg5hEUre5PhlCr0CMooswpzHeheinrO5OQDH+HW1FGeAQV3lbzo+VFuxzd24eoZ2nObWrbWcjv/u+izf9oei1vOF413nY32NblXPMORVmypd9OjHiMG5OiKeNYxqdmUi3ufUupE+ga3PTgiskguWpnML+DFofpw53PsgZJ3gdy2sTPfnyMobBT0MMSGSCNzzeSOb6nD2bx2imNJbx1n6ENEOM5fR7F6OOTHxUmkpZK2802kKYFu/uwVhT298ZwHwbWe43Fjd2u7I7d8cAXyMK94IOY50u7MLaYq+b+XIqR+VsGspWO/entNgOIFBaZqHncweniPDtlgu/FqWAYwJ/+pvVI6MgnEZVlyqL6J1U14++GQ4IjTDAoI15y0Y6f6jQwaYynokQenhdVBKittl4cfJCes1VqWLW07V1okWh373J2KGYKvGz+53r77z1KSGW68bXVtrVME7fL84T+eHsWl8Kg4luqubVOmen+QyheJN1a0z6QIQqqJWgqOsEs92Mv1p3TTgtOtvIvKcItGFSH7fHSaD7DjsiTMPv1Vod8Hz8Zj/BUzJF8HFeDE9wd61mMmfolGfYK0KdcdqvvQbC1h28qxRwB/eqcg1HfcO539lzhrYG5w8xX9XGHSWtIGyl8Kf8ghtPdCwakuR96CDXiqzU094Qycubrdskttt7TnxLVu8CnUXMGQcMLk+h0BJ37hIYE9LCFrCMHg3/Sxb0umSrPvotORBoXNrZfNc26+vGkxFs3ZedHoOI0PvCjwU6Z4onD8F4MtUepqR1o0BVJjUr9XUC6r6SV5D6jrGIc7p6TJSzfUxd9xUTPBiyqAFyoxT7TZ94rhzJKBebEAYca61WnBAfFb0wCEhMkhDhC2VXyzXaXYtqMkyYvUjIPfWkqqAHNydsxmwTU3gCUQjt1BS9hBhkeMyhUA6XOzHxfDrCUFjzuLPl4SnFXozExjIXvlw6ImMab702ZNHjuUoJUSCvEjMnb7smU1ZlyAmTKllTQ8q65HFU9Ffe8gr6n72ozil6UV9WyqhSUZJOy6rGsVxka/u03tCKIrL5j8kbFvMCc2Jo7C4awj0nRzfHm1VBxVNsoMycp8z4JmPA4WNRoquCUET7KxXY0egkQ2kdR3i+m3M0xEaqb88TIno45JsNDs+mtKTOZ7WxIQuWs3SmVzY4oWqsVCHoLAwuaCcP5QjK6115ZSZ8HEpVL2dJ8/FdfOnXNDB+tTe+Qy+eP3nXCL93LTXyzLUr0BWy03L2nMRhYokI9w1YPBWIhtoqVZpx7H3+LphTpZ7zoPVeRvO6QBCzLtR73yoYCGFrxctiu26mH/CQqJGQ6L+Cbfk2NJVHvStUqE8jhpaUiajPx+uu/GizbgerO2qQxdPamc3JtEhg66el1rlYujHPlu34LkFS4YVKgda59K15+dwuGXRSinUMBPyXdPyuOqxjqkp8CZRPa966xynUy5y78h3Xr2MAaY6WJAq0WnQVJVFzr20RmZURIeKH1ttYYPW+fS9YmdbXnsOJTDXCxySHiCYgBZhuEBdk3MRYdBw8vVhLhiHysEpQIBli6kkuiTYW9mvt8MZXTgTDYRD9CPdrCOd9n+LNUdRzxnqhh+Ttdu1FOeykz0giS8rsd0hdec1djItGoY5Tj80ORVWERWcEo+WonH3CBMs5NqYn4YI6EYluKcPVSSJX1dUijwRTHxZIX240S8DpIfZLii7jWs/Kaq+jP02E5IJ83cmSxTxOdbxgGqKhqA4dHeLIVNKplGknlG6AZ0UCVE34pL5Pxe9jgtn4yWGGh3IgD5RugxhhJCtv5mvk1diApWkm/LG/KjOcS4y7MN7H2HwtO3iLZfAb4fKaT1g9+PUGjd11VdytjirZylRv7orLiIPEHzZelOvXMfTuliMQRNMbgFLOh0G79U5KOSccYxuZsVnvQv5gAU/clJ5VvW/jlSZyeHVKYhiDtpOuRomXKCHCkwyxCLynRdipA22ovhcVVchh+zQ69AyTXL7aHG4oCE2CW7XYNdEuRvxU7GTMGOZoHJuGbEK9XRy5jlOOAn2n3PWGes0deHvK4Oa+/boV6u/acj2gsg6WW7C2cueV26+ynNhWwFxTedtcv1SifYMwz3tAJQpDD41OBxvpqr6X9EBZLS9DTF3fbanjwBv9RcgeIo2+HsfgqHlYTo0718kenrLxMBCDxjchziU7vnoOj4Wg73ROGEZZTJ8YLXswdsp8xlg1nAnHKwP7GuXL9U4w9I7y4XWnHMkVfegohsg7kmSuEIyXGzdb6wDXSRgd42DUwp+YOSd5KmrQV2Dm6HeRFGuDHU96y9/ADFdVMLGwEy0jW0kQOIbnSRYB3al8JeQC18qT6PkU+vDhSe8/POnDhxd6/9V7ej5fSIRsV5Z6fVLoD1aeRdeshfS/3ju9efOWfvjDz+jx9i19/Mn4e/uujXkAD3cKBbLAXgv8Xrh3SyMnWFr1Xxnuygic2r1Wwj00D3M6YfwVL7VcKqdbXxApHj5OXwEffIRCHwCDMf6rKFrdfEW4mJTowsR5dHW8MfpZEwUWrQ76ZaVYRxqUS+wbwS3J898xbn++NoeBNROe2arFzTZa21UkXejrrz7Qb3/7R/r1r39Lv/nNv9CvfvUr+tu//d/oH3/9D0S9kXSZT1hXJV15wFMQex+Avb0d2wC/+OIL+pu/+Rv67//6r+jf/+V/R3/+8x/TT37yQ/r00zdgwJQD69vZKt5brSIps2Zl90b9m2Zdr7K9r2vvHiy+okHzEJIXnoty610p3gYsTxCNO1QoP8JklG4KOYSATS7nXS7gbfK8rPheY520cA1xT7RS0dN7OwAXg5dy4cAqpnDB/kDMMI9n9kpUL7uUgUkD5o8/tfJda7DvNnnoKIXo8XjQ27cf09u3n1Jr7+nZH/QUok8//ZR6b9Q/vKQ0v2YvG2xmEGrtQUQPImHq/CSyY0xv376jN4+3xPSG5KnbNYU007wK9oqmdp5m5yF9bpfTcM1qCM4w2utW5zEe3rWBaMBj67EVcV5p18f4mkNbWl/nQJOY38pzorDDUcrPzOXhKsluJWJKAYyoHBJC2ttF0Euuk8/My5vJ1rGMum4kimvCD4jfxD+csRJgre+hxcY51Pe4NO5W8asdFfjv+X0+y4lY6O27BxF3ao3p0d5QFybmBz0eGhLMZ8pzo3HbnVAXojePRo/G82n9D3q8aWMdlYUej8ekrc/HknTYxH8SpDjPRLtQYYWpqHRoYHfx552EXqXAW34fwqAFkhIq89rGqX1XdjfSPRm3qtx+4vuwyNppbc0Y/knhoEOs+MZAb/AEUcOr4Xe0alO7HR3O2Duxg0y6WPC3mJONW+6i0saQ4gAAIABJREFUh5zuLynq2YobPyfs0d1DPrhORE8Sek/P/id6vHmhH37+jj7//DP6wx9+Ty8vH+jrr97T8+U5Pexb+uijj+b3NxbHN2JqrdHLy3jJ0MvLy1BGFvrw8p6GFyV68+ZBrdGcY5eDinbykUM8FcOJPO6slNhmVa7O3c3E5u+7+G9nEHaeF3WhSkB5aBc3iRyGdc9zhg6CxQskUO9axwXL7kbgAQmQGb5Cul+L2sGPHRyJFtf7wvFcCwN6vBlfE9kzzTjcYUG0PKhXaRxOqaStor0amx3nTh99/KCfvfuMPv/8Y/p3f/ET+h/+41/Sf/pP/yP97ne/p9/85rf0//zD7+gf/+Ef6fl8oV/+8hf0i1/8gr748ef005/+iN6+fdCjzf22xPSnP73Qv/zrn+h3//ov9M+/+R39+te/ppcPL9RlrD8/+5Neni/UO9u8nQQJaVfZOBnMk5e8ajvz6HTdqe4qYysNu7aQXjQwuz41ITQScTSZeYa2NxJCw+Lmxyu4FyJS2JpqDrGeu685WVKNh7JEr4kCZ5auueKxc7kfxMeHAtPYBaTMBQ+W7/CQOX57Eru2QT60Kh7DMd4RSJEHkRA1fkOffPyGPvl4CNNnn31Gn//Zj+gHP/gRMf0DffnHr+j9n76iL774Cf385z+nH//0C/rii8/oo3dMj+Z25P3zhT778gO9+6fP6IUe9Ps/fEn9q6/oSY1eRLcNviW2/aG18O2OKRy+U3axY1aokyfLSrLC0kyfy54jwUjzaxJHe8Q29ESUhUmmMkzGct4hhBsQCmsYiGuasm82SIVoPoA8eWRJpdz2sNTRsty5S+EuxBml8L6Ekwaul/3sqSmc4JxYCG0CvXdKGW8R0aM1avygLuMpC4/Hgx5vHvT88KQPH76iLu+JuBM/HqQJYSGhxm+IWOgpQi/S6euXD/T+5QN1YXp56fQUGTmjGzeG+NzxTCzVqOa1pUq0aF87pHG3TVfGDM1f1+4pFvbjo53WeDzRkhBG7/lyZP3j0RYFWeHNFOQZy4Rz86nueg2m+VGIM3QdzIsDrUo133ehjwWP84/nYrxtT2a2keV4imhsbew89kk0GnFZ3vicrb3+rscbY68qpKjKeKRsI+aH3dpHRPT+6/f0/v2Tnh86vbx0eunjNrQXJno2pk5P+vD+j/Th6/ckL0/iLvTy9Xt6ef+e3n/9NT3ff6D+4YW4n/ivsBRpHSjkCnZencc+8FNXBjIdSA8eq/rz6/JtXbWiYIyc28Y+ckxbxbgjjp/ydhEq3FpKiRYLIIFiN/HdPWZFpyUVmW+VIiLpbEmGkY12YczxCbr7HSzZze89ZQZXvTRAsHNmDJqDa4/NcGp8nSi39ndiZzwe63gooYdGWDyyskxMLy8v9J//89/Tf6H/gz793z+mz//sc/rFX/w5/eKXP6Of/uwn9Pmf/ZB+/NMf0g9/8Cn95Isf0Gc/eEd/8YtP6T/+h5/bZoTPfvApffzxW/r43Rtinl6U9kK5MsV/7zxLDGnuwPrIj5OC7GFsRcPo/2THx7lzprcKXdY5VLgYIfWuXMDaOgWdZMX67E/t2wNsbjNtLL41XiZMzLCvmgQceBUXvK6AMEhsBe+4JMbsGtnCPRPbZzMyIi1VzLObeB+jBGjoYx5rVv49joWFifu4gY2pU6NGD2701Vd/pD99+SXxb4jo/xT6v/7v/0p//Td/Tc+X0d4PPv2Y/uwHTG/fPejduwd99oOPiOTzMScPpvZwA/l8ClGfm+JtL3AUvprPa8x4BW2rfENVF2WyUsjrkunex33VGNA4ZlozjbmfGGLu+yV6RUJodJgJIBNpEYZXKqBpZ3U+ZN4KFFMvrZicvQ+lePBulg9Gk35NGixr7Jfl6FJ/a9wrIjCuteyEck+381l/17IfadUnDzLP12UQ0ds3b+j92wexjPXLEZc2avyW3vIn9KB31DtPiCihaekjdh3zMizvFW9RYZ32fT30WKui7K6Nnuh1iUEsOGZOx1dvWCE0ldd4zYqI7tD2jRNClbcck9Ydmk7PObZgOo4Wq5+JjgL9ilheqZr1anh4d7LQ6ZuSGrn7uObcz2r583X1JBINT4PLU1lw1qIhhnQhkrEDpbVxI/jz+aQ3jwcRM71/edJLF3o+O3348J56F+pPIXnMjQrkO7rUeQ+hu1acNQSpaa7YlZ9/mxXxPJcOC72JlWcrPVIqU7ymomPvWbFUSayqmEk+sPhGzKmMH58D6pLDQhnCba8VIGS2Oso4uF0WU8exWqbYnn43Cl+t4EpIHOZTDUcRWt7z0gp59nAuCnRu6+Qt87XucVVJW2N6PB70ySefUGtMP/vZj+nzzz+nL376Y/rFL39Jf/6zn9IXP/ohffzxG5Iu9NVXnf7wr3+kL7/8I/3+9/9Cz+cLvfv4Hf3ZF39Gn376CX366Uf00Udv6HHQkSzoMT7Gua4aqRGZgS45KZL2wfCZacM2VxorGUMUF+m/h4BOxtuohOGczM/lDiHbFqfKyGyecygmzeSJL0Jo7KRJELJjY4fQ2D62btvKc3CK2V6bYJmk+/fZiD4GZGE+rYw+K1bs/67nTBRS9DirMoZYh+Z6JA8PyA8mfjD91V//Nf3kJz+iv/zLf0+/+MXP6EdffE6ffPqW3r1j29/58oHoj3/8QL/73Xv6r//1H+lXv/oV/fa3v6Uf/fgL+pv/8Nf0F3/x5/SmvaWP3jLRo6Y2Cuc9I4l8VkOPsfapDxy5ow3kTTYIKww+Z4kJrj+P56SYr0Jxh2suYa1CFvN9QkQ8b6yldMtYiF/YjkUX7okWRUPV+uUuG3YW7quSPDuBd09GSA0RllNcUVF1zhCeaCTC1+EpRToXIkytjTVKISJpQvxoRE2mgs49s/Sg3onevGn09k2bmfJBrAjR89moPd7S4+07evvRJ/T2o6/pzduP6e3bT+jN4x29aY95A3wNsXOy0BWmhvfoEdfx0lLnVOolEFSOemvcoINJE2zrfKTMKtRb24njPBlxdWarpHzDmFOJZYbdL+yToNlLU7fIo7WlqQ3MFG+afkU5CXdlMVdBmNfMfbIsmtJC339dhMhia/1NFGHxtyvo0Z35GrOLEEkXQy7EnZjJNrATjUfAjHs8myMaoblhQejRmB4Pprdv31DnTk950svzA708nzNZdMPyE1HO4toIQvzlx7xezajdHLsCIJxd693xjjsYjGjvquySl3eOGV8O7V++ZcxuB6s6mRLKTc69ZD4G3u7Xrc7xBtAxG31txqwzUxMaaZD5eZqWIGwMCioyn34wlb9o5LUJK22k6e11kKV9+dDp5cOTvv7wpH/9w9f0hy+/pn/+59/SP/3Tb+jLL7+kL//wB6LO9A/v3tH791/T8+UDffzxG6JJq0inD+87ff210G9//zv6p9/+hv7597+hrz58TZ88P6XnsxPTg/qT6flkahtYC6OjuP2xjs2quaz4UmXu41w68qrCBpSJM7/V2K1o4KrkeHUXkp3qiUh+j3Qo168AJJ3QBHMZlJYd0vq9GKIn4DeZ5db2PVt5jlv23vBeqdeiELIL/Ot1dpYerzEYxHUssvt9QbH1wszU506dr756od/+5l/oH//hN/TrX/+W/u7v/57+7u/+C331pz/Ss3+gD+/fU38+590ng6GtPUzBtdVxO9kbIn5P8qbRexb6+b/7Jf3ki58TSaOX/qQndcJXyg9axifutc7wu7bUZwXA5M9rQhiXi9H/kccCGel4+BT6lTTG/msDsShi2eq+48tNCNIlxF/6cGhTOkYhVgvZiecT20Rg6qYXGAqvI2FbghGhZXJ2wXZmhjIhDPuC43kts5NvRLiODBUE34N/GJedDFCGh2Nsg5fMQ7TaoxG1N/TybPTV12NL5OPRqAvTx+/eUmsfzS2F4wFtvXd6MFObz9FrSrkQPfgTemlE7982evt4UJvLL12e1B5iG/t1DIN+rY+gfhVQH/f4U2Nc8QZjtMSV8thQKM1V4M3PsvRjciRExALzFsOFqr8r9BXHEZ3YLtscDP7hsT5H5WzC463K2LiQPdZydDT+8diIfRI7EiKm1HZ/plos3jNiH5fo+XsQolYKMChOBniXja1T5qdj+ZqapvIwXgETKjSfAUqaUOl93prXmKg9SegDdflAjzdMzG+ov7wfSZze6cFtxNQynqjw4G6wRWSsexIzvfnoHXXq9EZhlhA1ekPUH+PRnCC46oFrBLMznDXMxWvOCoAowj0dxuJIX5WQMh4CHX7NztvHkhM+C5XCpGu3pSefdFvNC52/dbN19l6umG5BGbTMM1Op92mFbXCsiaTXwD2lLU5qVj78nT07ji8DMyKwIdu+ayHUOElE/JEjSgzRFPzay6vFJ7tkeoEUCTdN+MhY5vr888/p408+oT/96cuRseU39PbxZgriUEx96mxnnq91JHoI0btp2DoTffaDT+hnf/4L+uTTB4n8kZ7PP9LLhwdR/5TsmbZh2AjZ8PfOoO45ej+DXa8DZyOM/QT0pdtRkzPAmLMyJvhZozTfjaOxt74DKNBrInj19Kt5+QnX/y9/+yI5MF8YyTRS+UqIpJNWBIibZ5taG8oVjclqhVDRKjr851rnVJg0l0mUDejrTUb0qsJApxoCidfOk0f7hOhAhMbT9756oa+/fk9fffUVffXVn0hI6D0RNRZ6w1MFJ6ppM6PzeDTi1ugdN3oI0WMKUiei56MRPxq9e/eW3rx50JtHo3dvm+04IuLJZIWfMZ5ax4SCHgV7xzOsH9uMSMmdgKIAh7OnMhL0/pymXak3TOzqOC9ivRGvhyGrCoiYbIgI/c9/08oOz0sprREpVG9zux48/FtkQtVpOCywNpmbF7JuGtdR4oDrCUNrnBVTv0dBAA6Q03KpoEzUIdMYrPT0WYe7OEPpJPBaYLLJcCLnh7nr0UdLwlt5VWMlE71796C3bx/0g8/eEdEPxvVznuy2Me1rNAxEkT1ZEKepG7FgMIhIxqPmJy+7zUteJmEGwaC4Vlgp724ua4gbvbMqhBstxzpbHhoLakNxhunqFSs47tchnFW+hL4SnkVDU5WLVwDG25WEhRq3sYwwma/7BwajMvE0HxKVFXMPR4/0JAaelmD0/KkNq2sMHjGJMtnhOY6pzuBmWC/i7zbVc/5UPgrxz47WTDN6T6fbP90geF8Iq71BfMAUJfmo95JiVhSrisAuspBY2cVlN+dkU6r+bxV1MgoCiLfys0z6EY5HD1/RGFpOhvLU/u2Yk8kfQmzNZWsDAt3t1gbvO1pdIrR2KIDZ0sbY8h7k2FnG/WQ6o1uLTF/a0auZJnRRLxuFOhgSInh+rUOs4KwOJYYWo1L0Jt5THCKHc1dth5asofh6vTxXHkZEGnWedwp5p6zo6Ho8SnvlPe+ocvbiJ0WrhrHziO5V9/KF5fZdKcTjLkZ9WS26Zg+dfCA2KL2NzCAIeCOOgqQW+G7mE+OQZU3ptRZ2xlHmfVanSfOUfYqoknJI9mC9AP8mvU2NlI5ta8ELEkNOeR1b5P/eG6EAZuMXeYWZbjDWpbEES0xK6z3F3M35PrO7b9vh9oqQEPKfSkQEdcEnT1R8W9skTQMQOqVt+9dkzo67BOXxzC10nghzN49KPpVpmjGZMIxkKOp4lIkLzcnbiazwAeOg85j0Wv1TmyZz+YHKSXTVEPtXRaVKnAUUQFTEr/fHpte7F7vvOc6owc/HR8lEGI3ZaBFZaMyJG+VvJYC5r0zHed517JOjGjYkOqt2y3je0Aa2w/MRMHvDHiE8Ud6wkYuFSqH3fbnYWzsHwxo/ET0ebAmioVwM1wrppgIVIocFsLF8QiCZVrkrPJA+YtowIHdhu8RB9pJ34th4XiZEz6zVZQz1qgxHZ3zK6xasrecvhXWFRsqvMQ54yYONPW5MrpIZCMfy9VWp+Kp5BeVn9BJrG/FYzB7ehbF3CyIsp1OTROv1dRaYKd9gsBt/TUNlxGpUoMYs8INVV+oOLhJC1paSA/krhl9YKW5TGvKoShSzjnp9w8FNXdClgAFDVgbVmdpzWeNZImOMCvJMcgVIzOyYFNQVao9MaREv3yn5sZxwpmzrCqbuzt+J/ep+YhZynCuHEmBnjkNP/Z5KVqyruro85zZx5Yn/jpN9xee6jR0duV0xh496tCsXnhMEl4vj9jj0deAxfQxEtgxj7NtQ+A7VNnHVVaniVsVgBcCahAmxDG+o70fRpA2p59cxQE27Rscyf78mA+nFvTTCKwJzcM4WZ0Fu23pLzwUM1ON2g/1CKxHR2PS+zg/Sf6/P6vgu41tDZayDs3QuvgzkvPJyjvyq5KV+L3o6sWMpZ+Wc0rdm5/p4dwdleDNMgm0vI1HpVeoLctmEv1t/OrgpaBslPQlbZtIgJ/r6bovXPGG2j1shjwJZpAKoKUGJKvQpqRHp91Z2Aq1w7cqL6DztoWcU7AxZd2WBZIGT943Qie7qmp3Hz3UwlPG2Meyp54DCyrQraL52FzfnsZzovZlusHKdEIIM7NgI7zGHdJqKSqaERteEs9J1S9NQQv2NOF1yfx3jJRjbt4hbnkL01DiR8uv6qnbHUxIEFpHG1b5l2v4w6JxNdbrr7XP/+rsDNOs0PNS1kK6/2f4CVBf39ieFqWLZagzR0PRQ/zWo507BpNRKawiqjvFncWY5cofnOx5dO4/aGGi5WOec1no2wayPGJkwgIl0MVeVOFiwRvZDky2mlLYdseuIZlLIobSNGWKHu2XLmGn13SPu2w3yFs/YvznyFmj9NfRlGlra0XUHktZKNikKCaKIhvC1gVW73twOhuIY3NjkZY0qi3o1vtOcoxLU3k358po7+1Xa4caOA01b3k9WHBWUdhwd5d77OacSEhG1BxKixFIQYJsUQeLQFWpmtMiEoYJCvbNwRg87uqmEDa9zSFsVh0hiVxN4T/yFnaN45nAgF90cj/BlTXTdE6wcY0KL0PakvvA4Rzh20e+V4bi65lQqus4Km+kjch5gvUaKSLKRxiSNHGTkOKaL4QYl3pTLm62JwCKKeCxpGVhzRpAI6YtFsTbhWT3jAmTEHFORaDpN8F2ZioqJ44z9xEp538+g1NX0WnnuxHM501sZm4qntUfKsDbW/66WNVQu9sO78g33SuWddrByHwcOeuLYK2NGpOopE+7Vy1R3If+hXMjtJawlovFcVBZ6BPgzoWzILKrSVpZnaLA4ioXzNOJSscsiHXNXElqxHFd9R/K2Tnz61P5cQWf/RVsnpRzKYmIQ4F+kJ9IVBQCZlbOj6Q3HZdvrbpq1lNDGklOxWs5WrvB2ab0wjNX1J0h8anO0q9+ay2nAOKkehlRFuRv7f1sjeHOHUCPWgU3CTQ7mMX0xi4nspAtfumsH4Tr2QzChOqmjzfbYCxlbrHp7zGGidxYwCA38WRAOcZjec7mqw0UROkrBoCFOkdMavWC1lFG3t++rVgDs73p0ma+n7Ppr4szdHL1WATwuTrFYvMri1GAKkze+ouHbJsJuKad3RkQ01iL10f+28VtoQMCpLExk74I0BSL9jVZqTjoTIcOSntqrAit4K30v49+FRzXoHjp18sxQgaHxyxj+iGQR8vG7mmRUROW9bxHzdqLHdP6613V+536yodJjdYkCnfvNbeSlkCthvaLjrrCfssSv8b7qLPSRNGPO6/rfxFDQbHNXXvEkBGXwmGx/wJPGo2SSiv3hg6B80zwWrdsJk0GZr0HgUwLjxJOlnW+Q+FguN3AAjwUNmHxCfZrKCF6WBBo4dIJxTU4Sxcyr9z3Cg9AKKWQ+xW2VUMVklh4jUDRtf60Xh1VDvRPMXePFa6XOY6kMwhoSuAFzfsINCRRM3jhPkTcVLecSkc+pnJ8h1PCWn/HX5pOJK8VAHnKbsepUZgIhEt2CQ35Dsw7M+0tWiqNC5riQaZ3I0stexAtZMIZwwziNtskjHUWIZYJ/CR8huTv5AtsfysldBc/HNg4pnX7eFUkKYRqf+a6Luu9onM/xY6RtB0+zR63K1dIL8qWqV9EaeaByVI8Zx+u/vNzxkirh+MsN6JTawwO+jrB2MBq7auMRjX0qDvuO/t4TlBAXEkVgDK8990GKHdPs2BreeB9x6DoJRLusmpbe7y9J7GCReZCQDJLJmZUu858iroACxkhsBEv/mf5MIypqLl5NlusdCrPNWR5jbhOP5YwyGkttbwehkf6V5vpxOFfLMGv+ISpmRcvg8ZinJS6WKHoiMNeLXGrRJZmFOpAE3Eiizzk+otr7L8+trdwwA5mBHutArDnwAHhNaMM+VSgVG5trmZgf4LNmfps/clH7q0i988r6qiB8nNMZaEW7F23yOkLC88ivBDV3sK5W2modkIrr9BhScW39gWKgp27/ypu8Jv4MvR+ufY0BwDoVTeekIj4WpT7vzsKhP8qE0DTiQqQ7z07l9lvGVng4YSsUhIF+MDW1iLCQdNPmeai4hoa1H8mN2U7SN8/E7cvdGKYsaGEpKeY84M9hA29IBCmh+XtWtgnaxEz3SqZ3ZybycPydoBhXxnbiBManHfh9kFeZ4Pz9lJGt5ghjcGwnxsb7NivDVkPe7M84fit9VNYNlQz2136gRDDHx8Rsyo23jEWhR8+0esuCOV09X2bKPC6Ku8c1CqG4aV+gEHZHi94IO4ULuq1gEAb9d4T+NNk4knDbnGi9OaZxwVY9BP4dNK63m51gelWcX65waxUUvhaEdxXc1SCfflftVOdXus9jrUOM8bv3fmlwI5zdX7cosMo5/GvXxh6K+tNRmakeMr8+I3fPq8sdQno7kHqsMUBX0jJJ0chv/ZpWo9ukuYI5QpxwFH2RIlvdeK8Id16nDDBei3vWdbzXi+C7yc/HhHwZabTsCiqc/aXTHUV9xiKYyT70fzcDKMiMG/UQftVCAnNF+ZprIY991dCzMkq7c1V5zfLFN8nUv7a4JyeK/IrhQJVQzeXWxvfx3V11NfkxSdFJbM1zvcYV2+gO+NuUWMBLa6+I581bD7oaPCVilxXcTSZm8XD+/PJsOSt7iuMBdB6MpTisLftfj18liLTdfP3rSh0S1DG8BLnIceiVF9Pzmge4N766/StPfT3f36KkJsqcgZAlTfGaO+V2Qmh8DoqyIA+vyqAwPP4HL2c+JdyYOx8tQcpk3bUwIACDMXCkwNq80yZkexlExK4huoax2UKjd28EXtv6EvKAMSGByRNsG/lnY7YkV6UM97wktpfr3lFipNFRyG7t7o6ycTCIp3IS0Kx4V9efPHJuD9vKc1N6e4WrhulUEHYjW+kx+SrqIO+qcqmcUXi5nCgcV4BIpiNRgIPVbXq0JbmB/bq0iqQqr/c98bysArrLeFaZQ4cksVMRjR6Uno1wwfeWRo9jQ+OyS1ZUiRQNLaIxHG0utNyIWXeZ3ysFix6zVuwdYrmjmFpHFeluouxuTgFvk9vWMTmNv28VQ3w5FMgPSdvTe2uHUPyNAjP+OrwcZ2T/5mTZG8SAsQiVqRPJfNy/6DHYcaMwocqsJLhld7tQkFjrF+GZCnhoLrTnFnPhif7DqHg5LR73DU1HiaR7uF3x2G1MGGQ2LG68sgDEcazH1/ZW6tZrawVxgava3vfzOkW7i4B2v7F8E0hbedrK4Nh5CHzCOHIq6UDLvfs5U+f6ljG03BnOeMZr4k0epCEtrYFQazsD0VodQ5KoMJJuZ7ZdFmL9rGbO10OreUbFrdqNAX6EN4MP87o55pqCfQnCN5G5kNBj4SmGBGhoqtb9+L147kzpPmnl/VRKhGvMr0l03fWCuWCSctfFVcIp05bjY/y+G8cO890tt5UzK170RLXFChDXzmchq9qY9SZ3s103xdXrU7+Zjh3EvYZKSRntexyTw3QaCj3j4B2orbvxuITtb6ckXnGnmKtVr2AlzpOPZWXHGdftIPh3Ue4mmO4mWk5x7K4NHY8u21TXf/MxfweeMwfSIrXFzkJhj9so+cYTFnevq95ANziUkXS0iKfHCyodSF9m5CrIuCB9v91xjEhhLZylEirid0AgIjQebF12X0HV1UPGpNtuHHuFrSh1w+p1Kpj3GoE9edKrLGyOR71NMlSxa28Xj9/pE79/G0N0ZUtuKyeRP9dGY8yrTNmArWRJJH30JApVfNjXgJytsUPKEoPOu11aTUOEoCt9V9fs6ox6tbddEhekoWkzGB6f+uvo3doeDRW01hsUrseQPeNOmNYN4bENfbO2jgrfzwqt3PRglVDv4rc731d6kT+x3xNNazvhirK/O956V670+v67UihiePWKqlAKdb0OeNqpY/OGFmhvwj8LSaKHWeCiVWR/XApVkHYdwzXz7xVUbvQkJysqRNR5jJdJbJQ8aUW1qHYAR8Ukkvl0NJ+P/CSIHFrsQgjMFu6MnCee9M3N6KlPtFalNugbZGHWS6ZDUO926pdpPLnwu/Fsi7zSDtF8415ox8tXJYQ8fT8G3vsq8NlihafIsRIDPzMsJmf0mh2uNxTwjPGGEbg3Ibvs4/3J5CS85/q2e0lAMfUcqb1PPTBAY0i+rJD7FJPR0jJ6UqQ56xXGtoGm9GyneG5FFLGNVWaCEd54ced1HWPG3zX0xj5rWvTcMECt5T7Slct4g9ndtO39Xhmyi1s14qQQUfmiG/WaWjQejUwnkuIWJeJ5d4ko8EtWnjR+2McEMgWehOZTEb6Dhy/dqIuZ3wjN97edkSlL4alSn0KDLcJE0nhKxwq3Xj80FGKeAq9jonTOf6txDjQXMbz1AkbW23fl1vhej48+mtGk7WOeg/nMY0R236S4t963ofzy/r2u83LllSq9Fx1vXW484KsO+jPxg3HRIrinhTiV4hKM30Q9jlVvMlssKhdeRo8zhwm081zf4HvlaXeZ3b2HWIs370gA7Wvaf+LtASIgItv4pHBYR14JwVlha+NQeZ4qtNG52PEgejZXarzclW+0p4PTYWu3I1fYkiz4OGLbkRcmyNrSAAAgAElEQVQ5aZTHl50HeuchQ3MHmxkF5E38RJnOY0V65xWFMVzLDVirlnWI0S4JNGLQFoRdH1GiyrYSqpMyhUmA0VPbBj9Sv6KOVkfIvmMvvfNzGc3GuFQlGp9aSas67uV9onSyx/EoaC0pLDRmfEEJEqqzlHdLhOP1fF61vRv/uqSCO71yKKEeMlpbN/RROWI959a4JkPViv5GRE8KRnLZgjnNpeB5pz8rGCbnLHSxx6W2RRarMGNXbt2V4sTp8VXA9x41Wq24IB0JztbHzxHUWUTYyMuPhDiVlUatd6/EpArSinxYldmvEaBSiGTj3U2wkfYd1L4PcU9eMmeeseyMc/U910P4PI5hG/E4M75ACQ2uVmqh3urhXFZiDNvmMSJVflTKle48v6645vXJFbNSxogYKu7U5dZza9fO42RUuyeiBa0Fytqbbm88Td7sofZA/nzcca3Vq7xxnkSwuBEqZ084+trRXQlrJYjVBK3wXwnznU47j2wohHxL4M6rjeM4vgjfkIbA30Pmdbdcgt+zYY7XxHHfEdQ8F9lgj2Po+YVEsrLGtlYl1bqRzopnFbvjMfXm+nuNI6sxXJVXrXOiR9pZSxR6jTVFMGsbYQo7f6w+EyrDFGJQrD5fpmRepXxIktNae997XuA1KfmK8bUnAQOgZmcKro6k9ou59qn/qDCrcNSJiCqvcDJKMb9Qe1yE968pESXtvhONx7NarUmDekj0fCq7lbJ4UjMv2wxP6c+4GPVWWmoUWSv3nXL7dQzM4zV6g8l7iIvWDt/r2OG19QFSsfitWdqeDgheH5ifPTQSJsnTES03dUeBeKV0pLIbd+V8fPzrc2G9Glv1Bcl6p9CoQ7A9ZF6ojmRylUCqjRLZ9RlW+7jWBBmbcmASZ28Q6zXZqJhYB6+dqKJnxNPSNR2cQXQSWbkUUrtBcWXcrRSMdveIJnvzu+XytfPeAeD+BDfIDuPtMJmwKBQGXYjpOTO5Y2cQUXhsSWpjfOoBsGy62dzmc8DhqlQCt7sOP7HUUJXSsexBMsMQ2AJsJ57PnlE+TPFJkHj1Wt7P4IuEc6P/FrzJKbG1KxnKVqGBhwObsZNf64riRnXFD554ybKEyrTC1JU+P75el+X2quxQxUmR75bLdc68/hRwqMDfvN69RSZewjH/w8meXlN4bPWD96rEwU6ahIn61GX0xkobxenV8pps5HmZZY4LufiNnDPwNN0va7ZmoxBX1joLbDCy2E/Rdj6f+6z5GCHdVcnXoGzsaFzCoou+fG01/w16mb2dXYn0XA/suNQmVOjOWm6tc+r3Yb3ZDJzGSwJQdGTZRj213t7WOqHKaJG5619Q8HwsNYGwvVwvaitEZOj3pJh3kz42HkPdyTD0waDd3t+itShwkHfe0YHGDfc6a9yXr0XPE/MCV6hAQrv5XZ5xfh0CXhtAhuvjcZO1Q6k83D7DvNZDrzl4X2WET7Rvztw0+iT+jttduQFrcSImBKHxkCt/TIcyWYNmWZ4mjpPrMEaVGYQFxVIVYGK6kvkYmmzH4JZePxk1HwRWyyVc0cvtdpJBrD7wS63xFTxEXuAuKdzuV44qIwW9NElixZ4qBs4xpJ7feckxbz3A1whuwkwuNPgyRzzeOygO07GN0J72GeJl7etQ0eg5MHtbp0ZXma95pQAopl3oRXSpnAyfMtL5OlnWLuL9uIa5SyboZ3wDmffpCYfocXMbxHPzwZxEZoJn6a4TXyYwcJtYj5BJvWKEdNNTMQFjRXucHU3Pd2D8MhbNDs6/RrI8BCyPJR6IoiVglQcthWHa0pLnMy+X4RytiumyEQgyBIMEm/mwwxNFEFNH9rK3gbSaPBrqiONhzkdxfLlce73FOB8LbuXD+FpcaA/l4nUMYnEKUf3U9Aqv37HM0bqteN7a5Fjf+4wbpjFDa/WnAunrIywDaI/ih2uDwxbzfrlUb3UQYpI+4LwKFvKtKjzRgMdOMnvW+1YUOKQ2gmP0u36Q8sAHBclMNlkZ1kaj5QZKacS9pNrPuPYOZNfLV09vNFUD5FylfmqTiMTmqeJ74bGwx5vDQMVUc2xGBv7weEauZvSMjL2M3FrnzG45esS4FzauqWUvpkrrkA9vw5E5usaNhOoXro/rUFHzxuMBqYeH0zqgmEzBOo+N95z6GL/tgcUdaZ7tkSrUHJWNeRqDEyYlt9wM9cJ58KT46V7JVXe52Vym1eVxJr/JAz1YhW4qb4MxJZfryrF9XQWzWxnsHzWO7jz63L7Jkuchmpl4LkJYr4qIaRpg8LzVs41z+KXtV8UNPBw7TjVuoKFU98zHVz0aMxKIHtWpi0LpXMjBdkxMhM6GUrSWGBgtLxqBXNyC5brkDCJavKMpoCm1vUOMiDwTbbfKNSHfGzzGyjagofT48iYkQxU9rucBiXlMQKOyYRiIDI98J5EPiOBpoaIkL7xxyIo9Li7sdWUaCH0fDEkcA3rhSueXOd4oAVK5xMqEy2yqKsAjikZqlyTDFQXvUX8fFG1er0rshvnMz8ubrU/QzOOQPJC6U1dIx+JEPRCrMEzhSl7TyiXHpJpNVA8aFaKIgbtnPXHmHcGthsk2U8zkhd1pQzomQzTUaejwnu4I17MCe5IoTb34Jx7vVjdmbtWT5ZkxdIIuWufDXEJUVJ8v2c61NRsUPZaRPwS/b7KvyaRhVU6xtxkSUflJvc28hFEBORMSGi/COrQ/+sgXOB9wzZ0qQ4txr6zLWCfZ/lZvGbu3FhhvH0IpGMdiEskUkgmE1q3gGrP6Oe/PlfpkXCJYdBoMaoYdPnFXzhITU6Rjai3tBNMoSONYWSnFN+8TOWqelOJ8Ba8yTInRKoy3R4FVXwC1IyCEjcexzc9OYxudxuJhzuy3U4jjOPVkPCcKMrggvSsCN/UwtmQ4phZRyc6G+U65CnuIbt2VciXgZyXdLZZv1w9VUARGrQqQFBHFLiqwtRYgDvadlW4cd++e1wCzUC0ebvZXwTAnd12OuFOsq6JtPBT8HA8eCq1CoKBlKKq2k3YYgY3J9zWionqL9VjMa3V/Xw6bp1PCgUrh2qYVzetYVen1OovE7+tKGg/FeBdRBzKZ/drd+H2OkX9i7Rzutb53V8ou/Y5CnCGZDXDTxq5NvAf0weN3J6LHY5eQQm9Z05X7xTqZznnE9gLXBYyCeQNJ5x3pyOxv3HWT72nMPMvmfPdjTLCGv2bDhMbWP/Qq8EvY1x8zlgltBwg7rrKYLXhPIryFKoT3IOBDnocx0LpIe1CMxaUpYqopnZeMNrXFG4pp848NQRgR+gGPaXF8r5BOVVRuJ2kQqp2qH5XT3iE4JzRuERA7rsQvhh28rmVAD0rKjE9M0H/jrhYiWu7zG3897V7BG2ZXJRARW/B2ZXMrFxhYPOEhx3TqWXeJKkyaIQ3Im3MBFROPd5RPYV4kXa+KZoYElaHaj4R9Kq2zpdksKh2F7wJtRg/LTZU0bvHUS0UvZnIPCkNiJpJ4c4h9hhzBZLUZbRgO6rjXQZ7NUeRMP9PYCWdXpackBE7M8QPactSAesFEzz3quFBOsAxzY7l3qoOVaUy04/02OUUDImKRZtxBkX/7ip8yUjcuVNA2vqtRFRsFurpLxOv7Mm720lHBt5AcvDKWM6xe24jHV9/GZr0H7S1czcZnXIoSim2awSGisA1yGrXRRU74DH4tS1yhaV4OMlFYUV+Mtu91sK2gKsUmfp38na2zvRGzzpsEUNPn10YET1gdctuYl6QZ0rk8s06dAzsPlXPWo6IjtCbaoBmESScp0lhRXS7nhBDm3D337q6daBEyjDFRyTITRlPumTtMBMMEy7QEWMeWLVi9LZHfJlTf/E0UM7lLQinEnlERV0WqN4/nko3AHibvS+BbEnyZcr0zFuvER+tuMzF3NI22nB/L5fosn9cPIxmdvA49BVgUgTitarBb8+/B1wdaxGCj1gWh0lEFmrAR9c5xzGQy6A3qakK6gRs8Mht784p13rW0F6TrbC0qaNWgKPwV4tbMe9pp8T0rxtg07yrpQkSPGZcIEYluYk/eJMaK6KnvQsTo5QRiXGh4gakiZO+JwREsFjgZqJNSVvUquIvQHvsTNVSkQmQ1dj36eZRwttsYHIpprATqwKolREHIjmjCHt41v5ufdgUcbVOQH9a9mUoF2CZbv1R9gZjf6mvbzStqvXidw03nnfJ3GKwAwWnOAQTNArQ1MATm6adRGzQ7KjzJ68UtY2pwRu+6DW5oDRujx39RMetkjwMiFVhP3GD2dJ3ocYiDwqgV965inyjsu6yx1ucJd/q8vs8/ra/WEBVDv+N1bjCykYoGIY4tw+D1uPfvI42+vPgmFS9zzXlUAJ6lKj7e0T6bx1gVM8SSxW9s08aj1ttODtkasq8nZOlH0vUysSWrtkhNY54/EXCKjPOkx0WdoLn4xj42czaIMue4xDA0miMib+5beE6HHjLhz9j1IpNxqABdPO7cZWOVanwgdWAcDYvdMLiYQchTdNzse1yFyHf6jOuq/neZXrTIBmGR0jlrWeBGQqnbwyvG5AyBch74tb7+SGOEMo0c0f/b3rUtS46DSPDZ///jNvsACQnCruqZiN15OJqYPlUuWwKJS4IudqxgpYAt1s82uf96aWDNTmXYoXF/ju44+qaJFeJTzkEcpZIlFT/u9FQogXtKS1TP+1C/3X0MYTSyBsBgao9zEqtsmrSxUglUaCr3FeOJvcYpa+6YbxFfBsq88rEo0mWx8in/wnO2LlFu+Zyz+wQoGxxbvIiGKd0UCP8mTF48AnunT/OJfO2mw66P+Lgp+F7HTIJ5PGXtNxHJNbqsmJJ1gAdNIeFy39bofCuWQvZ+39PvFdNXdhdIYPj85bnixe+w8h7UboZwTEt4Pq67nMBBZfPeDRFgDG3OR0OOLrE7HE+OgwgvwwQFHNKI9KRh9QK3zb/xNTZEp4xu5es3W3OlJYxK42NiytuI+sBllBbXr+6KCEP0ticTijrGQkxWkI2HjZ9e3/M86lYH/l5ShkmDD4vPgO4QcBHxVUf5btPhUlqDhF9B68NgPvO9V01PviowwzsW/jLPxxP56b7Rt2Cc+bHh4XmeHGMrdQ/g713PXpmw0la9f9VywFv/zg0LnJHOIYl11NZl5jZxYfb4J8ecE8ZILuodCBBwWSobDFj+liP86s3WM6vl1+ZCYefqUaGIb85+/jGrRdHzflg684UI8KzoAqZzm65odR4xZyl3xg7C8exTr5xtMoyeimImope3lf7fCO5GJvDSs5+tsF7zzJ/QQff8Vd+n8o1XlqTo2Rgg8WNy0sQC3HancNu5maCgI3sf9mZVn//b7byRgRz1r0x1lAQOTCSXct52e4Yl0QUhFdHVmII/GHGQwp+38uVbxvrSLoZMHBQn4yxAw9re4kvDLlYubpN6HLjcCDJX9q86J6/nPdGOMP2IlZ2XuSiAvcOTsn+TCd4U5751PbkQiYMGGjQUNp/FKyo6aultFQ8njTvNT9CqJ8924dnieTM+wmS2STmA6Od8fICHgrH0AzvJH5E7VwjMtwqI3H8KNmt7ZWVIg9pY/YVYlWXF273NfGE88aMhv2xIwdstnXO7MUWlOSNQ9DrBbxL1ledMwkbMWEddSnZkTzj0XSEMh2oL0yhTuDaF0Kqp6u87ZJz5fMC//yHPdFuDFGV8zuY2jzIVZH8m7rlNeMtoxT4hgCzso/1LIcjPiYxJwicj8grzm9KdzzHqEXMBhhe5b9eiuV6UjQZ7jbrQkdd1lRIbnSFbYlYOIRM/L+OWKqCO+NJYwFiHhYSSY0oq7vT7YgAvrWRUQ2NaY+7/TQM2nFDq0vNYvXtOr+Ww2pX96lMUWrJGPlsrgcMWGUwNCZgW/R1qdRhY3VnDTf9I74inRfGbfXjuwLffOEG03e8vACZBc5hApD4ndU6lyof+cdliVx6idRqKDHDCumPTdO/3s5SQ1r0qorztzwlJBNbgbShF6+uQPaTPAaPTUPh3RTDItMU96UtvW/e/gkynh6+DTjdc0Af36NpmKt7KR+W8rQvxBmlw97LrsAb3eMZqreIQgDcR273EiPXMmnKWBd2TRltMvfH6TwrS94/1kEVt3jvlJCbbF5pO+nr/P3n8Hpq833/2d79GpH5ZZvJHYgH5rMXE7lusveYpIKJZLA4opVzpTqXufDJfntQrz0lU+ocbA2TpVLwfiKrghxOAFq4SjslMRH+0O7Riay3vsLbBxiKqxyXahLsZTooFpvznc2OgxZDNLG/NnVH/+sDAguaak/EcJqdVApIYpynIKj940sl7q/svymbUZl/WNecp400JW2zuaZ88vtP4DpV6f343NbO1IyI5RTLHmL1XIREddfD4GF0nGRNt113PSjGfkoBbP0+j3BBfVJ58SPk+IW/MHtC9n3tfUyhkp+Gi9lmB7/voqLV8mOe8V2a3z2I+n2Vyu4BgmiVfUy7x905GoRgmePdTDcyj0GjUqV4TPHsnpRTUk0+BAjJ+2f/XMGV+zuDZ/n3f7eXBB2n/Av7inimo+OyrcsCJiJAw8Dwie4RP5Vu9nImnXn8JHniAeEOQk+CHtv3Mp668+a4Tavc2y/OQpqHcvf5+bcpwXxATf0O+fKGR0udIXVyxgJ6WBh5tjfbQ1szyP5VXz3ldl9if598ZHqlqw+UBOE7CrdsDw7+0ZhN181+OWfMev+EQ/CdFeIeoKnGoiKhcR6dtVnfWPe/5lHhhi3wkW4T7QAZ+1BDQU5EnrVv7O+p5LjOZkx4+LqrAo8fvRFsqbh7kti8wwW9M7zV4Q5sb6ngak9kn2zjp+D2ZiA8bMnAveNG4nQhBpDZh3Fw/bJeO+f5R3j2nXXKFeyabvXSA/wKrhr8+eE7MJZ4dRV1HHfa+QNzpcUtzW/m1i67POrsCfIZwfYDfaXlS3mnRQYNbzTvpgLf3gZ9xoORgK5BCklTtenZUaytVE7qTtyM8lWr76Tm2+naHl4511hSAFQBZEIevjNqQyPuYwKHlgqqFJ68HaOwck6dx72hgzB9LTPed/qCPC9HAepBtGvi8PTFl5nWK5Xa2eztrNcpXUylPeB33IE5yNgcjuGbwdEZHCZqIXYnlMVitfm9EJBNOVD/RA4vK9O0QcVjNQwdj3euR2Nqh/Sybt2TI99aXTGv1Z5i5UGIJ2Ji7G6Rb9aJhgVlG7Wnvrwon+jO7wZoIpgQxUS3avJ9XIc26p4EBLxYwshSjT9HN85T/xgibSa7j1kZ8yBTF1fw3l/pZIZ+iJxSWNmyLdq/pSPCVzO/fz2lSu7mfE0JhdfAAZ+BUUpBESKC0YtCnhs18L1Go3Fj656uMJGji3xosBDShzke75V1JefSm5NP35ZNgPMbsskFfn3C/risTQXmvWK48qvtPiDcV91uaOy23qF41QWBCIcswDPR7GaXatN3yAZ/CA77nhY+3MGYLJ9ibAooyVBYB4OuL01sYMlccUEEMezHRYTD/IAQIxX0Di6+wdlr6qZAqZclayU4MD4TkEHduEg3M4rEedmj4LZZKTUS1zs3cK2ONlZnyQn5rDQ7gpUi97LevGtn7ZWuufjvRBX6f9z0ZOvZ0DpM0vbrZnTS8K/xO085X56PG7KJrvOYr/o/n8ogZx24Bt8OAvDQ9oeeZQ+j3/k35nKTTZb7eURoUlyWQjfle55ng8ri2rxdA/f84ISRS2dBsuTEm4dm6GWlBdTCK2TqGomqycIrvlu1DQy00CUF86wB5Hn+O5epatVHKU52K358GgiHgJkzc5xskfjJ4nR4pY8WLtfEcLSq3uOctMfXmRcFP4x/3drAhUMh6oXH36B5beSx8i65tMhR9Uk69CjbCBmDWs44aOetgg3aGDachy+kq0jwoEzsfdoKX5vqu1eegZ+45ptHHd9i+t4TQdzHndWYvoSQJY7Y2VMRfoLlczg/WB79LdVTuCur/lisNZyIsIEfWTYRWmZjUG5cZouM3r6tIeFPAvVMR770JyFbqhIWFfRWhyeNALdpOEGch3RT9KQ7deZDoTr7fBKEB0Iuor8C5tLyP3ZifNJE4DrP4+eTFxvfkfWr3M2oBz8XLaRkSKQOF5bUnI6pH/wr1+RnuQW70fL9QtPcQvrfyfsBX1ViMob4F7rabrFTG/4Lo3lkFITSfhfDNun+oPjCKTto8EMcIqNyoZxy+1isXuHNn0uGpbH3wTWZYZMB1Yf5v6bv46S4YNKAJMlQsQCK1BvSvdu1mfcwXYx+bhLeF5Cm4oJIU5kj2DSWayjXllpUAKGpuKPjOAJxj0xakyzme9VLoE+nxLCAruY1leiaSezbSkL3Q+3HYMh1NDEJstriJBTM3HCdlVccGYw54E399CsJz97lexm7RS9uOjWSqLaN64sy5cM/PisC0P8dqTzHSEWs8PM/1sHczs1jsXtb6tH17e02ZwwjNEGAzHDnGuRvBtStjIu3ehlsDf4jVmYYJNbs36v33BD8PwBZDBZmc4cMMGzY+2ROin7fkEdez5Qqm+6trkRZdYs/r56LO/BdTKVySlIQBtEYS2B1esBEbPjNMe6GUvhWHGZgdc1GHVJxwtc+4nz3eJ0iZ9CsPQC3j4jeYPVXzpJhv1/I3mXGH998N2LqGC6Es8dFkbH9DHwfNA60lPVjzi2tuybsw4ll4AzNJa2+U9646gHrATWyVC1qutizzhODp11BHeoRABledGs/3F31FEWRCtW/QqGsY467gZpZL7FTL+O9e/WXsqV7lmJY3BXwAVx+Vk+Mybr5iGFqrik5lYQiB+8ntZagPCtWF5LB4qqJGCk7tT0X+Ni7MZ26RWl54xnv8d5bTQ0ob0Kczkg66JlogiwzBWRMdIoJXl0P4TSS3axyyMoRwe9fqvF9J51toIPs4vCH5ZVilCYpJU+r2rPi5Un36xpKpLb/wlhwy4zxEjbHD86s9JyKxBY7mNcdLlpOe5I3GL9lcxvRdPL+LRuAxRWPJlvKk/7NC8PsrayExDXzVvjCm6QVOpZ3Cvn9+K0V6QchKFAE6dyHclKw80fNvZ9u1coXve7q/1wXEEXtVWHlIS09Yp0c/HcKlE7k0qltdm+eb/M2n4JWAbtpzw7srZAxIYkBWhqN4BgqX1837Zwu70HSdzYQpkKCF/p/dUH3X4fmE1LP/sl0gQTkNEZeP2Vos37tVa9lcSAK/2q4JVlhcyzNvQ0kv+h3Xlna3wLxbRLaW1t42zULDfHC9c5lgnWrepxP+pkwB4Lq2OOYtvqFaW/183UjwyvB5nXd48R/tQ9+r39bVusV6pAtreW89+ukpITPHcnq1DVZme1dMvm11E7LBgeT42UzylEa3X+9rcMFbKVxN+jm9sVjApPh2KBf6vyGJcxoO8BYnZ6roP3+REcrN3hDkax290TsNmlKtfjOIs5wCUoNZpx4YXduftXGNY9KnNjca32JLQNqNXgz21g7Dn6ksLLjl1atcCseP1HxZbYwPjP9WrGvEx3saPLwsDnyWfIt3U4xLfesG0V3mb1FKTfJrzpH6U6+Kf+8/blyx/U/UUnHg1dAQlOhpLrFyFkUgGxDNa0l5fteUkS5j7nkXI5sKTgZzoJdZvjtDSCPuIyt4te4uJizgk8VP3XKfQvBmcav+7jVKaFlhuoy1HBiUQLpXVe3QcpLHvyVKWiGc04K5Sla2itlPAyWw+lbET4UFHDpjJ0ik0FEdgTLi13lQ/zQ+gntMch0Z9yNiLAUkE5aJ2ofMAqnhUdw91Mn4ar6YAt7Dv9x1gryKaBwFYiJx/H5AdxIkRz2lEY7elLxa1K7hOVXyHSVbjkJlGxtF8xkz4nQ9vAwYS/tU/C0FVYHkkTTgB8qtFrmI1t6zdn72nKyg+f20qP4F8ELH88/e8s2DloBGiWYtOqgE6dOcJC81pIrmPQuczl/HI5uB+PnZMUpXuM5TKkB6HyhkCVpa7DpfNK/mqSYD3eRnEzo1YGPbcmzXO9Lad6GKh6NTpfWdiPZ5T7oVmwlFRa5lax5+621UPXecmnC8IWR4IRwPksZjtqL0oAxDHEbkRsjErFNH8czE7C8mDRpetPAm9BNZoXzO1uLRMLH5PVuqpXt+4nVt0DZSyi0d3TupE6mSVWWyovADIBOeZ2alfS6EwUG/yp8/d8vMVZzIKXQRs5sU0+T68XWtJm71SqksByaNx+XmPN/aleY/6EleyxOo1rrL6qOwsOmhc0lH1tCmEcZguzMgabZTHPArv4Kxv3LUn7utjl2B50njIQU/3YFavpy3XkcQAxlGI9/Fs5zTmzJh0sZARMVi0f8dVvIyyFzPJqvS8nUoVWvD5bd5T62+rfREfMh9bNs0nbVbuWdIeLM7P5XvlJOUwSvWgq16Ucc/n5czIWjj4vjMz9WzgBh8ayn+82eu/y0xUPC2IJov9+s9ko9ebDn7IgIJIdFLHdKJJCSCKTZASvLo5BPT8GR8j2ZzWczJFzpq9ixNeCWY0fhieQ/1TzN2lg5BSZA1K9uMYfZWyI61+8Atb9KGIjaXE22maLd+sinzIlfQGPTf5MW4T4z+ZW/We+1UoswCL0jF4oHqC67/C20c5WPMCcVMwgSDhWmOavimTu1Zq3NtYp9KkIzdZrEwqhqNsTeeMBS0FJI61ztWIjCZ8u+ZYSkPpzT2aAPjwnGqewl/CnqTQiJh3FjoSAFE/PRypfqxgdrrRoVUf9RR79h8xm+K4wTmiXg+YtL8bPLW31OJvu0euUQSRiavEESchV7FSYgBfR/eCsvhyAjkqQhxPR0txtd6/RL3X+QFq9WYfYe8gPelh6i2HGtJmZXxBDkR2R3A35QPymkpZDO5cYUSoCNnoqVj+oAiwc2mmMFzFRgn5dUoHRqfEBQdYpmuxikraCsNn4grARkfWPjmGWLd6I2BHSMCL1QQzOrVE9FYmAZ+yp+7pF4rkApoApj3YJtFpXcU6u93Fd/HxfaFYCwbTfaQ2WdO23AcguDrNtsAAATISURBVJVUtWwSf+ueq3lIEn24NFbuZjejz1Mx6H+BwYCEKLByywdUW9rqr1dkbcXaX8gMG/rByaqw25TZt+VdOfNdOzF/FoKKbUFm8WYxVV/nWk8SpCwrM1dlmPVlZL1xSWVZadPYiKw/SSsLKWr8odQ8Zxpd+RncnLBOCBanlx4GBI3lfdj9TmRvk+24lFWQMQPMqna3HujGssfuZ9v9VL4ymGedRGTDjLU0kulLtuJaLUw5p7g6jBxGmzx79o3VM1e2ibnCdKNR5zBgZIhnSSBvtNSQUFjnnfkT4X20aLucjn/ndx1t0zjM15vifvCcOMLBwkvGIInkUfcFSSoyACwC3ChiwguOza1nuzR10WAsozeV/9GfNh92kyyxjMF4+A9wsUL9D+9co6k06GlpafGCWa+/iJdWB8fp1QvFR8JkERL8Kj173NvfFxHIsXyQf59ZytlOsjHo4PEqtMP0mEC5Z8Z7RUZBQ0L3dnVx9G2q5qT/qJm8/qRTa+COZx6nvdY2iu9S1JtkR0ZdQvLQmFvrfz99T5UUcVi6ZIaFipIiEcM1D4GplrEoGXVxyTobLDs7DkLOca0wu00WSdGlIIdenlEFFE2km7BoUTb6vWWfRx92iwwIqpQFrGcv5nfxar1evueEYGj7rIO9IRsIHb8VamBL7997HUUPt7t5dfYyfZUXPO0pqoHW7s5PZ+1Q51Sc2fbm9edz/PuTkTo99vz9rf7vykflPBu2s5O0BDi6ueJQQazHUw5W7564TthWS+woBkuci4aNRK3bXk7vc4KgH0cRdGNcbQoFExStM1R9WXb1NGCMqhFTEwh8ERkipNUbV1MIu6fe4x0yFIMu9oZd2Wa7Hq3d96mk1YauNEz42D1It6XpmRqU9jxBZ6tvu1Me1OQLMqgJsebOmq2c9E7nxO1tz+0tfBOHfrV878Tu3WO1e8VC0CIGXSwPloDhZLZn60SNHkuiijbEU0sf9STEajVrwBEzdH4r1gI7plD0NBlpvHFttn/CmD2psPH35An3rCjovYUTeewh36HbuSZ0UvkkdAVjZ108I9v7YiIAhvZFc92Da28ryzYoWomxLhBaH8uhfPCGLJ/+3N14Lzk7FfZvygfl3JiUQ1t7pjb+nreddV3PmaynWItLvdMy/s8X05BPjXlKp7FEpC+bOuEOdlQB0hWZYVDi78W4HXQPZHFa3w7ewGGP4xBCsLBCACxorL6pa1cdVa6dr62eOW7sgdBGIo58jUBX9DIIPG3WB37Y9sPzofSdbLRJQW9BrNkTi6dXPs8/pqkPEkrff+yX6qSDqjMNsOq6J5SXboLejjjOfm7cfXDbH7O1ELbeqDM8iWDrPBufivpmRT6nnzmL6rRoNKB2jHy2x/Bj7iP14x87XzWo3bOI0NmkR1Kkt4kYvNoBDTSQKWgTWw8BXzzlNBrNmGi8oo82OU9YNjO9pwCWINbcMCvC1e6dQkxdmSx970Cu4kV8sctUSlZGVYn8SMFZNyr+e58/dW+eS3sTXQDZScwR61DAU8630o1xm32Vt/W0XPQtG/Vbfstv+f8rf3P002/5Lb/l/7D8Kudv+S3/0fKrnL/lt/xHy69y/pbf8h8tv8r5W37Lf7T8Kudv+S3/0fK/g029WCRv8CoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predict: +blank\n",
            "Truth label: K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUIdrfbQIx7o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}